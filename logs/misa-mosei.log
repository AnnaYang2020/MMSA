2021-01-28 01:00:43:ERROR:name 'logger' is not defined
2021-01-28 04:19:42:INFO:########################################misa-(1/50)########################################
2021-01-28 04:19:42:INFO:batch_size:16
2021-01-28 04:19:42:INFO:learning_rate:0.001
2021-01-28 04:19:42:INFO:hidden_size:64
2021-01-28 04:19:42:INFO:dropout:0.0
2021-01-28 04:19:42:INFO:reverse_grad_weight:0.8
2021-01-28 04:19:42:INFO:diff_weight:0.5
2021-01-28 04:19:42:INFO:sim_weight:1.0
2021-01-28 04:19:42:INFO:sp_weight:1.0
2021-01-28 04:19:42:INFO:recon_weight:0.8
2021-01-28 04:19:42:INFO:grad_clip:-1.0
2021-01-28 04:19:42:INFO:weight_decay:5e-05
2021-01-28 04:19:42:INFO:##########################################################################################
2021-01-28 04:19:42:INFO:Start running misa...
2021-01-28 04:19:42:INFO:Find gpu: 0, with memory: 7621902336 left!
2021-01-28 04:19:42:INFO:Let's use 1 GPUs!
2021-01-28 04:19:56:INFO:train samples: (16326,)
2021-01-28 04:20:10:INFO:valid samples: (1871,)
2021-01-28 04:20:24:INFO:test samples: (4659,)
2021-01-28 04:20:24:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-01-28 04:20:24:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-01-28 04:20:24:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-01-28 04:20:24:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-01-28 04:20:24:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-01-28 04:20:24:INFO:loading file None
2021-01-28 04:20:24:INFO:loading file None
2021-01-28 04:20:24:INFO:loading file None
2021-01-28 04:20:24:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-01-28 04:20:24:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-01-28 04:20:24:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-01-28 04:20:29:ERROR:cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
2021-01-28 20:49:08:INFO:########################################misa-(1/50)########################################
2021-01-28 20:49:08:INFO:batch_size:64
2021-01-28 20:49:08:INFO:learning_rate:0.001
2021-01-28 20:49:08:INFO:hidden_size:256
2021-01-28 20:49:08:INFO:dropout:0.0
2021-01-28 20:49:08:INFO:reverse_grad_weight:0.5
2021-01-28 20:49:08:INFO:diff_weight:0.3
2021-01-28 20:49:08:INFO:sim_weight:0.5
2021-01-28 20:49:08:INFO:sp_weight:1.0
2021-01-28 20:49:08:INFO:recon_weight:0.8
2021-01-28 20:49:08:INFO:grad_clip:-1.0
2021-01-28 20:49:08:INFO:weight_decay:0.002
2021-01-28 20:49:08:INFO:##########################################################################################
2021-01-28 20:49:08:INFO:Start running misa...
2021-01-28 20:49:09:INFO:Find gpu: 1, with memory: 2256338944 left!
2021-01-28 20:49:09:INFO:Let's use 1 GPUs!
2021-01-28 20:51:28:INFO:train samples: (16326,)
2021-01-28 20:51:44:INFO:valid samples: (1871,)
2021-01-28 20:52:01:INFO:test samples: (4659,)
2021-01-28 20:52:02:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-01-28 20:52:02:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-01-28 20:52:02:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-01-28 20:52:02:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-01-28 20:52:02:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-01-28 20:52:02:INFO:loading file None
2021-01-28 20:52:02:INFO:loading file None
2021-01-28 20:52:02:INFO:loading file None
2021-01-28 20:52:02:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-01-28 20:52:02:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-01-28 20:52:02:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-01-28 20:52:05:INFO:The model has 113025633 trainable parameters
2021-01-28 20:54:23:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.9561  Has0_acc_2: 0.6348  Has0_F1_score: 0.6671  Non0_acc_2: 0.5812  Non0_F1_score: 0.6288  Mult_acc_5: 0.4087  Mult_acc_7: 0.4078  MAE: 0.9088  Corr: -0.0103 
2021-01-28 20:54:29:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: 0.2069  Loss: 1.0859 
2021-01-28 20:56:54:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.2943  Has0_acc_2: 0.6968  Has0_F1_score: 0.7914  Non0_acc_2: 0.6228  Non0_F1_score: 0.7378  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8503  Corr: 0.0082 
2021-01-28 20:56:59:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7781  Corr: 0.1464  Loss: 1.0812 
2021-01-28 20:59:21:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.3026  Has0_acc_2: 0.6921  Has0_F1_score: 0.7902  Non0_acc_2: 0.6156  Non0_F1_score: 0.7344  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8507  Corr: -0.0230 
2021-01-28 20:59:26:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7779  Corr: 0.0923  Loss: 1.0659 
2021-01-28 21:01:49:INFO:TRAIN-(misa) (1/4/1)>> loss: 1.2877  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8487  Corr: -0.0112 
2021-01-28 21:01:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: -0.0102  Loss: 1.0666 
2021-01-28 21:04:16:INFO:TRAIN-(misa) (2/5/1)>> loss: 1.2795  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8488  Corr: -0.0088 
2021-01-28 21:04:22:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: 0.0154  Loss: 1.0737 
2021-01-28 21:06:43:INFO:TRAIN-(misa) (3/6/1)>> loss: 1.2754  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8487  Corr: 0.0005 
2021-01-28 21:06:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: 0.0375  Loss: 1.0703 
2021-01-28 21:09:08:INFO:TRAIN-(misa) (4/7/1)>> loss: 1.2754  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8488  Corr: -0.0076 
2021-01-28 21:09:13:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: 0.0639  Loss: 1.0664 
2021-01-28 21:11:35:INFO:TRAIN-(misa) (5/8/1)>> loss: 1.2703  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8486  Corr: -0.0076 
2021-01-28 21:11:40:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7779  Corr: 0.0495  Loss: 1.0811 
2021-01-28 21:14:01:INFO:TRAIN-(misa) (6/9/1)>> loss: 1.2717  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8485  Corr: -0.0030 
2021-01-28 21:14:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: 0.0661  Loss: 1.0760 
2021-01-28 21:16:28:INFO:TRAIN-(misa) (7/10/1)>> loss: 1.2685  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8488  Corr: -0.0156 
2021-01-28 21:16:33:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: 0.0415  Loss: 1.0815 
2021-01-28 21:18:54:INFO:TRAIN-(misa) (8/11/1)>> loss: 1.2707  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8485  Corr: -0.0023 
2021-01-28 21:19:00:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7779  Corr: 0.0168  Loss: 1.0802 
2021-01-28 21:19:14:INFO:TEST-(misa) >>  Has0_acc_2: 0.7102  Has0_F1_score: 0.8306  Non0_acc_2: 0.6285  Non0_F1_score: 0.7719  Mult_acc_5: 0.4136  Mult_acc_7: 0.4136  MAE: 0.8419  Corr: 0.1133  Loss: 1.2302 
2021-01-28 21:19:14:INFO:Start saving results...
2021-01-28 21:19:14:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-01-28 21:19:14:INFO:########################################misa-(2/50)########################################
2021-01-28 21:19:14:INFO:batch_size:64
2021-01-28 21:19:14:INFO:learning_rate:0.0001
2021-01-28 21:19:14:INFO:hidden_size:64
2021-01-28 21:19:14:INFO:dropout:0.2
2021-01-28 21:19:14:INFO:reverse_grad_weight:0.8
2021-01-28 21:19:14:INFO:diff_weight:0.3
2021-01-28 21:19:14:INFO:sim_weight:1.0
2021-01-28 21:19:14:INFO:sp_weight:1.0
2021-01-28 21:19:14:INFO:recon_weight:0.5
2021-01-28 21:19:14:INFO:grad_clip:0.8
2021-01-28 21:19:14:INFO:weight_decay:0.0
2021-01-28 21:19:14:INFO:##########################################################################################
2021-01-28 21:19:14:INFO:Start running misa...
2021-01-28 21:19:14:INFO:Find gpu: 1, with memory: 2256338944 left!
2021-01-28 21:19:14:INFO:Let's use 1 GPUs!
2021-01-28 21:19:29:INFO:train samples: (16326,)
2021-01-28 21:19:46:INFO:valid samples: (1871,)
2021-01-28 21:20:02:INFO:test samples: (4659,)
2021-01-28 21:20:03:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-01-28 21:20:03:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-01-28 21:20:03:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-01-28 21:20:03:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-01-28 21:20:03:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-01-28 21:20:03:INFO:loading file None
2021-01-28 21:20:03:INFO:loading file None
2021-01-28 21:20:03:INFO:loading file None
2021-01-28 21:20:03:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-01-28 21:20:03:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-01-28 21:20:03:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-01-28 21:20:06:INFO:The model has 110219553 trainable parameters
2021-01-28 21:22:29:INFO:TRAIN-(misa) (1/1/1)>> loss: 2.1005  Has0_acc_2: 0.6424  Has0_F1_score: 0.6686  Non0_acc_2: 0.5982  Non0_F1_score: 0.6395  Mult_acc_5: 0.4158  Mult_acc_7: 0.4158  MAE: 0.8439  Corr: 0.1021 
2021-01-28 21:22:34:INFO:VAL-(misa) >>  Has0_acc_2: 0.7055  Has0_F1_score: 0.7493  Non0_acc_2: 0.6537  Non0_F1_score: 0.7139  Mult_acc_5: 0.4452  Mult_acc_7: 0.4452  MAE: 0.7627  Corr: 0.1761  Loss: 1.0289 
2021-01-28 21:25:01:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.5734  Has0_acc_2: 0.6333  Has0_F1_score: 0.6381  Non0_acc_2: 0.6128  Non0_F1_score: 0.6312  Mult_acc_5: 0.4191  Mult_acc_7: 0.4191  MAE: 0.8333  Corr: 0.1952 
2021-01-28 21:25:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.5954  Has0_F1_score: 0.5768  Non0_acc_2: 0.5974  Non0_F1_score: 0.5936  Mult_acc_5: 0.4479  Mult_acc_7: 0.4479  MAE: 0.7727  Corr: 0.1828  Loss: 1.0676 
2021-01-28 21:27:30:INFO:TRAIN-(misa) (2/3/1)>> loss: 1.4241  Has0_acc_2: 0.6376  Has0_F1_score: 0.6351  Non0_acc_2: 0.6276  Non0_F1_score: 0.6376  Mult_acc_5: 0.4188  Mult_acc_7: 0.4188  MAE: 0.8283  Corr: 0.2326 
2021-01-28 21:27:35:INFO:VAL-(misa) >>  Has0_acc_2: 0.5633  Has0_F1_score: 0.5399  Non0_acc_2: 0.5751  Non0_F1_score: 0.5674  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7788  Corr: 0.1783  Loss: 1.0528 
2021-01-28 21:29:56:INFO:TRAIN-(misa) (3/4/1)>> loss: 1.3564  Has0_acc_2: 0.6362  Has0_F1_score: 0.6321  Non0_acc_2: 0.6293  Non0_F1_score: 0.6377  Mult_acc_5: 0.4167  Mult_acc_7: 0.4167  MAE: 0.8271  Corr: 0.2478 
2021-01-28 21:30:01:INFO:VAL-(misa) >>  Has0_acc_2: 0.5895  Has0_F1_score: 0.5718  Non0_acc_2: 0.5925  Non0_F1_score: 0.5908  Mult_acc_5: 0.4500  Mult_acc_7: 0.4500  MAE: 0.7753  Corr: 0.1805  Loss: 1.0601 
2021-01-28 21:32:24:INFO:TRAIN-(misa) (4/5/1)>> loss: 1.3210  Has0_acc_2: 0.6490  Has0_F1_score: 0.6456  Non0_acc_2: 0.6445  Non0_F1_score: 0.6531  Mult_acc_5: 0.4215  Mult_acc_7: 0.4215  MAE: 0.8192  Corr: 0.2846 
2021-01-28 21:32:29:INFO:VAL-(misa) >>  Has0_acc_2: 0.6307  Has0_F1_score: 0.6271  Non0_acc_2: 0.6134  Non0_F1_score: 0.6250  Mult_acc_5: 0.4479  Mult_acc_7: 0.4479  MAE: 0.7731  Corr: 0.1806  Loss: 1.0384 
2021-01-28 21:34:51:INFO:TRAIN-(misa) (5/6/1)>> loss: 1.3120  Has0_acc_2: 0.6482  Has0_F1_score: 0.6431  Non0_acc_2: 0.6463  Non0_F1_score: 0.6531  Mult_acc_5: 0.4193  Mult_acc_7: 0.4193  MAE: 0.8186  Corr: 0.2893 
2021-01-28 21:34:57:INFO:VAL-(misa) >>  Has0_acc_2: 0.6606  Has0_F1_score: 0.6734  Non0_acc_2: 0.6314  Non0_F1_score: 0.6602  Mult_acc_5: 0.4148  Mult_acc_7: 0.4148  MAE: 0.7860  Corr: 0.1795  Loss: 1.0771 
2021-01-28 21:37:20:INFO:TRAIN-(misa) (6/7/1)>> loss: 1.2917  Has0_acc_2: 0.6563  Has0_F1_score: 0.6511  Non0_acc_2: 0.6570  Non0_F1_score: 0.6632  Mult_acc_5: 0.4174  Mult_acc_7: 0.4174  MAE: 0.8125  Corr: 0.3159 
2021-01-28 21:37:25:INFO:VAL-(misa) >>  Has0_acc_2: 0.6622  Has0_F1_score: 0.6751  Non0_acc_2: 0.6307  Non0_F1_score: 0.6589  Mult_acc_5: 0.4094  Mult_acc_7: 0.4094  MAE: 0.7871  Corr: 0.1839  Loss: 1.0551 
2021-01-28 21:39:46:INFO:TRAIN-(misa) (7/8/1)>> loss: 1.2833  Has0_acc_2: 0.6593  Has0_F1_score: 0.6551  Non0_acc_2: 0.6575  Non0_F1_score: 0.6647  Mult_acc_5: 0.4187  Mult_acc_7: 0.4186  MAE: 0.8123  Corr: 0.3179 
2021-01-28 21:39:52:INFO:VAL-(misa) >>  Has0_acc_2: 0.6328  Has0_F1_score: 0.6299  Non0_acc_2: 0.6189  Non0_F1_score: 0.6318  Mult_acc_5: 0.4190  Mult_acc_7: 0.4190  MAE: 0.7898  Corr: 0.1830  Loss: 1.0695 
2021-01-28 21:42:15:INFO:TRAIN-(misa) (8/9/1)>> loss: 1.2637  Has0_acc_2: 0.6604  Has0_F1_score: 0.6541  Non0_acc_2: 0.6658  Non0_F1_score: 0.6709  Mult_acc_5: 0.4153  Mult_acc_7: 0.4153  MAE: 0.8063  Corr: 0.3476 
2021-01-28 21:42:20:INFO:VAL-(misa) >>  Has0_acc_2: 0.5521  Has0_F1_score: 0.5272  Non0_acc_2: 0.5730  Non0_F1_score: 0.5640  Mult_acc_5: 0.4356  Mult_acc_7: 0.4356  MAE: 0.8044  Corr: 0.1805  Loss: 1.1021 
2021-01-28 21:42:35:INFO:TEST-(misa) >>  Has0_acc_2: 0.6838  Has0_F1_score: 0.7459  Non0_acc_2: 0.6219  Non0_F1_score: 0.7016  Mult_acc_5: 0.4140  Mult_acc_7: 0.4140  MAE: 0.8270  Corr: 0.1389  Loss: 1.2080 
2021-01-28 21:42:35:INFO:Start saving results...
2021-01-28 21:42:35:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-01-28 21:42:35:INFO:########################################misa-(3/50)########################################
2021-01-28 21:42:35:INFO:batch_size:16
2021-01-28 21:42:35:INFO:learning_rate:0.001
2021-01-28 21:42:35:INFO:hidden_size:64
2021-01-28 21:42:35:INFO:dropout:0.2
2021-01-28 21:42:35:INFO:reverse_grad_weight:1.0
2021-01-28 21:42:35:INFO:diff_weight:0.5
2021-01-28 21:42:35:INFO:sim_weight:0.8
2021-01-28 21:42:35:INFO:sp_weight:1.0
2021-01-28 21:42:35:INFO:recon_weight:0.5
2021-01-28 21:42:35:INFO:grad_clip:0.8
2021-01-28 21:42:35:INFO:weight_decay:0.002
2021-01-28 21:42:35:INFO:##########################################################################################
2021-01-28 21:42:35:INFO:Start running misa...
2021-01-28 21:42:35:INFO:Find gpu: 1, with memory: 2256338944 left!
2021-01-28 21:42:35:INFO:Let's use 1 GPUs!
2021-01-28 21:43:08:INFO:train samples: (16326,)
2021-01-28 21:43:24:INFO:valid samples: (1871,)
2021-01-28 21:43:41:INFO:test samples: (4659,)
2021-01-28 21:43:41:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-01-28 21:43:41:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-01-28 21:43:41:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-01-28 21:43:41:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-01-28 21:43:41:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-01-28 21:43:41:INFO:loading file None
2021-01-28 21:43:41:INFO:loading file None
2021-01-28 21:43:41:INFO:loading file None
2021-01-28 21:43:42:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-01-28 21:43:42:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-01-28 21:43:42:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-01-28 21:43:46:INFO:The model has 110219553 trainable parameters
2021-01-28 21:47:44:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.3561  Has0_acc_2: 0.6844  Has0_F1_score: 0.7649  Non0_acc_2: 0.6134  Non0_F1_score: 0.7128  Mult_acc_5: 0.4146  Mult_acc_7: 0.4146  MAE: 0.8530  Corr: -0.0007 
2021-01-28 21:47:52:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7785  Corr: -0.0132  Loss: 1.0845 
2021-01-28 21:51:38:INFO:TRAIN-(misa) (1/2/1)>> loss: nan  Has0_acc_2: 0.3436  Has0_F1_score: 0.4086  Non0_acc_2: 0.4038  Non0_F1_score: 0.4823  Mult_acc_5: 0.0551  Mult_acc_7: 0.0551  MAE: nan  Corr: nan 
2021-01-28 21:51:45:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 21:55:28:INFO:TRAIN-(misa) (2/3/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-01-28 21:55:35:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 21:59:18:INFO:TRAIN-(misa) (3/4/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-01-28 21:59:25:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 22:03:07:INFO:TRAIN-(misa) (4/5/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-01-28 22:03:14:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 22:06:58:INFO:TRAIN-(misa) (5/6/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-01-28 22:07:05:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 22:10:49:INFO:TRAIN-(misa) (6/7/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-01-28 22:10:56:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 22:14:44:INFO:TRAIN-(misa) (7/8/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-01-28 22:14:50:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 22:18:42:INFO:TRAIN-(misa) (8/9/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-01-28 22:18:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-01-28 22:19:07:INFO:TEST-(misa) >>  Has0_acc_2: 0.7102  Has0_F1_score: 0.8306  Non0_acc_2: 0.6285  Non0_F1_score: 0.7719  Mult_acc_5: 0.4136  Mult_acc_7: 0.4136  MAE: 0.8439  Corr: -0.0100  Loss: 1.2464 
2021-01-28 22:19:07:INFO:Start saving results...
2021-01-28 22:19:07:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-01-28 22:19:07:INFO:########################################misa-(4/50)########################################
2021-01-28 22:19:07:INFO:batch_size:64
2021-01-28 22:19:07:INFO:learning_rate:0.0001
2021-01-28 22:19:07:INFO:hidden_size:256
2021-01-28 22:19:07:INFO:dropout:0.0
2021-01-28 22:19:07:INFO:reverse_grad_weight:1.0
2021-01-28 22:19:07:INFO:diff_weight:0.3
2021-01-28 22:19:07:INFO:sim_weight:1.0
2021-01-28 22:19:07:INFO:sp_weight:1.0
2021-01-28 22:19:07:INFO:recon_weight:0.8
2021-01-28 22:19:07:INFO:grad_clip:-1.0
2021-01-28 22:19:07:INFO:weight_decay:0.0
2021-01-28 22:19:07:INFO:##########################################################################################
2021-01-28 22:19:07:INFO:Start running misa...
2021-01-28 22:19:07:INFO:Find gpu: 1, with memory: 2174550016 left!
2021-01-28 22:19:07:INFO:Let's use 1 GPUs!
2021-01-28 22:19:32:INFO:train samples: (16326,)
2021-01-28 22:20:12:INFO:valid samples: (1871,)
2021-01-28 22:20:50:INFO:test samples: (4659,)
2021-01-28 22:20:52:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-01-28 22:20:52:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-01-28 22:20:52:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-01-28 22:20:52:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-01-28 22:20:52:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-01-28 22:20:52:INFO:loading file None
2021-01-28 22:20:52:INFO:loading file None
2021-01-28 22:20:52:INFO:loading file None
2021-01-28 22:20:52:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-01-28 22:20:52:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-01-28 22:20:52:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-01-28 22:20:56:INFO:The model has 113025633 trainable parameters
2021-01-28 22:20:56:ERROR:CUDA out of memory. Tried to allocate 38.00 MiB (GPU 1; 7.77 GiB total capacity; 3.82 GiB already allocated; 30.50 MiB free; 99.23 MiB cached)
2021-01-29 14:08:29:INFO:########################################misa-(1/50)########################################
2021-01-29 14:08:29:INFO:batch_size:16
2021-01-29 14:08:29:INFO:learning_rate:0.0005
2021-01-29 14:08:29:INFO:hidden_size:64
2021-01-29 14:08:29:INFO:dropout:0.5
2021-01-29 14:08:29:INFO:reverse_grad_weight:0.5
2021-01-29 14:08:29:INFO:diff_weight:0.3
2021-01-29 14:08:29:INFO:sim_weight:0.5
2021-01-29 14:08:29:INFO:sp_weight:0.0
2021-01-29 14:08:29:INFO:recon_weight:1.0
2021-01-29 14:08:29:INFO:grad_clip:-1.0
2021-01-29 14:08:29:INFO:weight_decay:5e-05
2021-01-29 14:08:29:INFO:##########################################################################################
2021-01-29 14:08:29:INFO:Start running misa...
2021-01-29 14:08:29:INFO:Find gpu: 2, with memory: 3459055616 left!
2021-01-29 14:08:29:INFO:Let's use 1 GPUs!
2021-01-29 14:09:16:ERROR:
2021-02-02 00:44:39:INFO:########################################misa-(1/47)########################################
2021-02-02 00:44:39:INFO:batch_size:32
2021-02-02 00:44:39:INFO:learning_rate:0.0001
2021-02-02 00:44:39:INFO:hidden_size:128
2021-02-02 00:44:39:INFO:dropout:0.2
2021-02-02 00:44:39:INFO:reverse_grad_weight:0.5
2021-02-02 00:44:39:INFO:diff_weight:0.1
2021-02-02 00:44:39:INFO:sim_weight:1.0
2021-02-02 00:44:39:INFO:sp_weight:1.0
2021-02-02 00:44:39:INFO:recon_weight:0.8
2021-02-02 00:44:39:INFO:grad_clip:0.8
2021-02-02 00:44:39:INFO:weight_decay:0.0
2021-02-02 00:44:39:INFO:##########################################################################################
2021-02-02 00:44:39:INFO:Start running misa...
2021-02-02 00:44:39:INFO:Find gpu: 0, with memory: 11337728 left!
2021-02-02 00:44:39:INFO:Let's use 1 GPUs!
2021-02-02 00:44:50:INFO:train samples: (16326,)
2021-02-02 00:45:03:INFO:valid samples: (1871,)
2021-02-02 00:45:15:INFO:test samples: (4659,)
2021-02-02 00:45:16:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 00:45:16:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 00:45:16:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 00:45:16:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 00:45:16:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 00:45:16:INFO:loading file None
2021-02-02 00:45:16:INFO:loading file None
2021-02-02 00:45:16:INFO:loading file None
2021-02-02 00:45:16:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 00:45:16:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 00:45:16:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 00:45:22:INFO:The model has 110917345 trainable parameters
2021-02-02 00:48:00:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.2887  Has0_acc_2: 0.7848  Has0_F1_score: 0.7796  Non0_acc_2: 0.8170  Non0_F1_score: 0.8179  Mult_acc_5: 0.4908  Mult_acc_7: 0.4791  MAE: 0.6282  Corr: 0.6743 
2021-02-02 00:48:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.8022  Has0_F1_score: 0.7954  Non0_acc_2: 0.8414  Non0_F1_score: 0.8414  Mult_acc_5: 0.5452  Mult_acc_7: 0.5329  MAE: 0.5356  Corr: 0.7313  Loss: 0.5136 
2021-02-02 00:50:51:INFO:TRAIN-(misa) (1/2/1)>> loss: 0.5338  Has0_acc_2: 0.8316  Has0_F1_score: 0.8271  Non0_acc_2: 0.8782  Non0_F1_score: 0.8786  Mult_acc_5: 0.5779  Mult_acc_7: 0.5547  MAE: 0.4936  Corr: 0.8220 
2021-02-02 00:50:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.8375  Has0_F1_score: 0.8377  Non0_acc_2: 0.8435  Non0_F1_score: 0.8472  Mult_acc_5: 0.5345  Mult_acc_7: 0.5184  MAE: 0.5426  Corr: 0.7346  Loss: 0.5150 
2021-02-02 00:53:39:INFO:TRAIN-(misa) (2/3/1)>> loss: 0.3369  Has0_acc_2: 0.8591  Has0_F1_score: 0.8553  Non0_acc_2: 0.9146  Non0_F1_score: 0.9148  Mult_acc_5: 0.6601  Mult_acc_7: 0.6327  MAE: 0.3985  Corr: 0.8885 
2021-02-02 00:53:45:INFO:VAL-(misa) >>  Has0_acc_2: 0.8327  Has0_F1_score: 0.8308  Non0_acc_2: 0.8512  Non0_F1_score: 0.8536  Mult_acc_5: 0.5371  Mult_acc_7: 0.5243  MAE: 0.5403  Corr: 0.7279  Loss: 0.5154 
2021-02-02 00:56:27:INFO:TRAIN-(misa) (3/4/1)>> loss: 0.2271  Has0_acc_2: 0.8742  Has0_F1_score: 0.8704  Non0_acc_2: 0.9413  Non0_F1_score: 0.9413  Mult_acc_5: 0.7382  Mult_acc_7: 0.7089  MAE: 0.3212  Corr: 0.9281 
2021-02-02 00:56:33:INFO:VAL-(misa) >>  Has0_acc_2: 0.7606  Has0_F1_score: 0.7481  Non0_acc_2: 0.8241  Non0_F1_score: 0.8212  Mult_acc_5: 0.5318  Mult_acc_7: 0.5163  MAE: 0.5692  Corr: 0.7258  Loss: 0.5535 
2021-02-02 00:59:15:INFO:TRAIN-(misa) (4/5/1)>> loss: 0.1733  Has0_acc_2: 0.8787  Has0_F1_score: 0.8749  Non0_acc_2: 0.9531  Non0_F1_score: 0.9531  Mult_acc_5: 0.7743  Mult_acc_7: 0.7467  MAE: 0.2787  Corr: 0.9464 
2021-02-02 00:59:20:INFO:VAL-(misa) >>  Has0_acc_2: 0.8365  Has0_F1_score: 0.8350  Non0_acc_2: 0.8526  Non0_F1_score: 0.8551  Mult_acc_5: 0.5168  Mult_acc_7: 0.4992  MAE: 0.5776  Corr: 0.7235  Loss: 0.5733 
2021-02-02 01:02:01:INFO:TRAIN-(misa) (5/6/1)>> loss: 0.1338  Has0_acc_2: 0.8876  Has0_F1_score: 0.8840  Non0_acc_2: 0.9676  Non0_F1_score: 0.9676  Mult_acc_5: 0.8089  Mult_acc_7: 0.7827  MAE: 0.2412  Corr: 0.9597 
2021-02-02 01:02:07:INFO:VAL-(misa) >>  Has0_acc_2: 0.7846  Has0_F1_score: 0.7749  Non0_acc_2: 0.8345  Non0_F1_score: 0.8328  Mult_acc_5: 0.5307  Mult_acc_7: 0.5158  MAE: 0.5677  Corr: 0.7202  Loss: 0.5673 
2021-02-02 01:04:49:INFO:TRAIN-(misa) (6/7/1)>> loss: 0.1056  Has0_acc_2: 0.8902  Has0_F1_score: 0.8865  Non0_acc_2: 0.9765  Non0_F1_score: 0.9765  Mult_acc_5: 0.8358  Mult_acc_7: 0.8121  MAE: 0.2112  Corr: 0.9693 
2021-02-02 01:04:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.8258  Has0_F1_score: 0.8229  Non0_acc_2: 0.8477  Non0_F1_score: 0.8496  Mult_acc_5: 0.5366  Mult_acc_7: 0.5216  MAE: 0.5544  Corr: 0.7315  Loss: 0.5327 
2021-02-02 01:07:37:INFO:TRAIN-(misa) (7/8/1)>> loss: 0.0899  Has0_acc_2: 0.8970  Has0_F1_score: 0.8936  Non0_acc_2: 0.9830  Non0_F1_score: 0.9830  Mult_acc_5: 0.8493  Mult_acc_7: 0.8268  MAE: 0.1944  Corr: 0.9739 
2021-02-02 01:07:43:INFO:VAL-(misa) >>  Has0_acc_2: 0.8316  Has0_F1_score: 0.8297  Non0_acc_2: 0.8463  Non0_F1_score: 0.8485  Mult_acc_5: 0.5094  Mult_acc_7: 0.4923  MAE: 0.5828  Corr: 0.7293  Loss: 0.5732 
2021-02-02 01:10:23:INFO:TRAIN-(misa) (8/9/1)>> loss: 0.0836  Has0_acc_2: 0.8959  Has0_F1_score: 0.8924  Non0_acc_2: 0.9856  Non0_F1_score: 0.9856  Mult_acc_5: 0.8608  Mult_acc_7: 0.8387  MAE: 0.1873  Corr: 0.9756 
2021-02-02 01:10:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.8236  Has0_F1_score: 0.8194  Non0_acc_2: 0.8484  Non0_F1_score: 0.8492  Mult_acc_5: 0.5441  Mult_acc_7: 0.5270  MAE: 0.5471  Corr: 0.7226  Loss: 0.5358 
2021-02-02 01:10:45:INFO:TEST-(misa) >>  Has0_acc_2: 0.8012  Has0_F1_score: 0.7948  Non0_acc_2: 0.8459  Non0_F1_score: 0.8455  Mult_acc_5: 0.5450  Mult_acc_7: 0.5291  MAE: 0.5528  Corr: 0.7543  Loss: 0.5421 
2021-02-02 01:10:45:INFO:Start saving results...
2021-02-02 01:10:45:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 01:10:45:INFO:########################################misa-(2/47)########################################
2021-02-02 01:10:45:INFO:batch_size:32
2021-02-02 01:10:45:INFO:learning_rate:0.0005
2021-02-02 01:10:45:INFO:hidden_size:64
2021-02-02 01:10:45:INFO:dropout:0.5
2021-02-02 01:10:45:INFO:reverse_grad_weight:0.8
2021-02-02 01:10:45:INFO:diff_weight:0.5
2021-02-02 01:10:45:INFO:sim_weight:1.0
2021-02-02 01:10:45:INFO:sp_weight:1.0
2021-02-02 01:10:45:INFO:recon_weight:1.0
2021-02-02 01:10:45:INFO:grad_clip:0.8
2021-02-02 01:10:45:INFO:weight_decay:5e-05
2021-02-02 01:10:45:INFO:##########################################################################################
2021-02-02 01:10:45:INFO:Start running misa...
2021-02-02 01:10:46:INFO:Find gpu: 2, with memory: 836567040 left!
2021-02-02 01:10:46:INFO:Let's use 1 GPUs!
2021-02-02 01:10:59:INFO:train samples: (16326,)
2021-02-02 01:11:12:INFO:valid samples: (1871,)
2021-02-02 01:11:25:INFO:test samples: (4659,)
2021-02-02 01:11:25:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 01:11:25:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 01:11:25:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 01:11:25:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 01:11:25:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 01:11:25:INFO:loading file None
2021-02-02 01:11:25:INFO:loading file None
2021-02-02 01:11:25:INFO:loading file None
2021-02-02 01:11:25:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 01:11:25:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 01:11:25:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 01:11:31:INFO:The model has 110219553 trainable parameters
2021-02-02 01:13:50:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.5666  Has0_acc_2: 0.6702  Has0_F1_score: 0.7381  Non0_acc_2: 0.6057  Non0_F1_score: 0.6927  Mult_acc_5: 0.4157  Mult_acc_7: 0.4157  MAE: 0.8515  Corr: 0.0076 
2021-02-02 01:13:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: 0.0146  Loss: 1.0742 
2021-02-02 01:16:18:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.3450  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8490  Corr: -0.0092 
2021-02-02 01:16:23:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: 0.0406  Loss: 1.0812 
2021-02-02 01:18:43:INFO:TRAIN-(misa) (2/3/1)>> loss: nan  Has0_acc_2: 0.6793  Has0_F1_score: 0.7577  Non0_acc_2: 0.6103  Non0_F1_score: 0.7084  Mult_acc_5: 0.3934  Mult_acc_7: 0.3934  MAE: nan  Corr: nan 
2021-02-02 01:18:49:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 01:21:08:INFO:TRAIN-(misa) (3/4/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 01:21:13:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 01:23:32:INFO:TRAIN-(misa) (4/5/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 01:23:37:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 01:25:55:INFO:TRAIN-(misa) (5/6/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 01:26:01:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 01:28:19:INFO:TRAIN-(misa) (6/7/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 01:28:24:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 01:30:43:INFO:TRAIN-(misa) (7/8/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 01:30:49:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 01:33:07:INFO:TRAIN-(misa) (8/9/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 01:33:13:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 01:33:26:INFO:TEST-(misa) >>  Has0_acc_2: 0.7102  Has0_F1_score: 0.8306  Non0_acc_2: 0.6285  Non0_F1_score: 0.7719  Mult_acc_5: 0.4136  Mult_acc_7: 0.4136  MAE: 0.8411  Corr: 0.0198  Loss: 1.2304 
2021-02-02 01:33:26:INFO:Start saving results...
2021-02-02 01:33:26:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 01:33:26:INFO:########################################misa-(3/47)########################################
2021-02-02 01:33:26:INFO:batch_size:32
2021-02-02 01:33:26:INFO:learning_rate:0.0001
2021-02-02 01:33:26:INFO:hidden_size:64
2021-02-02 01:33:26:INFO:dropout:0.2
2021-02-02 01:33:26:INFO:reverse_grad_weight:1.0
2021-02-02 01:33:26:INFO:diff_weight:0.5
2021-02-02 01:33:26:INFO:sim_weight:1.0
2021-02-02 01:33:26:INFO:sp_weight:0.0
2021-02-02 01:33:26:INFO:recon_weight:1.0
2021-02-02 01:33:26:INFO:grad_clip:0.8
2021-02-02 01:33:26:INFO:weight_decay:0.0
2021-02-02 01:33:26:INFO:##########################################################################################
2021-02-02 01:33:26:INFO:Start running misa...
2021-02-02 01:33:26:INFO:Find gpu: 2, with memory: 1661796352 left!
2021-02-02 01:33:26:INFO:Let's use 1 GPUs!
2021-02-02 01:33:40:INFO:train samples: (16326,)
2021-02-02 01:33:53:INFO:valid samples: (1871,)
2021-02-02 01:34:06:INFO:test samples: (4659,)
2021-02-02 01:34:07:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 01:34:07:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 01:34:07:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 01:34:07:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 01:34:07:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 01:34:07:INFO:loading file None
2021-02-02 01:34:07:INFO:loading file None
2021-02-02 01:34:07:INFO:loading file None
2021-02-02 01:34:07:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 01:34:07:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 01:34:07:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 01:34:09:INFO:The model has 110219553 trainable parameters
2021-02-02 01:36:28:INFO:TRAIN-(misa) (1/1/1)>> loss: 2.0034  Has0_acc_2: 0.6341  Has0_F1_score: 0.6509  Non0_acc_2: 0.5995  Non0_F1_score: 0.6312  Mult_acc_5: 0.4137  Mult_acc_7: 0.4137  MAE: 0.8439  Corr: 0.1269 
2021-02-02 01:36:34:INFO:VAL-(misa) >>  Has0_acc_2: 0.6489  Has0_F1_score: 0.6483  Non0_acc_2: 0.6287  Non0_F1_score: 0.6421  Mult_acc_5: 0.4474  Mult_acc_7: 0.4474  MAE: 0.7646  Corr: 0.1917  Loss: 1.0521 
2021-02-02 01:38:56:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.3522  Has0_acc_2: 0.6332  Has0_F1_score: 0.6436  Non0_acc_2: 0.6064  Non0_F1_score: 0.6314  Mult_acc_5: 0.4181  Mult_acc_7: 0.4181  MAE: 0.8370  Corr: 0.1693 
2021-02-02 01:39:01:INFO:VAL-(misa) >>  Has0_acc_2: 0.7194  Has0_F1_score: 0.7844  Non0_acc_2: 0.6579  Non0_F1_score: 0.7419  Mult_acc_5: 0.4484  Mult_acc_7: 0.4484  MAE: 0.7594  Corr: 0.1893  Loss: 1.0429 
2021-02-02 01:41:22:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.3069  Has0_acc_2: 0.6329  Has0_F1_score: 0.6335  Non0_acc_2: 0.6160  Non0_F1_score: 0.6296  Mult_acc_5: 0.4212  Mult_acc_7: 0.4212  MAE: 0.8305  Corr: 0.2125 
2021-02-02 01:41:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.3896  Has0_F1_score: 0.4112  Non0_acc_2: 0.4583  Non0_F1_score: 0.4906  Mult_acc_5: 0.4474  Mult_acc_7: 0.4474  MAE: 0.8099  Corr: 0.1618  Loss: 1.1140 
2021-02-02 01:43:48:INFO:TRAIN-(misa) (2/4/1)>> loss: 1.2765  Has0_acc_2: 0.6299  Has0_F1_score: 0.6231  Non0_acc_2: 0.6293  Non0_F1_score: 0.6353  Mult_acc_5: 0.4186  Mult_acc_7: 0.4186  MAE: 0.8253  Corr: 0.2559 
2021-02-02 01:43:53:INFO:VAL-(misa) >>  Has0_acc_2: 0.6991  Has0_F1_score: 0.7458  Non0_acc_2: 0.6460  Non0_F1_score: 0.7106  Mult_acc_5: 0.4506  Mult_acc_7: 0.4506  MAE: 0.7649  Corr: 0.1563  Loss: 1.0547 
2021-02-02 01:46:13:INFO:TRAIN-(misa) (3/5/1)>> loss: 1.2599  Has0_acc_2: 0.6430  Has0_F1_score: 0.6379  Non0_acc_2: 0.6394  Non0_F1_score: 0.6463  Mult_acc_5: 0.4165  Mult_acc_7: 0.4165  MAE: 0.8197  Corr: 0.2800 
2021-02-02 01:46:18:INFO:VAL-(misa) >>  Has0_acc_2: 0.7066  Has0_F1_score: 0.7678  Non0_acc_2: 0.6481  Non0_F1_score: 0.7296  Mult_acc_5: 0.4057  Mult_acc_7: 0.4057  MAE: 0.7934  Corr: 0.1629  Loss: 1.0994 
2021-02-02 01:48:38:INFO:TRAIN-(misa) (4/6/1)>> loss: 1.2383  Has0_acc_2: 0.6471  Has0_F1_score: 0.6402  Non0_acc_2: 0.6501  Non0_F1_score: 0.6550  Mult_acc_5: 0.4166  Mult_acc_7: 0.4166  MAE: 0.8153  Corr: 0.3077 
2021-02-02 01:48:44:INFO:VAL-(misa) >>  Has0_acc_2: 0.6638  Has0_F1_score: 0.6818  Non0_acc_2: 0.6280  Non0_F1_score: 0.6620  Mult_acc_5: 0.4158  Mult_acc_7: 0.4158  MAE: 0.7917  Corr: 0.1516  Loss: 1.0921 
2021-02-02 01:51:04:INFO:TRAIN-(misa) (5/7/1)>> loss: 1.2189  Has0_acc_2: 0.6514  Has0_F1_score: 0.6442  Non0_acc_2: 0.6574  Non0_F1_score: 0.6620  Mult_acc_5: 0.4134  Mult_acc_7: 0.4134  MAE: 0.8108  Corr: 0.3296 
2021-02-02 01:51:10:INFO:VAL-(misa) >>  Has0_acc_2: 0.6921  Has0_F1_score: 0.7302  Non0_acc_2: 0.6453  Non0_F1_score: 0.7008  Mult_acc_5: 0.3998  Mult_acc_7: 0.3998  MAE: 0.8003  Corr: 0.1642  Loss: 1.1030 
2021-02-02 01:53:31:INFO:TRAIN-(misa) (6/8/1)>> loss: 1.2003  Has0_acc_2: 0.6587  Has0_F1_score: 0.6514  Non0_acc_2: 0.6654  Non0_F1_score: 0.6694  Mult_acc_5: 0.4130  Mult_acc_7: 0.4130  MAE: 0.8061  Corr: 0.3463 
2021-02-02 01:53:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.5542  Has0_F1_score: 0.5294  Non0_acc_2: 0.5744  Non0_F1_score: 0.5651  Mult_acc_5: 0.4142  Mult_acc_7: 0.4142  MAE: 0.8186  Corr: 0.1787  Loss: 1.1211 
2021-02-02 01:55:57:INFO:TRAIN-(misa) (7/9/1)>> loss: 1.1798  Has0_acc_2: 0.6598  Has0_F1_score: 0.6516  Non0_acc_2: 0.6682  Non0_F1_score: 0.6711  Mult_acc_5: 0.4120  Mult_acc_7: 0.4120  MAE: 0.8036  Corr: 0.3620 
2021-02-02 01:56:02:INFO:VAL-(misa) >>  Has0_acc_2: 0.6029  Has0_F1_score: 0.5879  Non0_acc_2: 0.6064  Non0_F1_score: 0.6077  Mult_acc_5: 0.4286  Mult_acc_7: 0.4286  MAE: 0.7973  Corr: 0.1684  Loss: 1.0953 
2021-02-02 01:58:22:INFO:TRAIN-(misa) (8/10/1)>> loss: 1.1475  Has0_acc_2: 0.6673  Has0_F1_score: 0.6603  Non0_acc_2: 0.6746  Non0_F1_score: 0.6785  Mult_acc_5: 0.4157  Mult_acc_7: 0.4157  MAE: 0.7982  Corr: 0.3744 
2021-02-02 01:58:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.6964  Has0_F1_score: 0.7335  Non0_acc_2: 0.6530  Non0_F1_score: 0.7075  Mult_acc_5: 0.3976  Mult_acc_7: 0.3976  MAE: 0.7963  Corr: 0.1827  Loss: 1.1016 
2021-02-02 01:58:41:INFO:TEST-(misa) >>  Has0_acc_2: 0.6954  Has0_F1_score: 0.7735  Non0_acc_2: 0.6291  Non0_F1_score: 0.7268  Mult_acc_5: 0.4166  Mult_acc_7: 0.4166  MAE: 0.8257  Corr: 0.1693  Loss: 1.2044 
2021-02-02 01:58:41:INFO:Start saving results...
2021-02-02 01:58:41:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 01:58:41:INFO:########################################misa-(4/47)########################################
2021-02-02 01:58:41:INFO:batch_size:64
2021-02-02 01:58:41:INFO:learning_rate:0.0001
2021-02-02 01:58:41:INFO:hidden_size:128
2021-02-02 01:58:41:INFO:dropout:0.2
2021-02-02 01:58:41:INFO:reverse_grad_weight:0.5
2021-02-02 01:58:41:INFO:diff_weight:0.5
2021-02-02 01:58:41:INFO:sim_weight:0.5
2021-02-02 01:58:41:INFO:sp_weight:0.0
2021-02-02 01:58:41:INFO:recon_weight:1.0
2021-02-02 01:58:41:INFO:grad_clip:0.8
2021-02-02 01:58:41:INFO:weight_decay:0.002
2021-02-02 01:58:41:INFO:##########################################################################################
2021-02-02 01:58:41:INFO:Start running misa...
2021-02-02 01:58:41:INFO:Find gpu: 2, with memory: 1661796352 left!
2021-02-02 01:58:41:INFO:Let's use 1 GPUs!
2021-02-02 01:58:54:INFO:train samples: (16326,)
2021-02-02 01:59:07:INFO:valid samples: (1871,)
2021-02-02 01:59:20:INFO:test samples: (4659,)
2021-02-02 01:59:21:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 01:59:21:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 01:59:21:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 01:59:21:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 01:59:21:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 01:59:21:INFO:loading file None
2021-02-02 01:59:21:INFO:loading file None
2021-02-02 01:59:21:INFO:loading file None
2021-02-02 01:59:21:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 01:59:21:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 01:59:21:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 01:59:23:INFO:The model has 110917345 trainable parameters
2021-02-02 02:01:19:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.4457  Has0_acc_2: 0.7761  Has0_F1_score: 0.7713  Non0_acc_2: 0.8011  Non0_F1_score: 0.8025  Mult_acc_5: 0.4806  Mult_acc_7: 0.4687  MAE: 0.6423  Corr: 0.6568 
2021-02-02 02:01:24:INFO:VAL-(misa) >>  Has0_acc_2: 0.8226  Has0_F1_score: 0.8212  Non0_acc_2: 0.8345  Non0_F1_score: 0.8375  Mult_acc_5: 0.5077  Mult_acc_7: 0.4944  MAE: 0.5665  Corr: 0.7138  Loss: 0.5546 
2021-02-02 02:03:22:INFO:TRAIN-(misa) (1/2/1)>> loss: 0.6103  Has0_acc_2: 0.8233  Has0_F1_score: 0.8189  Non0_acc_2: 0.8648  Non0_F1_score: 0.8653  Mult_acc_5: 0.5679  Mult_acc_7: 0.5454  MAE: 0.5112  Corr: 0.8057 
2021-02-02 02:03:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.7595  Has0_F1_score: 0.7466  Non0_acc_2: 0.8261  Non0_F1_score: 0.8229  Mult_acc_5: 0.5398  Mult_acc_7: 0.5275  MAE: 0.5539  Corr: 0.7337  Loss: 0.5669 
2021-02-02 02:05:23:INFO:TRAIN-(misa) (2/3/1)>> loss: 0.4070  Has0_acc_2: 0.8417  Has0_F1_score: 0.8374  Non0_acc_2: 0.8921  Non0_F1_score: 0.8924  Mult_acc_5: 0.6215  Mult_acc_7: 0.5955  MAE: 0.4395  Corr: 0.8650 
2021-02-02 02:05:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.7638  Has0_F1_score: 0.7519  Non0_acc_2: 0.8303  Non0_F1_score: 0.8281  Mult_acc_5: 0.5366  Mult_acc_7: 0.5232  MAE: 0.5440  Corr: 0.7266  Loss: 0.5202 
2021-02-02 02:07:23:INFO:TRAIN-(misa) (1/4/1)>> loss: 0.2788  Has0_acc_2: 0.8597  Has0_F1_score: 0.8555  Non0_acc_2: 0.9237  Non0_F1_score: 0.9237  Mult_acc_5: 0.6982  Mult_acc_7: 0.6710  MAE: 0.3532  Corr: 0.9143 
2021-02-02 02:07:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.7777  Has0_F1_score: 0.7665  Non0_acc_2: 0.8401  Non0_F1_score: 0.8378  Mult_acc_5: 0.5190  Mult_acc_7: 0.5008  MAE: 0.5875  Corr: 0.7316  Loss: 0.6035 
2021-02-02 02:09:23:INFO:TRAIN-(misa) (2/5/1)>> loss: 0.2090  Has0_acc_2: 0.8720  Has0_F1_score: 0.8679  Non0_acc_2: 0.9461  Non0_F1_score: 0.9461  Mult_acc_5: 0.7566  Mult_acc_7: 0.7299  MAE: 0.2896  Corr: 0.9436 
2021-02-02 02:09:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.7205  Has0_F1_score: 0.7049  Non0_acc_2: 0.8039  Non0_F1_score: 0.7996  Mult_acc_5: 0.5019  Mult_acc_7: 0.4869  MAE: 0.6167  Corr: 0.7311  Loss: 0.6455 
2021-02-02 02:11:23:INFO:TRAIN-(misa) (3/6/1)>> loss: 0.1906  Has0_acc_2: 0.8755  Has0_F1_score: 0.8714  Non0_acc_2: 0.9517  Non0_F1_score: 0.9517  Mult_acc_5: 0.7866  Mult_acc_7: 0.7590  MAE: 0.2653  Corr: 0.9530 
2021-02-02 02:11:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.8076  Has0_F1_score: 0.8001  Non0_acc_2: 0.8491  Non0_F1_score: 0.8482  Mult_acc_5: 0.5077  Mult_acc_7: 0.4890  MAE: 0.5894  Corr: 0.7317  Loss: 0.5908 
2021-02-02 02:13:23:INFO:TRAIN-(misa) (4/7/1)>> loss: 0.1654  Has0_acc_2: 0.8849  Has0_F1_score: 0.8810  Non0_acc_2: 0.9694  Non0_F1_score: 0.9694  Mult_acc_5: 0.8120  Mult_acc_7: 0.7879  MAE: 0.2365  Corr: 0.9624 
2021-02-02 02:13:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.8252  Has0_F1_score: 0.8219  Non0_acc_2: 0.8477  Non0_F1_score: 0.8492  Mult_acc_5: 0.5275  Mult_acc_7: 0.5094  MAE: 0.5602  Corr: 0.7350  Loss: 0.5380 
2021-02-02 02:15:23:INFO:TRAIN-(misa) (5/8/1)>> loss: 0.1509  Has0_acc_2: 0.8886  Has0_F1_score: 0.8848  Non0_acc_2: 0.9717  Non0_F1_score: 0.9717  Mult_acc_5: 0.8345  Mult_acc_7: 0.8136  MAE: 0.2143  Corr: 0.9693 
2021-02-02 02:15:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.8167  Has0_F1_score: 0.8114  Non0_acc_2: 0.8442  Non0_F1_score: 0.8444  Mult_acc_5: 0.5232  Mult_acc_7: 0.5061  MAE: 0.5579  Corr: 0.7319  Loss: 0.5292 
2021-02-02 02:17:24:INFO:TRAIN-(misa) (6/9/1)>> loss: 0.1412  Has0_acc_2: 0.8891  Has0_F1_score: 0.8853  Non0_acc_2: 0.9776  Non0_F1_score: 0.9776  Mult_acc_5: 0.8491  Mult_acc_7: 0.8269  MAE: 0.2019  Corr: 0.9724 
2021-02-02 02:17:29:INFO:VAL-(misa) >>  Has0_acc_2: 0.8177  Has0_F1_score: 0.8129  Non0_acc_2: 0.8498  Non0_F1_score: 0.8506  Mult_acc_5: 0.5206  Mult_acc_7: 0.5045  MAE: 0.5571  Corr: 0.7332  Loss: 0.5309 
2021-02-02 02:19:25:INFO:TRAIN-(misa) (7/10/1)>> loss: 0.1371  Has0_acc_2: 0.8909  Has0_F1_score: 0.8872  Non0_acc_2: 0.9794  Non0_F1_score: 0.9794  Mult_acc_5: 0.8516  Mult_acc_7: 0.8323  MAE: 0.1948  Corr: 0.9748 
2021-02-02 02:19:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.8391  Has0_F1_score: 0.8387  Non0_acc_2: 0.8449  Non0_F1_score: 0.8478  Mult_acc_5: 0.5077  Mult_acc_7: 0.4901  MAE: 0.5742  Corr: 0.7351  Loss: 0.5564 
2021-02-02 02:21:26:INFO:TRAIN-(misa) (8/11/1)>> loss: 0.1328  Has0_acc_2: 0.8923  Has0_F1_score: 0.8886  Non0_acc_2: 0.9827  Non0_F1_score: 0.9827  Mult_acc_5: 0.8647  Mult_acc_7: 0.8466  MAE: 0.1846  Corr: 0.9771 
2021-02-02 02:21:31:INFO:VAL-(misa) >>  Has0_acc_2: 0.8466  Has0_F1_score: 0.8529  Non0_acc_2: 0.8317  Non0_F1_score: 0.8404  Mult_acc_5: 0.4409  Mult_acc_7: 0.4228  MAE: 0.6624  Corr: 0.7401  Loss: 0.6813 
2021-02-02 02:21:43:INFO:TEST-(misa) >>  Has0_acc_2: 0.7626  Has0_F1_score: 0.7521  Non0_acc_2: 0.8297  Non0_F1_score: 0.8275  Mult_acc_5: 0.5375  Mult_acc_7: 0.5226  MAE: 0.5616  Corr: 0.7479  Loss: 0.5547 
2021-02-02 02:21:43:INFO:Start saving results...
2021-02-02 02:21:43:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 02:21:43:INFO:########################################misa-(5/47)########################################
2021-02-02 02:21:43:INFO:batch_size:64
2021-02-02 02:21:43:INFO:learning_rate:0.0005
2021-02-02 02:21:43:INFO:hidden_size:256
2021-02-02 02:21:43:INFO:dropout:0.0
2021-02-02 02:21:43:INFO:reverse_grad_weight:0.8
2021-02-02 02:21:43:INFO:diff_weight:0.3
2021-02-02 02:21:43:INFO:sim_weight:0.5
2021-02-02 02:21:43:INFO:sp_weight:1.0
2021-02-02 02:21:43:INFO:recon_weight:0.5
2021-02-02 02:21:43:INFO:grad_clip:1.0
2021-02-02 02:21:43:INFO:weight_decay:0.0
2021-02-02 02:21:43:INFO:##########################################################################################
2021-02-02 02:21:43:INFO:Start running misa...
2021-02-02 02:21:43:INFO:Find gpu: 2, with memory: 1661796352 left!
2021-02-02 02:21:43:INFO:Let's use 1 GPUs!
2021-02-02 02:21:56:INFO:train samples: (16326,)
2021-02-02 02:22:08:INFO:valid samples: (1871,)
2021-02-02 02:22:21:INFO:test samples: (4659,)
2021-02-02 02:22:22:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 02:22:22:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 02:22:22:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 02:22:22:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 02:22:22:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 02:22:22:INFO:loading file None
2021-02-02 02:22:22:INFO:loading file None
2021-02-02 02:22:22:INFO:loading file None
2021-02-02 02:22:22:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 02:22:22:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 02:22:22:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 02:22:24:INFO:The model has 113025633 trainable parameters
2021-02-02 02:24:20:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.7436  Has0_acc_2: 0.6481  Has0_F1_score: 0.6852  Non0_acc_2: 0.5966  Non0_F1_score: 0.6497  Mult_acc_5: 0.4062  Mult_acc_7: 0.4052  MAE: 0.8864  Corr: 0.0062 
2021-02-02 02:24:25:INFO:VAL-(misa) >>  Has0_acc_2: 0.6681  Has0_F1_score: 0.6821  Non0_acc_2: 0.6342  Non0_F1_score: 0.6628  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7678  Corr: 0.1916  Loss: 1.0698 
2021-02-02 02:26:24:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.2536  Has0_acc_2: 0.6405  Has0_F1_score: 0.6417  Non0_acc_2: 0.6246  Non0_F1_score: 0.6384  Mult_acc_5: 0.4161  Mult_acc_7: 0.4161  MAE: 0.8324  Corr: 0.2130 
2021-02-02 02:26:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.7071  Has0_F1_score: 0.7673  Non0_acc_2: 0.6537  Non0_F1_score: 0.7355  Mult_acc_5: 0.3998  Mult_acc_7: 0.3998  MAE: 0.8026  Corr: 0.1813  Loss: 1.1316 
2021-02-02 02:28:25:INFO:TRAIN-(misa) (2/3/1)>> loss: 1.2173  Has0_acc_2: 0.6378  Has0_F1_score: 0.6302  Non0_acc_2: 0.6409  Non0_F1_score: 0.6457  Mult_acc_5: 0.4098  Mult_acc_7: 0.4098  MAE: 0.8273  Corr: 0.2678 
2021-02-02 02:28:29:INFO:VAL-(misa) >>  Has0_acc_2: 0.6264  Has0_F1_score: 0.6183  Non0_acc_2: 0.6238  Non0_F1_score: 0.6318  Mult_acc_5: 0.3955  Mult_acc_7: 0.3955  MAE: 0.8043  Corr: 0.1780  Loss: 1.0982 
2021-02-02 02:30:25:INFO:TRAIN-(misa) (3/4/1)>> loss: 1.1498  Has0_acc_2: 0.6715  Has0_F1_score: 0.6679  Non0_acc_2: 0.6753  Non0_F1_score: 0.6829  Mult_acc_5: 0.4096  Mult_acc_7: 0.4096  MAE: 0.8100  Corr: 0.3371 
2021-02-02 02:30:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.6526  Has0_F1_score: 0.6558  Non0_acc_2: 0.6342  Non0_F1_score: 0.6527  Mult_acc_5: 0.3934  Mult_acc_7: 0.3934  MAE: 0.8110  Corr: 0.1688  Loss: 1.1203 
2021-02-02 02:32:25:INFO:TRAIN-(misa) (4/5/1)>> loss: 1.1228  Has0_acc_2: 0.6786  Has0_F1_score: 0.6735  Non0_acc_2: 0.6865  Non0_F1_score: 0.6920  Mult_acc_5: 0.4043  Mult_acc_7: 0.4043  MAE: 0.8039  Corr: 0.3657 
2021-02-02 02:32:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.6360  Has0_F1_score: 0.6302  Non0_acc_2: 0.6224  Non0_F1_score: 0.6310  Mult_acc_5: 0.3854  Mult_acc_7: 0.3854  MAE: 0.8229  Corr: 0.1534  Loss: 1.1497 
2021-02-02 02:34:26:INFO:TRAIN-(misa) (5/6/1)>> loss: 1.0812  Has0_acc_2: 0.6963  Has0_F1_score: 0.6931  Non0_acc_2: 0.7025  Non0_F1_score: 0.7090  Mult_acc_5: 0.4084  Mult_acc_7: 0.4084  MAE: 0.7893  Corr: 0.4064 
2021-02-02 02:34:31:INFO:VAL-(misa) >>  Has0_acc_2: 0.6114  Has0_F1_score: 0.5981  Non0_acc_2: 0.5987  Non0_F1_score: 0.5994  Mult_acc_5: 0.4046  Mult_acc_7: 0.4046  MAE: 0.8165  Corr: 0.1487  Loss: 1.1395 
2021-02-02 02:36:27:INFO:TRAIN-(misa) (6/7/1)>> loss: 1.0613  Has0_acc_2: 0.7081  Has0_F1_score: 0.7051  Non0_acc_2: 0.7160  Non0_F1_score: 0.7223  Mult_acc_5: 0.4081  Mult_acc_7: 0.4081  MAE: 0.7818  Corr: 0.4274 
2021-02-02 02:36:32:INFO:VAL-(misa) >>  Has0_acc_2: 0.6825  Has0_F1_score: 0.7067  Non0_acc_2: 0.6426  Non0_F1_score: 0.6820  Mult_acc_5: 0.3886  Mult_acc_7: 0.3886  MAE: 0.8107  Corr: 0.1621  Loss: 1.1229 
2021-02-02 02:38:28:INFO:TRAIN-(misa) (7/8/1)>> loss: 1.0321  Has0_acc_2: 0.7091  Has0_F1_score: 0.7059  Non0_acc_2: 0.7186  Non0_F1_score: 0.7247  Mult_acc_5: 0.4098  Mult_acc_7: 0.4096  MAE: 0.7743  Corr: 0.4490 
2021-02-02 02:38:33:INFO:VAL-(misa) >>  Has0_acc_2: 0.6537  Has0_F1_score: 0.6528  Non0_acc_2: 0.6377  Non0_F1_score: 0.6510  Mult_acc_5: 0.3886  Mult_acc_7: 0.3886  MAE: 0.8211  Corr: 0.1664  Loss: 1.1550 
2021-02-02 02:40:29:INFO:TRAIN-(misa) (8/9/1)>> loss: 1.0131  Has0_acc_2: 0.7201  Has0_F1_score: 0.7197  Non0_acc_2: 0.7249  Non0_F1_score: 0.7332  Mult_acc_5: 0.4109  Mult_acc_7: 0.4104  MAE: 0.7661  Corr: 0.4651 
2021-02-02 02:40:34:INFO:VAL-(misa) >>  Has0_acc_2: 0.6606  Has0_F1_score: 0.6620  Non0_acc_2: 0.6419  Non0_F1_score: 0.6570  Mult_acc_5: 0.4441  Mult_acc_7: 0.4441  MAE: 0.7775  Corr: 0.1717  Loss: 1.0642 
2021-02-02 02:42:31:INFO:TRAIN-(misa) (1/10/1)>> loss: 0.9762  Has0_acc_2: 0.7297  Has0_F1_score: 0.7284  Non0_acc_2: 0.7391  Non0_F1_score: 0.7460  Mult_acc_5: 0.4140  Mult_acc_7: 0.4133  MAE: 0.7560  Corr: 0.4879 
2021-02-02 02:42:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.6553  Has0_F1_score: 0.6512  Non0_acc_2: 0.6356  Non0_F1_score: 0.6437  Mult_acc_5: 0.4099  Mult_acc_7: 0.4099  MAE: 0.8130  Corr: 0.1781  Loss: 1.1507 
2021-02-02 02:44:32:INFO:TRAIN-(misa) (2/11/1)>> loss: 0.9556  Has0_acc_2: 0.7351  Has0_F1_score: 0.7338  Non0_acc_2: 0.7462  Non0_F1_score: 0.7529  Mult_acc_5: 0.4133  Mult_acc_7: 0.4125  MAE: 0.7457  Corr: 0.5082 
2021-02-02 02:44:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.6446  Has0_F1_score: 0.6373  Non0_acc_2: 0.6300  Non0_F1_score: 0.6357  Mult_acc_5: 0.4083  Mult_acc_7: 0.4083  MAE: 0.8125  Corr: 0.1692  Loss: 1.1415 
2021-02-02 02:46:32:INFO:TRAIN-(misa) (3/12/1)>> loss: 0.9300  Has0_acc_2: 0.7395  Has0_F1_score: 0.7378  Non0_acc_2: 0.7514  Non0_F1_score: 0.7575  Mult_acc_5: 0.4188  Mult_acc_7: 0.4177  MAE: 0.7393  Corr: 0.5237 
2021-02-02 02:46:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.4885  Has0_F1_score: 0.4643  Non0_acc_2: 0.5188  Non0_F1_score: 0.5105  Mult_acc_5: 0.3421  Mult_acc_7: 0.3415  MAE: 0.9355  Corr: 0.1644  Loss: 1.4403 
2021-02-02 02:48:33:INFO:TRAIN-(misa) (4/13/1)>> loss: 0.9127  Has0_acc_2: 0.7355  Has0_F1_score: 0.7331  Non0_acc_2: 0.7502  Non0_F1_score: 0.7559  Mult_acc_5: 0.4206  Mult_acc_7: 0.4182  MAE: 0.7332  Corr: 0.5377 
2021-02-02 02:48:37:INFO:VAL-(misa) >>  Has0_acc_2: 0.5526  Has0_F1_score: 0.5277  Non0_acc_2: 0.5675  Non0_F1_score: 0.5578  Mult_acc_5: 0.4137  Mult_acc_7: 0.4137  MAE: 0.8272  Corr: 0.1624  Loss: 1.1792 
2021-02-02 02:50:34:INFO:TRAIN-(misa) (5/14/1)>> loss: 0.8795  Has0_acc_2: 0.7476  Has0_F1_score: 0.7447  Non0_acc_2: 0.7651  Non0_F1_score: 0.7697  Mult_acc_5: 0.4280  Mult_acc_7: 0.4247  MAE: 0.7192  Corr: 0.5574 
2021-02-02 02:50:38:INFO:VAL-(misa) >>  Has0_acc_2: 0.6424  Has0_F1_score: 0.6373  Non0_acc_2: 0.6280  Non0_F1_score: 0.6366  Mult_acc_5: 0.3805  Mult_acc_7: 0.3800  MAE: 0.8448  Corr: 0.1748  Loss: 1.2067 
2021-02-02 02:52:34:INFO:TRAIN-(misa) (6/15/1)>> loss: 0.8676  Has0_acc_2: 0.7467  Has0_F1_score: 0.7439  Non0_acc_2: 0.7647  Non0_F1_score: 0.7696  Mult_acc_5: 0.4301  Mult_acc_7: 0.4275  MAE: 0.7151  Corr: 0.5684 
2021-02-02 02:52:39:INFO:VAL-(misa) >>  Has0_acc_2: 0.5842  Has0_F1_score: 0.5625  Non0_acc_2: 0.5848  Non0_F1_score: 0.5774  Mult_acc_5: 0.3672  Mult_acc_7: 0.3656  MAE: 0.8916  Corr: 0.1683  Loss: 1.3279 
2021-02-02 02:54:35:INFO:TRAIN-(misa) (7/16/1)>> loss: 0.8398  Has0_acc_2: 0.7540  Has0_F1_score: 0.7509  Non0_acc_2: 0.7754  Non0_F1_score: 0.7797  Mult_acc_5: 0.4356  Mult_acc_7: 0.4300  MAE: 0.7034  Corr: 0.5888 
2021-02-02 02:54:39:INFO:VAL-(misa) >>  Has0_acc_2: 0.5751  Has0_F1_score: 0.5523  Non0_acc_2: 0.5883  Non0_F1_score: 0.5809  Mult_acc_5: 0.3779  Mult_acc_7: 0.3757  MAE: 0.8857  Corr: 0.1650  Loss: 1.3193 
2021-02-02 02:56:35:INFO:TRAIN-(misa) (8/17/1)>> loss: 0.8058  Has0_acc_2: 0.7565  Has0_F1_score: 0.7530  Non0_acc_2: 0.7798  Non0_F1_score: 0.7837  Mult_acc_5: 0.4422  Mult_acc_7: 0.4357  MAE: 0.6915  Corr: 0.6068 
2021-02-02 02:56:40:INFO:VAL-(misa) >>  Has0_acc_2: 0.6761  Has0_F1_score: 0.6884  Non0_acc_2: 0.6433  Non0_F1_score: 0.6689  Mult_acc_5: 0.3645  Mult_acc_7: 0.3640  MAE: 0.8585  Corr: 0.1775  Loss: 1.2277 
2021-02-02 02:56:52:INFO:TEST-(misa) >>  Has0_acc_2: 0.6527  Has0_F1_score: 0.6584  Non0_acc_2: 0.6379  Non0_F1_score: 0.6571  Mult_acc_5: 0.4072  Mult_acc_7: 0.4072  MAE: 0.8271  Corr: 0.2364  Loss: 1.1695 
2021-02-02 02:56:52:INFO:Start saving results...
2021-02-02 02:56:52:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 02:56:52:INFO:########################################misa-(6/47)########################################
2021-02-02 02:56:52:INFO:batch_size:64
2021-02-02 02:56:52:INFO:learning_rate:0.001
2021-02-02 02:56:52:INFO:hidden_size:128
2021-02-02 02:56:52:INFO:dropout:0.2
2021-02-02 02:56:52:INFO:reverse_grad_weight:1.0
2021-02-02 02:56:52:INFO:diff_weight:0.5
2021-02-02 02:56:52:INFO:sim_weight:0.5
2021-02-02 02:56:52:INFO:sp_weight:1.0
2021-02-02 02:56:52:INFO:recon_weight:1.0
2021-02-02 02:56:52:INFO:grad_clip:0.8
2021-02-02 02:56:52:INFO:weight_decay:5e-05
2021-02-02 02:56:52:INFO:##########################################################################################
2021-02-02 02:56:52:INFO:Start running misa...
2021-02-02 02:56:52:INFO:Find gpu: 2, with memory: 1661796352 left!
2021-02-02 02:56:52:INFO:Let's use 1 GPUs!
2021-02-02 02:57:05:INFO:train samples: (16326,)
2021-02-02 02:57:18:INFO:valid samples: (1871,)
2021-02-02 02:57:32:INFO:test samples: (4659,)
2021-02-02 02:57:32:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 02:57:32:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 02:57:32:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 02:57:32:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 02:57:32:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 02:57:32:INFO:loading file None
2021-02-02 02:57:32:INFO:loading file None
2021-02-02 02:57:32:INFO:loading file None
2021-02-02 02:57:32:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 02:57:32:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 02:57:32:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 02:57:34:INFO:The model has 110917345 trainable parameters
2021-02-02 02:59:30:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.5516  Has0_acc_2: 0.6841  Has0_F1_score: 0.7650  Non0_acc_2: 0.6143  Non0_F1_score: 0.7149  Mult_acc_5: 0.4107  Mult_acc_7: 0.4104  MAE: 0.8614  Corr: 0.0082 
2021-02-02 02:59:35:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: 0.0533  Loss: 1.1052 
2021-02-02 03:01:33:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.3448  Has0_acc_2: 0.7089  Has0_F1_score: 0.8277  Non0_acc_2: 0.6289  Non0_F1_score: 0.7703  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8477  Corr: 0.0077 
2021-02-02 03:01:38:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: -0.0444  Loss: 1.0894 
2021-02-02 03:03:35:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.2959  Has0_acc_2: 0.7083  Has0_F1_score: 0.8260  Non0_acc_2: 0.6290  Non0_F1_score: 0.7693  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8483  Corr: -0.0036 
2021-02-02 03:03:40:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: 0.0217  Loss: 1.0943 
2021-02-02 03:05:34:INFO:TRAIN-(misa) (2/4/1)>> loss: nan  Has0_acc_2: 0.3805  Has0_F1_score: 0.4049  Non0_acc_2: 0.4280  Non0_F1_score: 0.4667  Mult_acc_5: 0.0858  Mult_acc_7: 0.0858  MAE: nan  Corr: nan 
2021-02-02 03:05:39:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:07:33:INFO:TRAIN-(misa) (3/5/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:07:38:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:09:32:INFO:TRAIN-(misa) (4/6/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:09:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:11:31:INFO:TRAIN-(misa) (5/7/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:11:35:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:13:29:INFO:TRAIN-(misa) (6/8/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:13:34:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:15:28:INFO:TRAIN-(misa) (7/9/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:15:33:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:17:27:INFO:TRAIN-(misa) (8/10/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:17:32:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:17:43:INFO:TEST-(misa) >>  Has0_acc_2: 0.7102  Has0_F1_score: 0.8306  Non0_acc_2: 0.6285  Non0_F1_score: 0.7719  Mult_acc_5: 0.4136  Mult_acc_7: 0.4136  MAE: 0.8412  Corr: -0.0022  Loss: 1.2291 
2021-02-02 03:17:43:INFO:Start saving results...
2021-02-02 03:17:43:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 03:17:43:INFO:########################################misa-(7/47)########################################
2021-02-02 03:17:43:INFO:batch_size:16
2021-02-02 03:17:43:INFO:learning_rate:0.0005
2021-02-02 03:17:43:INFO:hidden_size:128
2021-02-02 03:17:43:INFO:dropout:0.0
2021-02-02 03:17:43:INFO:reverse_grad_weight:0.8
2021-02-02 03:17:43:INFO:diff_weight:0.3
2021-02-02 03:17:43:INFO:sim_weight:1.0
2021-02-02 03:17:43:INFO:sp_weight:1.0
2021-02-02 03:17:43:INFO:recon_weight:1.0
2021-02-02 03:17:43:INFO:grad_clip:-1.0
2021-02-02 03:17:43:INFO:weight_decay:0.0
2021-02-02 03:17:43:INFO:##########################################################################################
2021-02-02 03:17:43:INFO:Start running misa...
2021-02-02 03:17:44:INFO:Find gpu: 2, with memory: 1849491456 left!
2021-02-02 03:17:44:INFO:Let's use 1 GPUs!
2021-02-02 03:17:57:INFO:train samples: (16326,)
2021-02-02 03:18:09:INFO:valid samples: (1871,)
2021-02-02 03:18:23:INFO:test samples: (4659,)
2021-02-02 03:18:23:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 03:18:23:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 03:18:23:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 03:18:23:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 03:18:23:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 03:18:23:INFO:loading file None
2021-02-02 03:18:23:INFO:loading file None
2021-02-02 03:18:23:INFO:loading file None
2021-02-02 03:18:23:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 03:18:23:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 03:18:23:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 03:18:25:INFO:The model has 110917345 trainable parameters
2021-02-02 03:21:24:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.4011  Has0_acc_2: 0.6213  Has0_F1_score: 0.6234  Non0_acc_2: 0.5953  Non0_F1_score: 0.6104  Mult_acc_5: 0.4142  Mult_acc_7: 0.4142  MAE: 0.8490  Corr: 0.1299 
2021-02-02 03:21:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.5345  Has0_F1_score: 0.5087  Non0_acc_2: 0.5619  Non0_F1_score: 0.5521  Mult_acc_5: 0.4468  Mult_acc_7: 0.4468  MAE: 0.7788  Corr: 0.1669  Loss: 1.0722 
2021-02-02 03:24:30:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.2055  Has0_acc_2: 0.6380  Has0_F1_score: 0.6299  Non0_acc_2: 0.6408  Non0_F1_score: 0.6448  Mult_acc_5: 0.4165  Mult_acc_7: 0.4165  MAE: 0.8264  Corr: 0.2638 
2021-02-02 03:24:37:INFO:VAL-(misa) >>  Has0_acc_2: 0.6959  Has0_F1_score: 0.7341  Non0_acc_2: 0.6467  Non0_F1_score: 0.7013  Mult_acc_5: 0.4500  Mult_acc_7: 0.4500  MAE: 0.7705  Corr: 0.1719  Loss: 1.0525 
2021-02-02 03:27:37:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.1562  Has0_acc_2: 0.6631  Has0_F1_score: 0.6576  Non0_acc_2: 0.6627  Non0_F1_score: 0.6680  Mult_acc_5: 0.4131  Mult_acc_7: 0.4131  MAE: 0.8153  Corr: 0.3185 
2021-02-02 03:27:44:INFO:VAL-(misa) >>  Has0_acc_2: 0.6205  Has0_F1_score: 0.6105  Non0_acc_2: 0.6050  Non0_F1_score: 0.6090  Mult_acc_5: 0.4180  Mult_acc_7: 0.4180  MAE: 0.8037  Corr: 0.1652  Loss: 1.1057 
2021-02-02 03:30:42:INFO:TRAIN-(misa) (2/4/1)>> loss: 1.1161  Has0_acc_2: 0.6821  Has0_F1_score: 0.6791  Non0_acc_2: 0.6844  Non0_F1_score: 0.6918  Mult_acc_5: 0.4104  Mult_acc_7: 0.4104  MAE: 0.8029  Corr: 0.3604 
2021-02-02 03:30:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.6489  Has0_F1_score: 0.6498  Non0_acc_2: 0.6335  Non0_F1_score: 0.6500  Mult_acc_5: 0.4196  Mult_acc_7: 0.4196  MAE: 0.7927  Corr: 0.1911  Loss: 1.0972 
2021-02-02 03:33:43:INFO:TRAIN-(misa) (3/5/1)>> loss: 1.0723  Has0_acc_2: 0.6974  Has0_F1_score: 0.6946  Non0_acc_2: 0.7019  Non0_F1_score: 0.7088  Mult_acc_5: 0.4120  Mult_acc_7: 0.4120  MAE: 0.7892  Corr: 0.4008 
2021-02-02 03:33:49:INFO:VAL-(misa) >>  Has0_acc_2: 0.6649  Has0_F1_score: 0.6700  Non0_acc_2: 0.6419  Non0_F1_score: 0.6608  Mult_acc_5: 0.4265  Mult_acc_7: 0.4265  MAE: 0.7919  Corr: 0.1981  Loss: 1.0740 
2021-02-02 03:36:39:INFO:TRAIN-(misa) (4/6/1)>> loss: 1.0462  Has0_acc_2: 0.7042  Has0_F1_score: 0.7012  Non0_acc_2: 0.7115  Non0_F1_score: 0.7179  Mult_acc_5: 0.4129  Mult_acc_7: 0.4128  MAE: 0.7813  Corr: 0.4234 
2021-02-02 03:36:45:INFO:VAL-(misa) >>  Has0_acc_2: 0.6510  Has0_F1_score: 0.6511  Non0_acc_2: 0.6356  Non0_F1_score: 0.6506  Mult_acc_5: 0.3993  Mult_acc_7: 0.3993  MAE: 0.8071  Corr: 0.1975  Loss: 1.1144 
2021-02-02 03:39:35:INFO:TRAIN-(misa) (5/7/1)>> loss: nan  Has0_acc_2: 0.4528  Has0_F1_score: 0.4604  Non0_acc_2: 0.5096  Non0_F1_score: 0.5292  Mult_acc_5: 0.1588  Mult_acc_7: 0.1588  MAE: nan  Corr: nan 
2021-02-02 03:39:41:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:42:31:INFO:TRAIN-(misa) (6/8/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:42:37:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:45:28:INFO:TRAIN-(misa) (7/9/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:45:33:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:48:24:INFO:TRAIN-(misa) (8/10/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 03:48:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 03:48:45:INFO:TEST-(misa) >>  Has0_acc_2: 0.6922  Has0_F1_score: 0.7352  Non0_acc_2: 0.6456  Non0_F1_score: 0.7038  Mult_acc_5: 0.4134  Mult_acc_7: 0.4134  MAE: 0.8230  Corr: 0.2289  Loss: 1.1791 
2021-02-02 03:48:45:INFO:Start saving results...
2021-02-02 03:48:45:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 03:48:45:INFO:########################################misa-(8/47)########################################
2021-02-02 03:48:45:INFO:batch_size:32
2021-02-02 03:48:45:INFO:learning_rate:0.0005
2021-02-02 03:48:45:INFO:hidden_size:64
2021-02-02 03:48:45:INFO:dropout:0.5
2021-02-02 03:48:45:INFO:reverse_grad_weight:1.0
2021-02-02 03:48:45:INFO:diff_weight:0.1
2021-02-02 03:48:45:INFO:sim_weight:1.0
2021-02-02 03:48:45:INFO:sp_weight:0.0
2021-02-02 03:48:45:INFO:recon_weight:1.0
2021-02-02 03:48:45:INFO:grad_clip:1.0
2021-02-02 03:48:45:INFO:weight_decay:0.002
2021-02-02 03:48:45:INFO:##########################################################################################
2021-02-02 03:48:45:INFO:Start running misa...
2021-02-02 03:48:45:INFO:Find gpu: 2, with memory: 2674720768 left!
2021-02-02 03:48:45:INFO:Let's use 1 GPUs!
2021-02-02 03:48:57:INFO:train samples: (16326,)
2021-02-02 03:49:08:INFO:valid samples: (1871,)
2021-02-02 03:49:19:INFO:test samples: (4659,)
2021-02-02 03:49:20:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 03:49:20:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 03:49:20:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 03:49:20:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 03:49:20:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 03:49:20:INFO:loading file None
2021-02-02 03:49:20:INFO:loading file None
2021-02-02 03:49:20:INFO:loading file None
2021-02-02 03:49:20:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 03:49:20:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 03:49:20:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 03:49:22:INFO:The model has 110219553 trainable parameters
2021-02-02 03:51:37:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.5036  Has0_acc_2: 0.6749  Has0_F1_score: 0.7456  Non0_acc_2: 0.6094  Non0_F1_score: 0.6994  Mult_acc_5: 0.4150  Mult_acc_7: 0.4150  MAE: 0.8516  Corr: 0.0027 
2021-02-02 03:51:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: 0.1005  Loss: 1.0726 
2021-02-02 03:53:58:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.3007  Has0_acc_2: 0.7095  Has0_F1_score: 0.8299  Non0_acc_2: 0.6291  Non0_F1_score: 0.7721  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8489  Corr: -0.0095 
2021-02-02 03:54:03:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: -0.0057  Loss: 1.0842 
2021-02-02 03:56:17:INFO:TRAIN-(misa) (2/3/1)>> loss: 1.2924  Has0_acc_2: 0.7068  Has0_F1_score: 0.8210  Non0_acc_2: 0.6284  Non0_F1_score: 0.7649  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8486  Corr: -0.0096 
2021-02-02 03:56:22:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: 0.0060  Loss: 1.0760 
2021-02-02 03:58:36:INFO:TRAIN-(misa) (3/4/1)>> loss: 1.2800  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8487  Corr: -0.0047 
2021-02-02 03:58:41:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: -0.0237  Loss: 1.0847 
2021-02-02 04:00:55:INFO:TRAIN-(misa) (4/5/1)>> loss: 1.2768  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8485  Corr: -0.0001 
2021-02-02 04:01:00:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: -0.0128  Loss: 1.0763 
2021-02-02 04:03:14:INFO:TRAIN-(misa) (5/6/1)>> loss: 1.2741  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8486  Corr: -0.0117 
2021-02-02 04:03:19:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: -0.0918  Loss: 1.0820 
2021-02-02 04:05:34:INFO:TRAIN-(misa) (6/7/1)>> loss: 1.2721  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8482  Corr: 0.0103 
2021-02-02 04:05:39:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: -0.0324  Loss: 1.0735 
2021-02-02 04:07:53:INFO:TRAIN-(misa) (7/8/1)>> loss: 1.2711  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8487  Corr: -0.0056 
2021-02-02 04:07:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: -0.0163  Loss: 1.0753 
2021-02-02 04:10:12:INFO:TRAIN-(misa) (8/9/1)>> loss: 1.2708  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8484  Corr: 0.0022 
2021-02-02 04:10:17:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7778  Corr: -0.0397  Loss: 1.0779 
2021-02-02 04:10:30:INFO:TEST-(misa) >>  Has0_acc_2: 0.7102  Has0_F1_score: 0.8306  Non0_acc_2: 0.6285  Non0_F1_score: 0.7719  Mult_acc_5: 0.4136  Mult_acc_7: 0.4136  MAE: 0.8414  Corr: 0.0568  Loss: 1.2305 
2021-02-02 04:10:30:INFO:Start saving results...
2021-02-02 04:10:30:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 04:10:30:INFO:########################################misa-(9/47)########################################
2021-02-02 04:10:30:INFO:batch_size:32
2021-02-02 04:10:30:INFO:learning_rate:0.0001
2021-02-02 04:10:30:INFO:hidden_size:64
2021-02-02 04:10:30:INFO:dropout:0.2
2021-02-02 04:10:30:INFO:reverse_grad_weight:1.0
2021-02-02 04:10:30:INFO:diff_weight:0.5
2021-02-02 04:10:30:INFO:sim_weight:0.8
2021-02-02 04:10:30:INFO:sp_weight:1.0
2021-02-02 04:10:30:INFO:recon_weight:1.0
2021-02-02 04:10:30:INFO:grad_clip:0.8
2021-02-02 04:10:30:INFO:weight_decay:5e-05
2021-02-02 04:10:30:INFO:##########################################################################################
2021-02-02 04:10:30:INFO:Start running misa...
2021-02-02 04:10:30:INFO:Find gpu: 2, with memory: 2674720768 left!
2021-02-02 04:10:30:INFO:Let's use 1 GPUs!
2021-02-02 04:10:42:INFO:train samples: (16326,)
2021-02-02 04:10:53:INFO:valid samples: (1871,)
2021-02-02 04:11:04:INFO:test samples: (4659,)
2021-02-02 04:11:05:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 04:11:05:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 04:11:05:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 04:11:05:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 04:11:05:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 04:11:05:INFO:loading file None
2021-02-02 04:11:05:INFO:loading file None
2021-02-02 04:11:05:INFO:loading file None
2021-02-02 04:11:05:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 04:11:05:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 04:11:05:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 04:11:06:INFO:The model has 110219553 trainable parameters
2021-02-02 04:13:21:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.9472  Has0_acc_2: 0.6381  Has0_F1_score: 0.6572  Non0_acc_2: 0.6024  Non0_F1_score: 0.6367  Mult_acc_5: 0.4148  Mult_acc_7: 0.4148  MAE: 0.8445  Corr: 0.1163 
2021-02-02 04:13:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.6713  Has0_F1_score: 0.6851  Non0_acc_2: 0.6419  Non0_F1_score: 0.6708  Mult_acc_5: 0.4479  Mult_acc_7: 0.4479  MAE: 0.7631  Corr: 0.1910  Loss: 1.0477 
2021-02-02 04:15:43:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.3404  Has0_acc_2: 0.6342  Has0_F1_score: 0.6443  Non0_acc_2: 0.6075  Non0_F1_score: 0.6319  Mult_acc_5: 0.4185  Mult_acc_7: 0.4185  MAE: 0.8361  Corr: 0.1771 
2021-02-02 04:15:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.7055  Has0_F1_score: 0.7595  Non0_acc_2: 0.6488  Non0_F1_score: 0.7213  Mult_acc_5: 0.4452  Mult_acc_7: 0.4452  MAE: 0.7606  Corr: 0.1854  Loss: 1.0449 
2021-02-02 04:18:03:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.3010  Has0_acc_2: 0.6318  Has0_F1_score: 0.6300  Non0_acc_2: 0.6170  Non0_F1_score: 0.6277  Mult_acc_5: 0.4197  Mult_acc_7: 0.4197  MAE: 0.8305  Corr: 0.2179 
2021-02-02 04:18:08:INFO:VAL-(misa) >>  Has0_acc_2: 0.3677  Has0_F1_score: 0.4050  Non0_acc_2: 0.4395  Non0_F1_score: 0.4867  Mult_acc_5: 0.4452  Mult_acc_7: 0.4452  MAE: 0.8269  Corr: 0.1601  Loss: 1.1380 
2021-02-02 04:20:23:INFO:TRAIN-(misa) (2/4/1)>> loss: 1.2685  Has0_acc_2: 0.6343  Has0_F1_score: 0.6270  Non0_acc_2: 0.6348  Non0_F1_score: 0.6400  Mult_acc_5: 0.4202  Mult_acc_7: 0.4202  MAE: 0.8244  Corr: 0.2621 
2021-02-02 04:20:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.7189  Has0_F1_score: 0.8021  Non0_acc_2: 0.6495  Non0_F1_score: 0.7548  Mult_acc_5: 0.4452  Mult_acc_7: 0.4452  MAE: 0.7680  Corr: 0.1563  Loss: 1.0609 
2021-02-02 04:22:43:INFO:TRAIN-(misa) (3/5/1)>> loss: 1.2540  Has0_acc_2: 0.6430  Has0_F1_score: 0.6372  Non0_acc_2: 0.6409  Non0_F1_score: 0.6471  Mult_acc_5: 0.4183  Mult_acc_7: 0.4183  MAE: 0.8189  Corr: 0.2845 
2021-02-02 04:22:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.7055  Has0_F1_score: 0.7718  Non0_acc_2: 0.6453  Non0_F1_score: 0.7333  Mult_acc_5: 0.3982  Mult_acc_7: 0.3982  MAE: 0.7958  Corr: 0.1605  Loss: 1.1059 
2021-02-02 04:25:02:INFO:TRAIN-(misa) (4/6/1)>> loss: 1.2324  Has0_acc_2: 0.6448  Has0_F1_score: 0.6373  Non0_acc_2: 0.6479  Non0_F1_score: 0.6523  Mult_acc_5: 0.4146  Mult_acc_7: 0.4146  MAE: 0.8147  Corr: 0.3110 
2021-02-02 04:25:07:INFO:VAL-(misa) >>  Has0_acc_2: 0.6483  Has0_F1_score: 0.6564  Non0_acc_2: 0.6217  Non0_F1_score: 0.6460  Mult_acc_5: 0.4062  Mult_acc_7: 0.4062  MAE: 0.7962  Corr: 0.1507  Loss: 1.0989 
2021-02-02 04:27:22:INFO:TRAIN-(misa) (5/7/1)>> loss: 1.2094  Has0_acc_2: 0.6528  Has0_F1_score: 0.6451  Non0_acc_2: 0.6588  Non0_F1_score: 0.6627  Mult_acc_5: 0.4146  Mult_acc_7: 0.4146  MAE: 0.8092  Corr: 0.3362 
2021-02-02 04:27:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.6884  Has0_F1_score: 0.7211  Non0_acc_2: 0.6453  Non0_F1_score: 0.6949  Mult_acc_5: 0.3944  Mult_acc_7: 0.3944  MAE: 0.7973  Corr: 0.1620  Loss: 1.0977 
2021-02-02 04:29:41:INFO:TRAIN-(misa) (6/8/1)>> loss: 1.1905  Has0_acc_2: 0.6599  Has0_F1_score: 0.6522  Non0_acc_2: 0.6677  Non0_F1_score: 0.6712  Mult_acc_5: 0.4093  Mult_acc_7: 0.4093  MAE: 0.8061  Corr: 0.3494 
2021-02-02 04:29:46:INFO:VAL-(misa) >>  Has0_acc_2: 0.5559  Has0_F1_score: 0.5311  Non0_acc_2: 0.5730  Non0_F1_score: 0.5639  Mult_acc_5: 0.3987  Mult_acc_7: 0.3987  MAE: 0.8272  Corr: 0.1731  Loss: 1.1341 
2021-02-02 04:32:01:INFO:TRAIN-(misa) (7/9/1)>> loss: 1.1590  Has0_acc_2: 0.6629  Has0_F1_score: 0.6550  Non0_acc_2: 0.6715  Non0_F1_score: 0.6746  Mult_acc_5: 0.4123  Mult_acc_7: 0.4123  MAE: 0.8022  Corr: 0.3653 
2021-02-02 04:32:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.5714  Has0_F1_score: 0.5491  Non0_acc_2: 0.5869  Non0_F1_score: 0.5811  Mult_acc_5: 0.4254  Mult_acc_7: 0.4254  MAE: 0.8096  Corr: 0.1671  Loss: 1.1117 
2021-02-02 04:34:21:INFO:TRAIN-(misa) (8/10/1)>> loss: 1.1252  Has0_acc_2: 0.6705  Has0_F1_score: 0.6634  Non0_acc_2: 0.6791  Non0_F1_score: 0.6828  Mult_acc_5: 0.4158  Mult_acc_7: 0.4158  MAE: 0.7959  Corr: 0.3807 
2021-02-02 04:34:26:INFO:VAL-(misa) >>  Has0_acc_2: 0.6916  Has0_F1_score: 0.7235  Non0_acc_2: 0.6516  Non0_F1_score: 0.7006  Mult_acc_5: 0.4046  Mult_acc_7: 0.4046  MAE: 0.7937  Corr: 0.1805  Loss: 1.0956 
2021-02-02 04:34:38:INFO:TEST-(misa) >>  Has0_acc_2: 0.6914  Has0_F1_score: 0.7577  Non0_acc_2: 0.6299  Non0_F1_score: 0.7145  Mult_acc_5: 0.4145  Mult_acc_7: 0.4145  MAE: 0.8253  Corr: 0.1753  Loss: 1.2032 
2021-02-02 04:34:38:INFO:Start saving results...
2021-02-02 04:34:38:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 04:34:38:INFO:########################################misa-(10/47)########################################
2021-02-02 04:34:38:INFO:batch_size:32
2021-02-02 04:34:38:INFO:learning_rate:0.001
2021-02-02 04:34:38:INFO:hidden_size:128
2021-02-02 04:34:38:INFO:dropout:0.2
2021-02-02 04:34:38:INFO:reverse_grad_weight:1.0
2021-02-02 04:34:38:INFO:diff_weight:0.3
2021-02-02 04:34:38:INFO:sim_weight:1.0
2021-02-02 04:34:38:INFO:sp_weight:0.0
2021-02-02 04:34:38:INFO:recon_weight:0.8
2021-02-02 04:34:38:INFO:grad_clip:1.0
2021-02-02 04:34:38:INFO:weight_decay:5e-05
2021-02-02 04:34:38:INFO:##########################################################################################
2021-02-02 04:34:38:INFO:Start running misa...
2021-02-02 04:34:38:INFO:Find gpu: 2, with memory: 2674720768 left!
2021-02-02 04:34:38:INFO:Let's use 1 GPUs!
2021-02-02 04:34:51:INFO:train samples: (16326,)
2021-02-02 04:35:03:INFO:valid samples: (1871,)
2021-02-02 04:35:15:INFO:test samples: (4659,)
2021-02-02 04:35:15:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 04:35:15:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 04:35:15:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 04:35:15:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 04:35:15:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 04:35:15:INFO:loading file None
2021-02-02 04:35:15:INFO:loading file None
2021-02-02 04:35:15:INFO:loading file None
2021-02-02 04:35:15:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 04:35:15:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 04:35:15:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 04:35:17:INFO:The model has 110917345 trainable parameters
2021-02-02 04:37:32:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.4916  Has0_acc_2: 0.6643  Has0_F1_score: 0.7218  Non0_acc_2: 0.6021  Non0_F1_score: 0.6769  Mult_acc_5: 0.4120  Mult_acc_7: 0.4119  MAE: 0.8615  Corr: 0.0118 
2021-02-02 04:37:37:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7773  Corr: 0.0658  Loss: 1.1085 
2021-02-02 04:39:53:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.2942  Has0_acc_2: 0.7078  Has0_F1_score: 0.8251  Non0_acc_2: 0.6280  Non0_F1_score: 0.7676  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8480  Corr: -0.0012 
2021-02-02 04:39:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7776  Corr: 0.0560  Loss: 1.0861 
2021-02-02 04:42:14:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.2819  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8481  Corr: -0.0087 
2021-02-02 04:42:19:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7776  Corr: -0.0616  Loss: 1.0915 
2021-02-02 04:44:33:INFO:TRAIN-(misa) (2/4/1)>> loss: 1.2745  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8479  Corr: 0.0046 
2021-02-02 04:44:38:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7781  Corr: -0.0002  Loss: 1.0725 
2021-02-02 04:46:54:INFO:TRAIN-(misa) (1/5/1)>> loss: 1.2702  Has0_acc_2: 0.7098  Has0_F1_score: 0.8303  Non0_acc_2: 0.6294  Non0_F1_score: 0.7726  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8485  Corr: -0.0046 
2021-02-02 04:46:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: nan  Loss: 1.0772 
2021-02-02 04:49:13:INFO:TRAIN-(misa) (2/6/1)>> loss: nan  Has0_acc_2: 0.6861  Has0_F1_score: 0.7706  Non0_acc_2: 0.6147  Non0_F1_score: 0.7188  Mult_acc_5: 0.3936  Mult_acc_7: 0.3936  MAE: nan  Corr: nan 
2021-02-02 04:49:18:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 04:51:31:INFO:TRAIN-(misa) (3/7/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 04:51:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 04:53:49:INFO:TRAIN-(misa) (4/8/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 04:53:54:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 04:56:07:INFO:TRAIN-(misa) (5/9/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 04:56:12:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 04:58:25:INFO:TRAIN-(misa) (6/10/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 04:58:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:00:44:INFO:TRAIN-(misa) (7/11/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:00:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:03:02:INFO:TRAIN-(misa) (8/12/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:03:07:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:03:19:INFO:TEST-(misa) >>  Has0_acc_2: 0.7102  Has0_F1_score: 0.8306  Non0_acc_2: 0.6285  Non0_F1_score: 0.7719  Mult_acc_5: 0.4136  Mult_acc_7: 0.4136  MAE: 0.8426  Corr: 0.0062  Loss: 1.2327 
2021-02-02 05:03:19:INFO:Start saving results...
2021-02-02 05:03:19:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 05:03:19:INFO:########################################misa-(11/47)########################################
2021-02-02 05:03:19:INFO:batch_size:16
2021-02-02 05:03:19:INFO:learning_rate:0.001
2021-02-02 05:03:19:INFO:hidden_size:128
2021-02-02 05:03:19:INFO:dropout:0.0
2021-02-02 05:03:19:INFO:reverse_grad_weight:1.0
2021-02-02 05:03:19:INFO:diff_weight:0.3
2021-02-02 05:03:19:INFO:sim_weight:0.8
2021-02-02 05:03:19:INFO:sp_weight:0.0
2021-02-02 05:03:19:INFO:recon_weight:1.0
2021-02-02 05:03:19:INFO:grad_clip:1.0
2021-02-02 05:03:19:INFO:weight_decay:0.0
2021-02-02 05:03:19:INFO:##########################################################################################
2021-02-02 05:03:19:INFO:Start running misa...
2021-02-02 05:03:19:INFO:Find gpu: 2, with memory: 2674720768 left!
2021-02-02 05:03:19:INFO:Let's use 1 GPUs!
2021-02-02 05:03:31:INFO:train samples: (16326,)
2021-02-02 05:03:43:INFO:valid samples: (1871,)
2021-02-02 05:03:55:INFO:test samples: (4659,)
2021-02-02 05:03:56:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 05:03:56:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 05:03:56:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 05:03:56:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 05:03:56:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 05:03:56:INFO:loading file None
2021-02-02 05:03:56:INFO:loading file None
2021-02-02 05:03:56:INFO:loading file None
2021-02-02 05:03:56:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 05:03:56:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 05:03:56:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 05:03:57:INFO:The model has 110917345 trainable parameters
2021-02-02 05:06:52:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.3856  Has0_acc_2: 0.6553  Has0_F1_score: 0.7035  Non0_acc_2: 0.5936  Non0_F1_score: 0.6575  Mult_acc_5: 0.4133  Mult_acc_7: 0.4133  MAE: 0.8587  Corr: -0.0109 
2021-02-02 05:06:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7777  Corr: 0.0138  Loss: 1.0825 
2021-02-02 05:09:54:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.2791  Has0_acc_2: 0.7038  Has0_F1_score: 0.8145  Non0_acc_2: 0.6252  Non0_F1_score: 0.7575  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8491  Corr: -0.0075 
2021-02-02 05:10:00:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7780  Corr: -0.0383  Loss: 1.0776 
2021-02-02 05:12:56:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.2712  Has0_acc_2: 0.7024  Has0_F1_score: 0.8086  Non0_acc_2: 0.6258  Non0_F1_score: 0.7535  Mult_acc_5: 0.4164  Mult_acc_7: 0.4164  MAE: 0.8485  Corr: 0.0068 
2021-02-02 05:13:02:INFO:VAL-(misa) >>  Has0_acc_2: 0.7296  Has0_F1_score: 0.8436  Non0_acc_2: 0.6481  Non0_F1_score: 0.7865  Mult_acc_5: 0.4463  Mult_acc_7: 0.4463  MAE: 0.7774  Corr: 0.0218  Loss: 1.0949 
2021-02-02 05:15:57:INFO:TRAIN-(misa) (2/4/1)>> loss: nan  Has0_acc_2: 0.5519  Has0_F1_score: 0.5369  Non0_acc_2: 0.5320  Non0_F1_score: 0.5306  Mult_acc_5: 0.2571  Mult_acc_7: 0.2571  MAE: nan  Corr: nan 
2021-02-02 05:16:03:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:19:01:INFO:TRAIN-(misa) (3/5/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:19:08:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:22:35:INFO:TRAIN-(misa) (4/6/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:22:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:26:10:INFO:TRAIN-(misa) (5/7/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:26:17:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:29:45:INFO:TRAIN-(misa) (6/8/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:29:52:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:33:20:INFO:TRAIN-(misa) (7/9/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:33:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:36:54:INFO:TRAIN-(misa) (8/10/1)>> loss: nan  Has0_acc_2: 0.2902  Has0_F1_score: 0.4499  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan 
2021-02-02 05:37:01:INFO:VAL-(misa) >>  Has0_acc_2: 0.2704  Has0_F1_score: 0.4257  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Mult_acc_5: 0.0000  Mult_acc_7: 0.0000  MAE: nan  Corr: nan  Loss: nan 
2021-02-02 05:37:20:INFO:TEST-(misa) >>  Has0_acc_2: 0.7102  Has0_F1_score: 0.8306  Non0_acc_2: 0.6285  Non0_F1_score: 0.7719  Mult_acc_5: 0.4136  Mult_acc_7: 0.4136  MAE: 0.8422  Corr: -0.0317  Loss: 1.2317 
2021-02-02 05:37:20:INFO:Start saving results...
2021-02-02 05:37:20:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 05:37:20:INFO:########################################misa-(12/47)########################################
2021-02-02 05:37:20:INFO:batch_size:32
2021-02-02 05:37:20:INFO:learning_rate:0.0005
2021-02-02 05:37:20:INFO:hidden_size:128
2021-02-02 05:37:20:INFO:dropout:0.0
2021-02-02 05:37:20:INFO:reverse_grad_weight:1.0
2021-02-02 05:37:20:INFO:diff_weight:0.1
2021-02-02 05:37:20:INFO:sim_weight:1.0
2021-02-02 05:37:20:INFO:sp_weight:0.0
2021-02-02 05:37:20:INFO:recon_weight:0.5
2021-02-02 05:37:20:INFO:grad_clip:-1.0
2021-02-02 05:37:20:INFO:weight_decay:0.002
2021-02-02 05:37:20:INFO:##########################################################################################
2021-02-02 05:37:20:INFO:Start running misa...
2021-02-02 05:37:20:INFO:Find gpu: 2, with memory: 2743926784 left!
2021-02-02 05:37:20:INFO:Let's use 1 GPUs!
2021-02-02 05:37:32:INFO:train samples: (16326,)
2021-02-02 05:37:44:INFO:valid samples: (1871,)
2021-02-02 05:37:56:INFO:test samples: (4659,)
2021-02-02 05:37:56:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 05:37:56:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 05:37:56:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 05:37:56:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 05:37:56:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 05:37:56:INFO:loading file None
2021-02-02 05:37:56:INFO:loading file None
2021-02-02 05:37:56:INFO:loading file None
2021-02-02 05:37:56:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 05:37:56:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 05:37:56:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 05:37:58:INFO:The model has 110917345 trainable parameters
2021-02-02 05:40:31:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.4567  Has0_acc_2: 0.6206  Has0_F1_score: 0.6203  Non0_acc_2: 0.6006  Non0_F1_score: 0.6134  Mult_acc_5: 0.4114  Mult_acc_7: 0.4114  MAE: 0.8491  Corr: 0.1393 
2021-02-02 05:40:37:INFO:VAL-(misa) >>  Has0_acc_2: 0.6766  Has0_F1_score: 0.6969  Non0_acc_2: 0.6405  Non0_F1_score: 0.6761  Mult_acc_5: 0.4436  Mult_acc_7: 0.4436  MAE: 0.7715  Corr: 0.1859  Loss: 1.0591 
2021-02-02 05:43:12:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.2266  Has0_acc_2: 0.6370  Has0_F1_score: 0.6302  Non0_acc_2: 0.6344  Non0_F1_score: 0.6396  Mult_acc_5: 0.4160  Mult_acc_7: 0.4160  MAE: 0.8265  Corr: 0.2590 
2021-02-02 05:43:17:INFO:VAL-(misa) >>  Has0_acc_2: 0.6724  Has0_F1_score: 0.6918  Non0_acc_2: 0.6335  Non0_F1_score: 0.6680  Mult_acc_5: 0.4148  Mult_acc_7: 0.4148  MAE: 0.7845  Corr: 0.1936  Loss: 1.0714 
2021-02-02 05:45:50:INFO:TRAIN-(misa) (2/3/1)>> loss: 1.1619  Has0_acc_2: 0.6599  Has0_F1_score: 0.6550  Non0_acc_2: 0.6596  Non0_F1_score: 0.6659  Mult_acc_5: 0.4157  Mult_acc_7: 0.4157  MAE: 0.8119  Corr: 0.3239 
2021-02-02 05:45:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.5954  Has0_F1_score: 0.5767  Non0_acc_2: 0.5994  Non0_F1_score: 0.5958  Mult_acc_5: 0.4137  Mult_acc_7: 0.4137  MAE: 0.8077  Corr: 0.1774  Loss: 1.1119 
2021-02-02 05:48:28:INFO:TRAIN-(misa) (3/4/1)>> loss: 1.1253  Has0_acc_2: 0.6791  Has0_F1_score: 0.6750  Non0_acc_2: 0.6832  Non0_F1_score: 0.6897  Mult_acc_5: 0.4142  Mult_acc_7: 0.4142  MAE: 0.8011  Corr: 0.3620 
2021-02-02 05:48:34:INFO:VAL-(misa) >>  Has0_acc_2: 0.6403  Has0_F1_score: 0.6370  Non0_acc_2: 0.6259  Non0_F1_score: 0.6374  Mult_acc_5: 0.3907  Mult_acc_7: 0.3907  MAE: 0.8066  Corr: 0.1907  Loss: 1.0986 
2021-02-02 05:51:05:INFO:TRAIN-(misa) (4/5/1)>> loss: 1.0841  Has0_acc_2: 0.6879  Has0_F1_score: 0.6838  Non0_acc_2: 0.6958  Non0_F1_score: 0.7019  Mult_acc_5: 0.4107  Mult_acc_7: 0.4107  MAE: 0.7900  Corr: 0.4010 
2021-02-02 05:51:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.6392  Has0_F1_score: 0.6368  Non0_acc_2: 0.6259  Non0_F1_score: 0.6388  Mult_acc_5: 0.4228  Mult_acc_7: 0.4228  MAE: 0.8030  Corr: 0.1846  Loss: 1.0972 
2021-02-02 05:53:44:INFO:TRAIN-(misa) (5/6/1)>> loss: 1.0560  Has0_acc_2: 0.6994  Has0_F1_score: 0.6972  Non0_acc_2: 0.7066  Non0_F1_score: 0.7144  Mult_acc_5: 0.4150  Mult_acc_7: 0.4149  MAE: 0.7803  Corr: 0.4271 
2021-02-02 05:53:50:INFO:VAL-(misa) >>  Has0_acc_2: 0.6777  Has0_F1_score: 0.6884  Non0_acc_2: 0.6530  Non0_F1_score: 0.6780  Mult_acc_5: 0.3768  Mult_acc_7: 0.3768  MAE: 0.8056  Corr: 0.2102  Loss: 1.1046 
2021-02-02 05:56:22:INFO:TRAIN-(misa) (6/7/1)>> loss: 1.0182  Has0_acc_2: 0.7141  Has0_F1_score: 0.7115  Non0_acc_2: 0.7252  Non0_F1_score: 0.7318  Mult_acc_5: 0.4153  Mult_acc_7: 0.4153  MAE: 0.7686  Corr: 0.4577 
2021-02-02 05:56:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.6489  Has0_F1_score: 0.6508  Non0_acc_2: 0.6349  Non0_F1_score: 0.6529  Mult_acc_5: 0.3923  Mult_acc_7: 0.3923  MAE: 0.8196  Corr: 0.1859  Loss: 1.1437 
2021-02-02 05:59:00:INFO:TRAIN-(misa) (7/8/1)>> loss: 0.9866  Has0_acc_2: 0.7217  Has0_F1_score: 0.7195  Non0_acc_2: 0.7310  Non0_F1_score: 0.7373  Mult_acc_5: 0.4153  Mult_acc_7: 0.4142  MAE: 0.7586  Corr: 0.4826 
2021-02-02 05:59:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.5997  Has0_F1_score: 0.5806  Non0_acc_2: 0.6106  Non0_F1_score: 0.6068  Mult_acc_5: 0.3805  Mult_acc_7: 0.3805  MAE: 0.8440  Corr: 0.1979  Loss: 1.1937 
2021-02-02 06:01:38:INFO:TRAIN-(misa) (8/9/1)>> loss: 0.9634  Has0_acc_2: 0.7236  Has0_F1_score: 0.7208  Non0_acc_2: 0.7378  Non0_F1_score: 0.7437  Mult_acc_5: 0.4176  Mult_acc_7: 0.4169  MAE: 0.7520  Corr: 0.4988 
2021-02-02 06:01:44:INFO:VAL-(misa) >>  Has0_acc_2: 0.6622  Has0_F1_score: 0.6655  Non0_acc_2: 0.6349  Non0_F1_score: 0.6510  Mult_acc_5: 0.4201  Mult_acc_7: 0.4201  MAE: 0.7991  Corr: 0.1747  Loss: 1.1086 
2021-02-02 06:01:59:INFO:TEST-(misa) >>  Has0_acc_2: 0.6774  Has0_F1_score: 0.7046  Non0_acc_2: 0.6373  Non0_F1_score: 0.6778  Mult_acc_5: 0.4061  Mult_acc_7: 0.4061  MAE: 0.8306  Corr: 0.2045  Loss: 1.1993 
2021-02-02 06:01:59:INFO:Start saving results...
2021-02-02 06:01:59:INFO:Results are saved to results/results/mosei-misa-regression-tune.csv...
2021-02-02 06:01:59:INFO:########################################misa-(13/47)########################################
2021-02-02 06:01:59:INFO:batch_size:64
2021-02-02 06:01:59:INFO:learning_rate:0.001
2021-02-02 06:01:59:INFO:hidden_size:128
2021-02-02 06:01:59:INFO:dropout:0.0
2021-02-02 06:01:59:INFO:reverse_grad_weight:1.0
2021-02-02 06:01:59:INFO:diff_weight:0.1
2021-02-02 06:01:59:INFO:sim_weight:0.8
2021-02-02 06:01:59:INFO:sp_weight:1.0
2021-02-02 06:01:59:INFO:recon_weight:0.8
2021-02-02 06:01:59:INFO:grad_clip:1.0
2021-02-02 06:01:59:INFO:weight_decay:0.0
2021-02-02 06:01:59:INFO:##########################################################################################
2021-02-02 06:01:59:INFO:Start running misa...
2021-02-02 06:01:59:INFO:Find gpu: 2, with memory: 2748121088 left!
2021-02-02 06:01:59:INFO:Let's use 1 GPUs!
2021-02-02 06:02:11:INFO:train samples: (16326,)
2021-02-02 06:02:23:INFO:valid samples: (1871,)
2021-02-02 06:02:34:INFO:test samples: (4659,)
2021-02-02 06:02:35:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 06:02:35:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 06:02:35:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 06:02:35:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 06:02:35:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 06:02:35:INFO:loading file None
2021-02-02 06:02:35:INFO:loading file None
2021-02-02 06:02:35:INFO:loading file None
2021-02-02 06:02:35:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 06:02:35:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 06:02:35:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 06:02:37:INFO:The model has 110917345 trainable parameters
2021-02-02 06:02:38:ERROR:CUDA out of memory. Tried to allocate 38.00 MiB (GPU 2; 7.77 GiB total capacity; 5.03 GiB already allocated; 19.50 MiB free; 156.75 MiB cached)
2021-02-02 08:08:17:INFO:########################################misa-(1/50)########################################
2021-02-02 08:08:17:INFO:batch_size:16
2021-02-02 08:08:17:INFO:learning_rate:0.001
2021-02-02 08:08:17:INFO:hidden_size:256
2021-02-02 08:08:17:INFO:dropout:0.0
2021-02-02 08:08:17:INFO:reverse_grad_weight:0.5
2021-02-02 08:08:17:INFO:diff_weight:0.5
2021-02-02 08:08:17:INFO:sim_weight:1.0
2021-02-02 08:08:17:INFO:sp_weight:0.0
2021-02-02 08:08:17:INFO:recon_weight:0.5
2021-02-02 08:08:17:INFO:grad_clip:0.8
2021-02-02 08:08:17:INFO:weight_decay:0.0
2021-02-02 08:08:17:INFO:##########################################################################################
2021-02-02 08:08:17:INFO:Start running misa...
2021-02-02 08:08:17:INFO:Find gpu: 2, with memory: 3428646912 left!
2021-02-02 08:08:17:INFO:Let's use 1 GPUs!
2021-02-02 08:08:29:INFO:train samples: (16326,)
2021-02-02 08:08:41:INFO:valid samples: (1871,)
2021-02-02 08:08:53:INFO:test samples: (4659,)
2021-02-02 08:08:53:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 08:08:53:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 08:08:53:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 08:08:53:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 08:08:53:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 08:08:53:INFO:loading file None
2021-02-02 08:08:53:INFO:loading file None
2021-02-02 08:08:53:INFO:loading file None
2021-02-02 08:08:53:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 08:08:53:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 08:08:53:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 08:08:55:INFO:The model has 113027171 trainable parameters
2021-02-02 08:12:32:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.1630  Has0_acc_2: 0.4933  Has0_F1_score: 0.6510  Non0_acc_2: 0.0041  Non0_F1_score: 0.0002  Acc_3: 0.4896  F1_score_3: 0.6475 
2021-02-02 08:12:40:INFO:VAL-(misa) >>  Has0_acc_2: 0.4981  Has0_F1_score: 0.6650  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4981  F1_score_3: 0.6650  Loss: 1.0466 
2021-02-02 08:16:18:INFO:TRAIN-(misa) (1/2/1)>> loss: 1.0476  Has0_acc_2: 0.4930  Has0_F1_score: 0.6604  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4930  F1_score_3: 0.6604 
2021-02-02 08:16:26:INFO:VAL-(misa) >>  Has0_acc_2: 0.4981  Has0_F1_score: 0.6650  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4981  F1_score_3: 0.6650  Loss: 1.0401 
2021-02-02 08:20:03:INFO:TRAIN-(misa) (1/3/1)>> loss: 1.0442  Has0_acc_2: 0.4930  Has0_F1_score: 0.6604  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4930  F1_score_3: 0.6604 
2021-02-02 08:20:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.4981  Has0_F1_score: 0.6650  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4981  F1_score_3: 0.6650  Loss: 1.0409 
2021-02-02 08:23:48:INFO:TRAIN-(misa) (2/4/1)>> loss: 1.0413  Has0_acc_2: 0.4930  Has0_F1_score: 0.6604  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4930  F1_score_3: 0.6604 
2021-02-02 08:23:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.4981  Has0_F1_score: 0.6650  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4981  F1_score_3: 0.6650  Loss: 1.0427 
2021-02-02 08:27:31:INFO:TRAIN-(misa) (3/5/1)>> loss: 1.0415  Has0_acc_2: 0.4930  Has0_F1_score: 0.6604  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4930  F1_score_3: 0.6604 
2021-02-02 08:27:39:INFO:VAL-(misa) >>  Has0_acc_2: 0.4981  Has0_F1_score: 0.6650  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4981  F1_score_3: 0.6650  Loss: 1.0408 
2021-02-02 08:31:18:INFO:TRAIN-(misa) (4/6/1)>> loss: nan  Has0_acc_2: 0.5055  Has0_F1_score: 0.5270  Non0_acc_2: 0.2615  Non0_F1_score: 0.3424  Acc_3: 0.3520  F1_score_3: 0.4004 
2021-02-02 08:31:25:INFO:VAL-(misa) >>  Has0_acc_2: 0.5019  Has0_F1_score: 0.6683  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Acc_3: 0.2704  F1_score_3: 0.4257  Loss: nan 
2021-02-02 08:35:04:INFO:TRAIN-(misa) (5/7/1)>> loss: nan  Has0_acc_2: 0.5070  Has0_F1_score: 0.6729  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Acc_3: 0.2902  F1_score_3: 0.4499 
2021-02-02 08:35:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.5019  Has0_F1_score: 0.6683  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Acc_3: 0.2704  F1_score_3: 0.4257  Loss: nan 
2021-02-02 08:38:50:INFO:TRAIN-(misa) (6/8/1)>> loss: nan  Has0_acc_2: 0.5070  Has0_F1_score: 0.6729  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Acc_3: 0.2902  F1_score_3: 0.4499 
2021-02-02 08:38:57:INFO:VAL-(misa) >>  Has0_acc_2: 0.5019  Has0_F1_score: 0.6683  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Acc_3: 0.2704  F1_score_3: 0.4257  Loss: nan 
2021-02-02 08:42:36:INFO:TRAIN-(misa) (7/9/1)>> loss: nan  Has0_acc_2: 0.5070  Has0_F1_score: 0.6729  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Acc_3: 0.2902  F1_score_3: 0.4499 
2021-02-02 08:42:44:INFO:VAL-(misa) >>  Has0_acc_2: 0.5019  Has0_F1_score: 0.6683  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Acc_3: 0.2704  F1_score_3: 0.4257  Loss: nan 
2021-02-02 08:46:22:INFO:TRAIN-(misa) (8/10/1)>> loss: nan  Has0_acc_2: 0.5070  Has0_F1_score: 0.6729  Non0_acc_2: 0.3706  Non0_F1_score: 0.5407  Acc_3: 0.2902  F1_score_3: 0.4499 
2021-02-02 08:46:29:INFO:VAL-(misa) >>  Has0_acc_2: 0.5019  Has0_F1_score: 0.6683  Non0_acc_2: 0.3519  Non0_F1_score: 0.5206  Acc_3: 0.2704  F1_score_3: 0.4257  Loss: nan 
2021-02-02 08:46:49:INFO:TEST-(misa) >>  Has0_acc_2: 0.4902  Has0_F1_score: 0.6579  Non0_acc_2: 0.0000  Non0_F1_score: 0.0000  Acc_3: 0.4902  F1_score_3: 0.6579  Loss: 1.0420 
2021-02-02 08:46:49:INFO:Start saving results...
2021-02-02 08:46:49:INFO:Results are saved to results/results/mosei-misa-classification-tune.csv...
2021-02-02 08:46:49:INFO:########################################misa-(2/50)########################################
2021-02-02 08:46:49:INFO:batch_size:32
2021-02-02 08:46:49:INFO:learning_rate:0.0001
2021-02-02 08:46:49:INFO:hidden_size:128
2021-02-02 08:46:49:INFO:dropout:0.2
2021-02-02 08:46:49:INFO:reverse_grad_weight:1.0
2021-02-02 08:46:49:INFO:diff_weight:0.3
2021-02-02 08:46:49:INFO:sim_weight:0.5
2021-02-02 08:46:49:INFO:sp_weight:1.0
2021-02-02 08:46:49:INFO:recon_weight:0.8
2021-02-02 08:46:49:INFO:grad_clip:0.8
2021-02-02 08:46:49:INFO:weight_decay:0.0
2021-02-02 08:46:49:INFO:##########################################################################################
2021-02-02 08:46:49:INFO:Start running misa...
2021-02-02 08:46:49:INFO:Find gpu: 2, with memory: 3428646912 left!
2021-02-02 08:46:49:INFO:Let's use 1 GPUs!
2021-02-02 08:47:01:INFO:train samples: (16326,)
2021-02-02 08:47:13:INFO:valid samples: (1871,)
2021-02-02 08:47:25:INFO:test samples: (4659,)
2021-02-02 08:47:25:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 08:47:25:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 08:47:25:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 08:47:25:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 08:47:25:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 08:47:25:INFO:loading file None
2021-02-02 08:47:25:INFO:loading file None
2021-02-02 08:47:25:INFO:loading file None
2021-02-02 08:47:25:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 08:47:25:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 08:47:25:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 08:47:27:INFO:The model has 110918115 trainable parameters
2021-02-02 08:50:19:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.1964  Has0_acc_2: 0.6831  Has0_F1_score: 0.6971  Non0_acc_2: 0.2448  Non0_F1_score: 0.2234  Acc_3: 0.6427  F1_score_3: 0.6765 
2021-02-02 08:50:26:INFO:VAL-(misa) >>  Has0_acc_2: 0.7087  Has0_F1_score: 0.7087  Non0_acc_2: 0.3185  Non0_F1_score: 0.3741  Acc_3: 0.6082  F1_score_3: 0.6084  Loss: 0.8683 
2021-02-02 08:53:14:INFO:TRAIN-(misa) (1/2/1)>> loss: 0.6925  Has0_acc_2: 0.7573  Has0_F1_score: 0.7649  Non0_acc_2: 0.3092  Non0_F1_score: 0.2996  Acc_3: 0.7433  F1_score_3: 0.7514 
2021-02-02 08:53:20:INFO:VAL-(misa) >>  Has0_acc_2: 0.7050  Has0_F1_score: 0.7126  Non0_acc_2: 0.2663  Non0_F1_score: 0.2671  Acc_3: 0.6537  F1_score_3: 0.6931  Loss: 0.8824 
2021-02-02 08:55:58:INFO:TRAIN-(misa) (2/3/1)>> loss: 0.5003  Has0_acc_2: 0.7912  Has0_F1_score: 0.7971  Non0_acc_2: 0.3356  Non0_F1_score: 0.3300  Acc_3: 0.8283  F1_score_3: 0.8301 
2021-02-02 08:56:03:INFO:VAL-(misa) >>  Has0_acc_2: 0.6382  Has0_F1_score: 0.6807  Non0_acc_2: 0.1801  Non0_F1_score: 0.1318  Acc_3: 0.6088  F1_score_3: 0.6067  Loss: 0.9816 
2021-02-02 08:58:38:INFO:TRAIN-(misa) (3/4/1)>> loss: 0.3488  Has0_acc_2: 0.8181  Has0_F1_score: 0.8225  Non0_acc_2: 0.3527  Non0_F1_score: 0.3506  Acc_3: 0.8931  F1_score_3: 0.8933 
2021-02-02 08:58:44:INFO:VAL-(misa) >>  Has0_acc_2: 0.7055  Has0_F1_score: 0.7109  Non0_acc_2: 0.2677  Non0_F1_score: 0.2747  Acc_3: 0.6184  F1_score_3: 0.6150  Loss: 1.2837 
2021-02-02 09:01:20:INFO:TRAIN-(misa) (4/5/1)>> loss: 0.2603  Has0_acc_2: 0.8282  Has0_F1_score: 0.8320  Non0_acc_2: 0.3582  Non0_F1_score: 0.3573  Acc_3: 0.9254  F1_score_3: 0.9256 
2021-02-02 09:01:26:INFO:VAL-(misa) >>  Has0_acc_2: 0.6863  Has0_F1_score: 0.6957  Non0_acc_2: 0.2587  Non0_F1_score: 0.2582  Acc_3: 0.6130  F1_score_3: 0.6087  Loss: 1.2074 
2021-02-02 09:04:00:INFO:TRAIN-(misa) (5/6/1)>> loss: 0.1909  Has0_acc_2: 0.8505  Has0_F1_score: 0.8531  Non0_acc_2: 0.3634  Non0_F1_score: 0.3632  Acc_3: 0.9498  F1_score_3: 0.9499 
2021-02-02 09:04:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.6852  Has0_F1_score: 0.6944  Non0_acc_2: 0.2559  Non0_F1_score: 0.2552  Acc_3: 0.6350  F1_score_3: 0.6519  Loss: 1.4225 
2021-02-02 09:06:43:INFO:TRAIN-(misa) (6/7/1)>> loss: 0.1443  Has0_acc_2: 0.8714  Has0_F1_score: 0.8730  Non0_acc_2: 0.3643  Non0_F1_score: 0.3640  Acc_3: 0.9649  F1_score_3: 0.9650 
2021-02-02 09:06:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.6804  Has0_F1_score: 0.6991  Non0_acc_2: 0.2309  Non0_F1_score: 0.2064  Acc_3: 0.6296  F1_score_3: 0.6472  Loss: 1.6397 
2021-02-02 09:09:24:INFO:TRAIN-(misa) (7/8/1)>> loss: 0.1333  Has0_acc_2: 0.8614  Has0_F1_score: 0.8636  Non0_acc_2: 0.3660  Non0_F1_score: 0.3666  Acc_3: 0.9660  F1_score_3: 0.9660 
2021-02-02 09:09:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.6980  Has0_F1_score: 0.7035  Non0_acc_2: 0.2698  Non0_F1_score: 0.2796  Acc_3: 0.6264  F1_score_3: 0.6379  Loss: 1.6289 
2021-02-02 09:12:04:INFO:TRAIN-(misa) (8/9/1)>> loss: 0.1122  Has0_acc_2: 0.8810  Has0_F1_score: 0.8824  Non0_acc_2: 0.3663  Non0_F1_score: 0.3662  Acc_3: 0.9726  F1_score_3: 0.9726 
2021-02-02 09:12:10:INFO:VAL-(misa) >>  Has0_acc_2: 0.7012  Has0_F1_score: 0.7023  Non0_acc_2: 0.2872  Non0_F1_score: 0.3199  Acc_3: 0.6120  F1_score_3: 0.6193  Loss: 1.9720 
2021-02-02 09:12:25:INFO:TEST-(misa) >>  Has0_acc_2: 0.7242  Has0_F1_score: 0.7242  Non0_acc_2: 0.3396  Non0_F1_score: 0.3924  Acc_3: 0.6306  F1_score_3: 0.6322  Loss: 0.8427 
2021-02-02 09:12:25:INFO:Start saving results...
2021-02-02 09:12:25:INFO:Results are saved to results/results/mosei-misa-classification-tune.csv...
2021-02-02 09:12:25:INFO:########################################misa-(3/50)########################################
2021-02-02 09:12:25:INFO:batch_size:64
2021-02-02 09:12:25:INFO:learning_rate:0.0005
2021-02-02 09:12:25:INFO:hidden_size:128
2021-02-02 09:12:25:INFO:dropout:0.5
2021-02-02 09:12:25:INFO:reverse_grad_weight:1.0
2021-02-02 09:12:25:INFO:diff_weight:0.3
2021-02-02 09:12:25:INFO:sim_weight:1.0
2021-02-02 09:12:25:INFO:sp_weight:0.0
2021-02-02 09:12:25:INFO:recon_weight:0.5
2021-02-02 09:12:25:INFO:grad_clip:1.0
2021-02-02 09:12:25:INFO:weight_decay:0.0
2021-02-02 09:12:25:INFO:##########################################################################################
2021-02-02 09:12:25:INFO:Start running misa...
2021-02-02 09:12:25:INFO:Find gpu: 2, with memory: 3267166208 left!
2021-02-02 09:12:25:INFO:Let's use 1 GPUs!
2021-02-02 09:12:37:INFO:train samples: (16326,)
2021-02-02 09:12:49:INFO:valid samples: (1871,)
2021-02-02 09:13:01:INFO:test samples: (4659,)
2021-02-02 09:13:01:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-02 09:13:01:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-02 09:13:01:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-02 09:13:01:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-02 09:13:01:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-02 09:13:01:INFO:loading file None
2021-02-02 09:13:01:INFO:loading file None
2021-02-02 09:13:01:INFO:loading file None
2021-02-02 09:13:01:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-02 09:13:01:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-02 09:13:01:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-02 09:13:03:INFO:The model has 110918115 trainable parameters
2021-02-02 09:13:04:ERROR:CUDA out of memory. Tried to allocate 38.00 MiB (GPU 2; 7.77 GiB total capacity; 4.57 GiB already allocated; 14.50 MiB free; 144.07 MiB cached)
2021-02-02 20:16:37:INFO:Start running misa...
2021-02-02 20:16:37:INFO:<Storage{'is_tune': False, 'train_mode': 'regression', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1111}>
2021-02-02 20:16:37:INFO:Find gpu: 1, with memory: 2252144640 left!
2021-02-02 20:16:37:INFO:Let's use 1 GPUs!
2021-02-02 20:19:20:ERROR:Unable to allocate array with shape (16326, 500, 74) and data type float32
2021-02-05 01:05:46:INFO:Start running misa...
2021-02-05 01:05:46:INFO:<Storage{'is_tune': False, 'train_mode': 'regression', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1111}>
2021-02-05 01:05:46:INFO:Find gpu: 1, with memory: 12320768 left!
2021-02-05 01:05:46:INFO:Let's use 1 GPUs!
2021-02-05 01:08:19:INFO:train samples: (16326,)
2021-02-05 01:08:31:INFO:valid samples: (1871,)
2021-02-05 01:08:43:INFO:test samples: (4659,)
2021-02-05 01:08:43:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 01:08:43:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 01:08:43:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 01:08:43:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 01:08:43:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 01:08:43:INFO:loading file None
2021-02-05 01:08:43:INFO:loading file None
2021-02-05 01:08:43:INFO:loading file None
2021-02-05 01:08:43:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 01:08:43:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 01:08:43:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 01:08:47:INFO:The model has 110917345 trainable parameters
2021-02-05 01:10:56:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.2887  Has0_acc_2: 0.7848  Has0_F1_score: 0.7796  Non0_acc_2: 0.8170  Non0_F1_score: 0.8179  Mult_acc_5: 0.4908  Mult_acc_7: 0.4791  MAE: 0.6282  Corr: 0.6743 
2021-02-05 01:11:01:INFO:VAL-(misa) >>  Has0_acc_2: 0.8022  Has0_F1_score: 0.7954  Non0_acc_2: 0.8414  Non0_F1_score: 0.8414  Mult_acc_5: 0.5452  Mult_acc_7: 0.5329  MAE: 0.5356  Corr: 0.7313  Loss: 0.5136 
2021-02-05 01:13:17:INFO:TRAIN-(misa) (1/2/1)>> loss: 0.5338  Has0_acc_2: 0.8316  Has0_F1_score: 0.8271  Non0_acc_2: 0.8782  Non0_F1_score: 0.8786  Mult_acc_5: 0.5779  Mult_acc_7: 0.5547  MAE: 0.4936  Corr: 0.8220 
2021-02-05 01:13:22:INFO:VAL-(misa) >>  Has0_acc_2: 0.8375  Has0_F1_score: 0.8377  Non0_acc_2: 0.8435  Non0_F1_score: 0.8472  Mult_acc_5: 0.5345  Mult_acc_7: 0.5184  MAE: 0.5426  Corr: 0.7346  Loss: 0.5150 
2021-02-05 01:15:37:INFO:TRAIN-(misa) (2/3/1)>> loss: 0.3369  Has0_acc_2: 0.8591  Has0_F1_score: 0.8553  Non0_acc_2: 0.9146  Non0_F1_score: 0.9148  Mult_acc_5: 0.6601  Mult_acc_7: 0.6327  MAE: 0.3985  Corr: 0.8885 
2021-02-05 01:15:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.8327  Has0_F1_score: 0.8308  Non0_acc_2: 0.8512  Non0_F1_score: 0.8536  Mult_acc_5: 0.5371  Mult_acc_7: 0.5243  MAE: 0.5403  Corr: 0.7279  Loss: 0.5154 
2021-02-05 01:17:57:INFO:TRAIN-(misa) (3/4/1)>> loss: 0.2271  Has0_acc_2: 0.8742  Has0_F1_score: 0.8704  Non0_acc_2: 0.9413  Non0_F1_score: 0.9413  Mult_acc_5: 0.7382  Mult_acc_7: 0.7089  MAE: 0.3212  Corr: 0.9281 
2021-02-05 01:18:02:INFO:VAL-(misa) >>  Has0_acc_2: 0.7606  Has0_F1_score: 0.7481  Non0_acc_2: 0.8241  Non0_F1_score: 0.8212  Mult_acc_5: 0.5318  Mult_acc_7: 0.5163  MAE: 0.5692  Corr: 0.7258  Loss: 0.5535 
2021-02-05 01:20:16:INFO:TRAIN-(misa) (4/5/1)>> loss: 0.1733  Has0_acc_2: 0.8787  Has0_F1_score: 0.8749  Non0_acc_2: 0.9531  Non0_F1_score: 0.9531  Mult_acc_5: 0.7743  Mult_acc_7: 0.7467  MAE: 0.2787  Corr: 0.9464 
2021-02-05 01:20:21:INFO:VAL-(misa) >>  Has0_acc_2: 0.8365  Has0_F1_score: 0.8350  Non0_acc_2: 0.8526  Non0_F1_score: 0.8551  Mult_acc_5: 0.5168  Mult_acc_7: 0.4992  MAE: 0.5776  Corr: 0.7235  Loss: 0.5733 
2021-02-05 01:22:35:INFO:TRAIN-(misa) (5/6/1)>> loss: 0.1338  Has0_acc_2: 0.8876  Has0_F1_score: 0.8840  Non0_acc_2: 0.9676  Non0_F1_score: 0.9676  Mult_acc_5: 0.8089  Mult_acc_7: 0.7827  MAE: 0.2412  Corr: 0.9597 
2021-02-05 01:22:40:INFO:VAL-(misa) >>  Has0_acc_2: 0.7846  Has0_F1_score: 0.7749  Non0_acc_2: 0.8345  Non0_F1_score: 0.8328  Mult_acc_5: 0.5307  Mult_acc_7: 0.5158  MAE: 0.5677  Corr: 0.7202  Loss: 0.5673 
2021-02-05 01:24:54:INFO:TRAIN-(misa) (6/7/1)>> loss: 0.1056  Has0_acc_2: 0.8902  Has0_F1_score: 0.8865  Non0_acc_2: 0.9765  Non0_F1_score: 0.9765  Mult_acc_5: 0.8358  Mult_acc_7: 0.8121  MAE: 0.2112  Corr: 0.9693 
2021-02-05 01:24:59:INFO:VAL-(misa) >>  Has0_acc_2: 0.8258  Has0_F1_score: 0.8229  Non0_acc_2: 0.8477  Non0_F1_score: 0.8496  Mult_acc_5: 0.5366  Mult_acc_7: 0.5216  MAE: 0.5544  Corr: 0.7315  Loss: 0.5327 
2021-02-05 01:27:13:INFO:TRAIN-(misa) (7/8/1)>> loss: 0.0899  Has0_acc_2: 0.8970  Has0_F1_score: 0.8936  Non0_acc_2: 0.9830  Non0_F1_score: 0.9830  Mult_acc_5: 0.8493  Mult_acc_7: 0.8268  MAE: 0.1944  Corr: 0.9739 
2021-02-05 01:27:18:INFO:VAL-(misa) >>  Has0_acc_2: 0.8316  Has0_F1_score: 0.8297  Non0_acc_2: 0.8463  Non0_F1_score: 0.8485  Mult_acc_5: 0.5094  Mult_acc_7: 0.4923  MAE: 0.5828  Corr: 0.7293  Loss: 0.5732 
2021-02-05 01:29:32:INFO:TRAIN-(misa) (8/9/1)>> loss: 0.0836  Has0_acc_2: 0.8959  Has0_F1_score: 0.8924  Non0_acc_2: 0.9856  Non0_F1_score: 0.9856  Mult_acc_5: 0.8608  Mult_acc_7: 0.8387  MAE: 0.1873  Corr: 0.9756 
2021-02-05 01:29:37:INFO:VAL-(misa) >>  Has0_acc_2: 0.8236  Has0_F1_score: 0.8194  Non0_acc_2: 0.8484  Non0_F1_score: 0.8492  Mult_acc_5: 0.5441  Mult_acc_7: 0.5270  MAE: 0.5471  Corr: 0.7226  Loss: 0.5358 
2021-02-05 01:29:49:INFO:TEST-(misa) >>  Has0_acc_2: 0.8012  Has0_F1_score: 0.7948  Non0_acc_2: 0.8459  Non0_F1_score: 0.8455  Mult_acc_5: 0.5450  Mult_acc_7: 0.5291  MAE: 0.5528  Corr: 0.7543  Loss: 0.5421 
2021-02-05 01:29:56:INFO:Start running misa...
2021-02-05 01:29:56:INFO:<Storage{'is_tune': False, 'train_mode': 'regression', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1112}>
2021-02-05 01:29:56:INFO:Let's use 1 GPUs!
2021-02-05 01:30:08:INFO:train samples: (16326,)
2021-02-05 01:30:19:INFO:valid samples: (1871,)
2021-02-05 01:30:31:INFO:test samples: (4659,)
2021-02-05 01:30:32:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 01:30:32:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 01:30:32:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 01:30:32:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 01:30:32:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 01:30:32:INFO:loading file None
2021-02-05 01:30:32:INFO:loading file None
2021-02-05 01:30:32:INFO:loading file None
2021-02-05 01:30:32:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 01:30:32:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 01:30:32:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 01:30:33:INFO:The model has 110917345 trainable parameters
2021-02-05 01:32:47:INFO:TRAIN-(misa) (1/1/2)>> loss: 1.3278  Has0_acc_2: 0.7753  Has0_F1_score: 0.7697  Non0_acc_2: 0.8057  Non0_F1_score: 0.8066  Mult_acc_5: 0.4860  Mult_acc_7: 0.4745  MAE: 0.6340  Corr: 0.6671 
2021-02-05 01:32:52:INFO:VAL-(misa) >>  Has0_acc_2: 0.8065  Has0_F1_score: 0.8007  Non0_acc_2: 0.8394  Non0_F1_score: 0.8398  Mult_acc_5: 0.5190  Mult_acc_7: 0.5067  MAE: 0.5458  Corr: 0.7283  Loss: 0.5152 
2021-02-05 01:35:08:INFO:TRAIN-(misa) (1/2/2)>> loss: 0.5802  Has0_acc_2: 0.8320  Has0_F1_score: 0.8280  Non0_acc_2: 0.8743  Non0_F1_score: 0.8749  Mult_acc_5: 0.5644  Mult_acc_7: 0.5433  MAE: 0.5102  Corr: 0.8060 
2021-02-05 01:35:13:INFO:VAL-(misa) >>  Has0_acc_2: 0.8263  Has0_F1_score: 0.8232  Non0_acc_2: 0.8408  Non0_F1_score: 0.8420  Mult_acc_5: 0.4944  Mult_acc_7: 0.4751  MAE: 0.5951  Corr: 0.7274  Loss: 0.6002 
2021-02-05 01:37:28:INFO:TRAIN-(misa) (2/3/2)>> loss: 0.3680  Has0_acc_2: 0.8541  Has0_F1_score: 0.8502  Non0_acc_2: 0.9087  Non0_F1_score: 0.9089  Mult_acc_5: 0.6451  Mult_acc_7: 0.6181  MAE: 0.4154  Corr: 0.8771 
2021-02-05 01:37:32:INFO:VAL-(misa) >>  Has0_acc_2: 0.8306  Has0_F1_score: 0.8287  Non0_acc_2: 0.8421  Non0_F1_score: 0.8443  Mult_acc_5: 0.5013  Mult_acc_7: 0.4858  MAE: 0.5959  Corr: 0.7158  Loss: 0.6031 
2021-02-05 01:39:46:INFO:TRAIN-(misa) (3/4/2)>> loss: 0.2369  Has0_acc_2: 0.8703  Has0_F1_score: 0.8665  Non0_acc_2: 0.9352  Non0_F1_score: 0.9353  Mult_acc_5: 0.7301  Mult_acc_7: 0.7016  MAE: 0.3300  Corr: 0.9239 
2021-02-05 01:39:51:INFO:VAL-(misa) >>  Has0_acc_2: 0.7819  Has0_F1_score: 0.7720  Non0_acc_2: 0.8324  Non0_F1_score: 0.8307  Mult_acc_5: 0.5254  Mult_acc_7: 0.5099  MAE: 0.5483  Corr: 0.7274  Loss: 0.5314 
2021-02-05 01:42:05:INFO:TRAIN-(misa) (4/5/2)>> loss: 0.1659  Has0_acc_2: 0.8796  Has0_F1_score: 0.8758  Non0_acc_2: 0.9551  Non0_F1_score: 0.9551  Mult_acc_5: 0.7778  Mult_acc_7: 0.7497  MAE: 0.2737  Corr: 0.9483 
2021-02-05 01:42:09:INFO:VAL-(misa) >>  Has0_acc_2: 0.7889  Has0_F1_score: 0.7791  Non0_acc_2: 0.8345  Non0_F1_score: 0.8324  Mult_acc_5: 0.5243  Mult_acc_7: 0.5083  MAE: 0.5562  Corr: 0.7182  Loss: 0.5452 
2021-02-05 01:44:23:INFO:TRAIN-(misa) (5/6/2)>> loss: 0.1306  Has0_acc_2: 0.8875  Has0_F1_score: 0.8839  Non0_acc_2: 0.9666  Non0_F1_score: 0.9666  Mult_acc_5: 0.8112  Mult_acc_7: 0.7855  MAE: 0.2399  Corr: 0.9606 
2021-02-05 01:44:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.8044  Has0_F1_score: 0.7973  Non0_acc_2: 0.8414  Non0_F1_score: 0.8409  Mult_acc_5: 0.5142  Mult_acc_7: 0.4949  MAE: 0.6065  Corr: 0.7208  Loss: 0.6343 
2021-02-05 01:46:42:INFO:TRAIN-(misa) (6/7/2)>> loss: 0.1071  Has0_acc_2: 0.8905  Has0_F1_score: 0.8868  Non0_acc_2: 0.9756  Non0_F1_score: 0.9756  Mult_acc_5: 0.8330  Mult_acc_7: 0.8091  MAE: 0.2160  Corr: 0.9681 
2021-02-05 01:46:47:INFO:VAL-(misa) >>  Has0_acc_2: 0.8129  Has0_F1_score: 0.8078  Non0_acc_2: 0.8421  Non0_F1_score: 0.8427  Mult_acc_5: 0.5158  Mult_acc_7: 0.5019  MAE: 0.5683  Corr: 0.7278  Loss: 0.5544 
2021-02-05 01:49:01:INFO:TRAIN-(misa) (7/8/2)>> loss: 0.0963  Has0_acc_2: 0.8954  Has0_F1_score: 0.8920  Non0_acc_2: 0.9786  Non0_F1_score: 0.9786  Mult_acc_5: 0.8463  Mult_acc_7: 0.8245  MAE: 0.2040  Corr: 0.9714 
2021-02-05 01:49:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.8215  Has0_F1_score: 0.8176  Non0_acc_2: 0.8401  Non0_F1_score: 0.8409  Mult_acc_5: 0.5227  Mult_acc_7: 0.5067  MAE: 0.5743  Corr: 0.7274  Loss: 0.5590 
2021-02-05 01:51:20:INFO:TRAIN-(misa) (8/9/2)>> loss: 0.0836  Has0_acc_2: 0.8988  Has0_F1_score: 0.8954  Non0_acc_2: 0.9853  Non0_F1_score: 0.9853  Mult_acc_5: 0.8573  Mult_acc_7: 0.8355  MAE: 0.1887  Corr: 0.9753 
2021-02-05 01:51:24:INFO:VAL-(misa) >>  Has0_acc_2: 0.7953  Has0_F1_score: 0.7875  Non0_acc_2: 0.8380  Non0_F1_score: 0.8374  Mult_acc_5: 0.5211  Mult_acc_7: 0.5051  MAE: 0.5498  Corr: 0.7304  Loss: 0.5240 
2021-02-05 01:51:37:INFO:TEST-(misa) >>  Has0_acc_2: 0.8002  Has0_F1_score: 0.7939  Non0_acc_2: 0.8434  Non0_F1_score: 0.8431  Mult_acc_5: 0.5287  Mult_acc_7: 0.5158  MAE: 0.5654  Corr: 0.7416  Loss: 0.5630 
2021-02-05 01:51:42:INFO:Start running misa...
2021-02-05 01:51:42:INFO:<Storage{'is_tune': False, 'train_mode': 'regression', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1113}>
2021-02-05 01:51:42:INFO:Let's use 1 GPUs!
2021-02-05 01:51:54:INFO:train samples: (16326,)
2021-02-05 01:52:05:INFO:valid samples: (1871,)
2021-02-05 01:52:16:INFO:test samples: (4659,)
2021-02-05 01:52:17:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 01:52:17:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 01:52:17:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 01:52:17:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 01:52:17:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 01:52:17:INFO:loading file None
2021-02-05 01:52:17:INFO:loading file None
2021-02-05 01:52:17:INFO:loading file None
2021-02-05 01:52:17:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 01:52:17:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 01:52:17:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 01:52:18:INFO:The model has 110917345 trainable parameters
2021-02-05 01:54:33:INFO:TRAIN-(misa) (1/1/3)>> loss: 1.3465  Has0_acc_2: 0.7764  Has0_F1_score: 0.7706  Non0_acc_2: 0.8092  Non0_F1_score: 0.8098  Mult_acc_5: 0.4848  Mult_acc_7: 0.4746  MAE: 0.6339  Corr: 0.6667 
2021-02-05 01:54:38:INFO:VAL-(misa) >>  Has0_acc_2: 0.8284  Has0_F1_score: 0.8297  Non0_acc_2: 0.8387  Non0_F1_score: 0.8441  Mult_acc_5: 0.5056  Mult_acc_7: 0.4885  MAE: 0.5698  Corr: 0.7212  Loss: 0.5487 
2021-02-05 01:56:53:INFO:TRAIN-(misa) (1/2/3)>> loss: 0.5623  Has0_acc_2: 0.8349  Has0_F1_score: 0.8303  Non0_acc_2: 0.8831  Non0_F1_score: 0.8832  Mult_acc_5: 0.5751  Mult_acc_7: 0.5533  MAE: 0.4979  Corr: 0.8170 
2021-02-05 01:56:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.7552  Has0_F1_score: 0.7428  Non0_acc_2: 0.8143  Non0_F1_score: 0.8116  Mult_acc_5: 0.5243  Mult_acc_7: 0.5099  MAE: 0.5544  Corr: 0.7198  Loss: 0.5406 
2021-02-05 01:59:13:INFO:TRAIN-(misa) (1/3/3)>> loss: 0.3379  Has0_acc_2: 0.8599  Has0_F1_score: 0.8559  Non0_acc_2: 0.9203  Non0_F1_score: 0.9204  Mult_acc_5: 0.6614  Mult_acc_7: 0.6336  MAE: 0.3952  Corr: 0.8893 
2021-02-05 01:59:18:INFO:VAL-(misa) >>  Has0_acc_2: 0.8161  Has0_F1_score: 0.8124  Non0_acc_2: 0.8401  Non0_F1_score: 0.8417  Mult_acc_5: 0.5179  Mult_acc_7: 0.5008  MAE: 0.5720  Corr: 0.7238  Loss: 0.5631 
2021-02-05 02:01:31:INFO:TRAIN-(misa) (2/4/3)>> loss: 0.2328  Has0_acc_2: 0.8687  Has0_F1_score: 0.8647  Non0_acc_2: 0.9345  Non0_F1_score: 0.9345  Mult_acc_5: 0.7301  Mult_acc_7: 0.7016  MAE: 0.3275  Corr: 0.9264 
2021-02-05 02:01:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.8322  Has0_F1_score: 0.8341  Non0_acc_2: 0.8310  Non0_F1_score: 0.8363  Mult_acc_5: 0.5120  Mult_acc_7: 0.4971  MAE: 0.5705  Corr: 0.7242  Loss: 0.5562 
2021-02-05 02:03:50:INFO:TRAIN-(misa) (3/5/3)>> loss: 0.1658  Has0_acc_2: 0.8801  Has0_F1_score: 0.8762  Non0_acc_2: 0.9573  Non0_F1_score: 0.9573  Mult_acc_5: 0.7816  Mult_acc_7: 0.7552  MAE: 0.2716  Corr: 0.9491 
2021-02-05 02:03:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.7846  Has0_F1_score: 0.7756  Non0_acc_2: 0.8275  Non0_F1_score: 0.8262  Mult_acc_5: 0.5249  Mult_acc_7: 0.5067  MAE: 0.5754  Corr: 0.7220  Loss: 0.5727 
2021-02-05 02:06:08:INFO:TRAIN-(misa) (4/6/3)>> loss: 0.1287  Has0_acc_2: 0.8914  Has0_F1_score: 0.8879  Non0_acc_2: 0.9708  Non0_F1_score: 0.9708  Mult_acc_5: 0.8171  Mult_acc_7: 0.7919  MAE: 0.2369  Corr: 0.9615 
2021-02-05 02:06:13:INFO:VAL-(misa) >>  Has0_acc_2: 0.7814  Has0_F1_score: 0.7719  Non0_acc_2: 0.8317  Non0_F1_score: 0.8304  Mult_acc_5: 0.5200  Mult_acc_7: 0.5040  MAE: 0.5707  Corr: 0.7208  Loss: 0.5544 
2021-02-05 02:08:27:INFO:TRAIN-(misa) (5/7/3)>> loss: 0.1024  Has0_acc_2: 0.8926  Has0_F1_score: 0.8890  Non0_acc_2: 0.9784  Non0_F1_score: 0.9784  Mult_acc_5: 0.8423  Mult_acc_7: 0.8193  MAE: 0.2093  Corr: 0.9699 
2021-02-05 02:08:32:INFO:VAL-(misa) >>  Has0_acc_2: 0.8226  Has0_F1_score: 0.8196  Non0_acc_2: 0.8387  Non0_F1_score: 0.8403  Mult_acc_5: 0.5462  Mult_acc_7: 0.5286  MAE: 0.5609  Corr: 0.7298  Loss: 0.5469 
2021-02-05 02:10:46:INFO:TRAIN-(misa) (6/8/3)>> loss: 0.0894  Has0_acc_2: 0.8937  Has0_F1_score: 0.8900  Non0_acc_2: 0.9851  Non0_F1_score: 0.9851  Mult_acc_5: 0.8544  Mult_acc_7: 0.8321  MAE: 0.1932  Corr: 0.9742 
2021-02-05 02:10:51:INFO:VAL-(misa) >>  Has0_acc_2: 0.7830  Has0_F1_score: 0.7729  Non0_acc_2: 0.8414  Non0_F1_score: 0.8398  Mult_acc_5: 0.5371  Mult_acc_7: 0.5227  MAE: 0.5583  Corr: 0.7270  Loss: 0.5435 
2021-02-05 02:13:04:INFO:TRAIN-(misa) (7/9/3)>> loss: 0.0812  Has0_acc_2: 0.8959  Has0_F1_score: 0.8923  Non0_acc_2: 0.9862  Non0_F1_score: 0.9862  Mult_acc_5: 0.8657  Mult_acc_7: 0.8451  MAE: 0.1848  Corr: 0.9765 
2021-02-05 02:13:09:INFO:VAL-(misa) >>  Has0_acc_2: 0.8381  Has0_F1_score: 0.8367  Non0_acc_2: 0.8498  Non0_F1_score: 0.8521  Mult_acc_5: 0.5281  Mult_acc_7: 0.5142  MAE: 0.5525  Corr: 0.7313  Loss: 0.5239 
2021-02-05 02:15:24:INFO:TRAIN-(misa) (1/10/3)>> loss: 0.0769  Has0_acc_2: 0.8954  Has0_F1_score: 0.8918  Non0_acc_2: 0.9854  Non0_F1_score: 0.9854  Mult_acc_5: 0.8650  Mult_acc_7: 0.8442  MAE: 0.1796  Corr: 0.9774 
2021-02-05 02:15:29:INFO:VAL-(misa) >>  Has0_acc_2: 0.8471  Has0_F1_score: 0.8486  Non0_acc_2: 0.8484  Non0_F1_score: 0.8527  Mult_acc_5: 0.5029  Mult_acc_7: 0.4890  MAE: 0.5589  Corr: 0.7275  Loss: 0.5245 
2021-02-05 02:17:43:INFO:TRAIN-(misa) (2/11/3)>> loss: 0.0732  Has0_acc_2: 0.8978  Has0_F1_score: 0.8943  Non0_acc_2: 0.9883  Non0_F1_score: 0.9883  Mult_acc_5: 0.8703  Mult_acc_7: 0.8503  MAE: 0.1754  Corr: 0.9784 
2021-02-05 02:17:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.7793  Has0_F1_score: 0.7684  Non0_acc_2: 0.8366  Non0_F1_score: 0.8343  Mult_acc_5: 0.5190  Mult_acc_7: 0.5024  MAE: 0.5750  Corr: 0.7259  Loss: 0.5711 
2021-02-05 02:20:03:INFO:TRAIN-(misa) (3/12/3)>> loss: 0.0680  Has0_acc_2: 0.8976  Has0_F1_score: 0.8941  Non0_acc_2: 0.9879  Non0_F1_score: 0.9879  Mult_acc_5: 0.8791  Mult_acc_7: 0.8606  MAE: 0.1694  Corr: 0.9798 
2021-02-05 02:20:08:INFO:VAL-(misa) >>  Has0_acc_2: 0.7841  Has0_F1_score: 0.7741  Non0_acc_2: 0.8387  Non0_F1_score: 0.8370  Mult_acc_5: 0.5462  Mult_acc_7: 0.5297  MAE: 0.5486  Corr: 0.7329  Loss: 0.5388 
2021-02-05 02:22:22:INFO:TRAIN-(misa) (4/13/3)>> loss: 0.0612  Has0_acc_2: 0.8969  Has0_F1_score: 0.8933  Non0_acc_2: 0.9916  Non0_F1_score: 0.9916  Mult_acc_5: 0.8868  Mult_acc_7: 0.8678  MAE: 0.1604  Corr: 0.9820 
2021-02-05 02:22:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.7819  Has0_F1_score: 0.7715  Non0_acc_2: 0.8373  Non0_F1_score: 0.8352  Mult_acc_5: 0.5404  Mult_acc_7: 0.5243  MAE: 0.5465  Corr: 0.7331  Loss: 0.5248 
2021-02-05 02:24:42:INFO:TRAIN-(misa) (5/14/3)>> loss: 0.0605  Has0_acc_2: 0.8986  Has0_F1_score: 0.8951  Non0_acc_2: 0.9923  Non0_F1_score: 0.9923  Mult_acc_5: 0.8866  Mult_acc_7: 0.8694  MAE: 0.1597  Corr: 0.9820 
2021-02-05 02:24:47:INFO:VAL-(misa) >>  Has0_acc_2: 0.7830  Has0_F1_score: 0.7733  Non0_acc_2: 0.8310  Non0_F1_score: 0.8293  Mult_acc_5: 0.5526  Mult_acc_7: 0.5382  MAE: 0.5454  Corr: 0.7278  Loss: 0.5291 
2021-02-05 02:27:01:INFO:TRAIN-(misa) (6/15/3)>> loss: 0.0584  Has0_acc_2: 0.8993  Has0_F1_score: 0.8959  Non0_acc_2: 0.9921  Non0_F1_score: 0.9921  Mult_acc_5: 0.8888  Mult_acc_7: 0.8720  MAE: 0.1567  Corr: 0.9825 
2021-02-05 02:27:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.7814  Has0_F1_score: 0.7710  Non0_acc_2: 0.8296  Non0_F1_score: 0.8273  Mult_acc_5: 0.5339  Mult_acc_7: 0.5200  MAE: 0.5464  Corr: 0.7285  Loss: 0.5214 
2021-02-05 02:29:22:INFO:TRAIN-(misa) (1/16/3)>> loss: 0.0567  Has0_acc_2: 0.8993  Has0_F1_score: 0.8958  Non0_acc_2: 0.9923  Non0_F1_score: 0.9923  Mult_acc_5: 0.8886  Mult_acc_7: 0.8719  MAE: 0.1552  Corr: 0.9830 
2021-02-05 02:29:27:INFO:VAL-(misa) >>  Has0_acc_2: 0.8151  Has0_F1_score: 0.8111  Non0_acc_2: 0.8387  Non0_F1_score: 0.8400  Mult_acc_5: 0.5350  Mult_acc_7: 0.5216  MAE: 0.5388  Corr: 0.7283  Loss: 0.5058 
2021-02-05 02:31:42:INFO:TRAIN-(misa) (1/17/3)>> loss: 0.0565  Has0_acc_2: 0.8976  Has0_F1_score: 0.8941  Non0_acc_2: 0.9928  Non0_F1_score: 0.9928  Mult_acc_5: 0.8923  Mult_acc_7: 0.8748  MAE: 0.1556  Corr: 0.9829 
2021-02-05 02:31:47:INFO:VAL-(misa) >>  Has0_acc_2: 0.8397  Has0_F1_score: 0.8376  Non0_acc_2: 0.8484  Non0_F1_score: 0.8498  Mult_acc_5: 0.5211  Mult_acc_7: 0.5056  MAE: 0.5500  Corr: 0.7322  Loss: 0.5121 
2021-02-05 02:34:01:INFO:TRAIN-(misa) (2/18/3)>> loss: 0.0573  Has0_acc_2: 0.9018  Has0_F1_score: 0.8984  Non0_acc_2: 0.9926  Non0_F1_score: 0.9927  Mult_acc_5: 0.8905  Mult_acc_7: 0.8735  MAE: 0.1564  Corr: 0.9825 
2021-02-05 02:34:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.7953  Has0_F1_score: 0.7870  Non0_acc_2: 0.8380  Non0_F1_score: 0.8369  Mult_acc_5: 0.5323  Mult_acc_7: 0.5168  MAE: 0.5577  Corr: 0.7258  Loss: 0.5357 
2021-02-05 02:36:20:INFO:TRAIN-(misa) (3/19/3)>> loss: 0.0562  Has0_acc_2: 0.8970  Has0_F1_score: 0.8934  Non0_acc_2: 0.9917  Non0_F1_score: 0.9917  Mult_acc_5: 0.8897  Mult_acc_7: 0.8740  MAE: 0.1548  Corr: 0.9829 
2021-02-05 02:36:25:INFO:VAL-(misa) >>  Has0_acc_2: 0.8183  Has0_F1_score: 0.8137  Non0_acc_2: 0.8414  Non0_F1_score: 0.8420  Mult_acc_5: 0.5200  Mult_acc_7: 0.5035  MAE: 0.5501  Corr: 0.7291  Loss: 0.5226 
2021-02-05 02:38:39:INFO:TRAIN-(misa) (4/20/3)>> loss: 0.0529  Has0_acc_2: 0.8985  Has0_F1_score: 0.8950  Non0_acc_2: 0.9933  Non0_F1_score: 0.9933  Mult_acc_5: 0.8982  Mult_acc_7: 0.8827  MAE: 0.1494  Corr: 0.9840 
2021-02-05 02:38:44:INFO:VAL-(misa) >>  Has0_acc_2: 0.8108  Has0_F1_score: 0.8045  Non0_acc_2: 0.8505  Non0_F1_score: 0.8506  Mult_acc_5: 0.5420  Mult_acc_7: 0.5249  MAE: 0.5401  Corr: 0.7306  Loss: 0.5161 
2021-02-05 02:40:58:INFO:TRAIN-(misa) (5/21/3)>> loss: 0.0511  Has0_acc_2: 0.9002  Has0_F1_score: 0.8968  Non0_acc_2: 0.9934  Non0_F1_score: 0.9934  Mult_acc_5: 0.9008  Mult_acc_7: 0.8839  MAE: 0.1454  Corr: 0.9846 
2021-02-05 02:41:03:INFO:VAL-(misa) >>  Has0_acc_2: 0.7910  Has0_F1_score: 0.7818  Non0_acc_2: 0.8401  Non0_F1_score: 0.8385  Mult_acc_5: 0.5425  Mult_acc_7: 0.5249  MAE: 0.5474  Corr: 0.7331  Loss: 0.5287 
2021-02-05 02:43:17:INFO:TRAIN-(misa) (6/22/3)>> loss: 0.0550  Has0_acc_2: 0.8994  Has0_F1_score: 0.8960  Non0_acc_2: 0.9934  Non0_F1_score: 0.9934  Mult_acc_5: 0.8969  Mult_acc_7: 0.8784  MAE: 0.1514  Corr: 0.9831 
2021-02-05 02:43:22:INFO:VAL-(misa) >>  Has0_acc_2: 0.7798  Has0_F1_score: 0.7699  Non0_acc_2: 0.8227  Non0_F1_score: 0.8207  Mult_acc_5: 0.5366  Mult_acc_7: 0.5216  MAE: 0.5543  Corr: 0.6997  Loss: 0.5659 
2021-02-05 02:45:35:INFO:TRAIN-(misa) (7/23/3)>> loss: 0.0905  Has0_acc_2: 0.8894  Has0_F1_score: 0.8855  Non0_acc_2: 0.9820  Non0_F1_score: 0.9820  Mult_acc_5: 0.8558  Mult_acc_7: 0.8375  MAE: 0.1909  Corr: 0.9691 
2021-02-05 02:45:40:INFO:VAL-(misa) >>  Has0_acc_2: 0.7306  Has0_F1_score: 0.7158  Non0_acc_2: 0.7949  Non0_F1_score: 0.7907  Mult_acc_5: 0.5286  Mult_acc_7: 0.5131  MAE: 0.5950  Corr: 0.6920  Loss: 0.6268 
2021-02-05 02:47:54:INFO:TRAIN-(misa) (8/24/3)>> loss: 0.1746  Has0_acc_2: 0.8711  Has0_F1_score: 0.8669  Non0_acc_2: 0.9496  Non0_F1_score: 0.9496  Mult_acc_5: 0.7713  Mult_acc_7: 0.7442  MAE: 0.2862  Corr: 0.9345 
2021-02-05 02:47:59:INFO:VAL-(misa) >>  Has0_acc_2: 0.8022  Has0_F1_score: 0.7976  Non0_acc_2: 0.8227  Non0_F1_score: 0.8238  Mult_acc_5: 0.5158  Mult_acc_7: 0.5008  MAE: 0.5917  Corr: 0.6892  Loss: 0.6247 
2021-02-05 02:48:11:INFO:TEST-(misa) >>  Has0_acc_2: 0.8126  Has0_F1_score: 0.8088  Non0_acc_2: 0.8426  Non0_F1_score: 0.8437  Mult_acc_5: 0.5400  Mult_acc_7: 0.5218  MAE: 0.5523  Corr: 0.7515  Loss: 0.5364 
2021-02-05 02:48:16:INFO:Start running misa...
2021-02-05 02:48:16:INFO:<Storage{'is_tune': False, 'train_mode': 'regression', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1114}>
2021-02-05 02:48:16:INFO:Let's use 1 GPUs!
2021-02-05 02:48:28:INFO:train samples: (16326,)
2021-02-05 02:48:39:INFO:valid samples: (1871,)
2021-02-05 02:48:50:INFO:test samples: (4659,)
2021-02-05 02:48:50:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 02:48:50:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 02:48:50:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 02:48:50:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 02:48:50:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 02:48:50:INFO:loading file None
2021-02-05 02:48:50:INFO:loading file None
2021-02-05 02:48:50:INFO:loading file None
2021-02-05 02:48:50:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 02:48:50:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 02:48:50:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 02:48:52:INFO:The model has 110917345 trainable parameters
2021-02-05 02:51:06:INFO:TRAIN-(misa) (1/1/4)>> loss: 1.3479  Has0_acc_2: 0.7840  Has0_F1_score: 0.7784  Non0_acc_2: 0.8134  Non0_F1_score: 0.8138  Mult_acc_5: 0.4877  Mult_acc_7: 0.4762  MAE: 0.6257  Corr: 0.6794 
2021-02-05 02:51:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.8113  Has0_F1_score: 0.8044  Non0_acc_2: 0.8484  Non0_F1_score: 0.8477  Mult_acc_5: 0.5302  Mult_acc_7: 0.5168  MAE: 0.5411  Corr: 0.7308  Loss: 0.5165 
2021-02-05 02:53:27:INFO:TRAIN-(misa) (1/2/4)>> loss: 0.5748  Has0_acc_2: 0.8285  Has0_F1_score: 0.8241  Non0_acc_2: 0.8738  Non0_F1_score: 0.8742  Mult_acc_5: 0.5720  Mult_acc_7: 0.5509  MAE: 0.5020  Corr: 0.8112 
2021-02-05 02:53:32:INFO:VAL-(misa) >>  Has0_acc_2: 0.7643  Has0_F1_score: 0.7524  Non0_acc_2: 0.8227  Non0_F1_score: 0.8200  Mult_acc_5: 0.5436  Mult_acc_7: 0.5249  MAE: 0.5535  Corr: 0.7314  Loss: 0.5492 
2021-02-05 02:55:46:INFO:TRAIN-(misa) (2/3/4)>> loss: 0.3450  Has0_acc_2: 0.8605  Has0_F1_score: 0.8565  Non0_acc_2: 0.9192  Non0_F1_score: 0.9194  Mult_acc_5: 0.6616  Mult_acc_7: 0.6347  MAE: 0.3975  Corr: 0.8874 
2021-02-05 02:55:51:INFO:VAL-(misa) >>  Has0_acc_2: 0.8092  Has0_F1_score: 0.8036  Non0_acc_2: 0.8310  Non0_F1_score: 0.8309  Mult_acc_5: 0.5243  Mult_acc_7: 0.5061  MAE: 0.5771  Corr: 0.7169  Loss: 0.5837 
2021-02-05 02:58:04:INFO:TRAIN-(misa) (3/4/4)>> loss: 0.2325  Has0_acc_2: 0.8711  Has0_F1_score: 0.8670  Non0_acc_2: 0.9395  Non0_F1_score: 0.9395  Mult_acc_5: 0.7322  Mult_acc_7: 0.7038  MAE: 0.3237  Corr: 0.9273 
2021-02-05 02:58:09:INFO:VAL-(misa) >>  Has0_acc_2: 0.8300  Has0_F1_score: 0.8277  Non0_acc_2: 0.8470  Non0_F1_score: 0.8490  Mult_acc_5: 0.5211  Mult_acc_7: 0.5056  MAE: 0.5721  Corr: 0.7267  Loss: 0.5587 
2021-02-05 03:00:21:INFO:TRAIN-(misa) (4/5/4)>> loss: 0.1745  Has0_acc_2: 0.8816  Has0_F1_score: 0.8779  Non0_acc_2: 0.9557  Non0_F1_score: 0.9557  Mult_acc_5: 0.7718  Mult_acc_7: 0.7448  MAE: 0.2780  Corr: 0.9469 
2021-02-05 03:00:25:INFO:VAL-(misa) >>  Has0_acc_2: 0.7958  Has0_F1_score: 0.7873  Non0_acc_2: 0.8380  Non0_F1_score: 0.8366  Mult_acc_5: 0.5430  Mult_acc_7: 0.5286  MAE: 0.5436  Corr: 0.7258  Loss: 0.5270 
2021-02-05 03:02:37:INFO:TRAIN-(misa) (5/6/4)>> loss: 0.1308  Has0_acc_2: 0.8858  Has0_F1_score: 0.8821  Non0_acc_2: 0.9679  Non0_F1_score: 0.9679  Mult_acc_5: 0.8121  Mult_acc_7: 0.7862  MAE: 0.2374  Corr: 0.9613 
2021-02-05 03:02:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.8290  Has0_F1_score: 0.8258  Non0_acc_2: 0.8491  Non0_F1_score: 0.8504  Mult_acc_5: 0.5126  Mult_acc_7: 0.4949  MAE: 0.5770  Corr: 0.7272  Loss: 0.5633 
2021-02-05 03:04:53:INFO:TRAIN-(misa) (6/7/4)>> loss: 0.1094  Has0_acc_2: 0.8922  Has0_F1_score: 0.8886  Non0_acc_2: 0.9777  Non0_F1_score: 0.9777  Mult_acc_5: 0.8349  Mult_acc_7: 0.8103  MAE: 0.2146  Corr: 0.9680 
2021-02-05 03:04:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.8193  Has0_F1_score: 0.8148  Non0_acc_2: 0.8442  Non0_F1_score: 0.8449  Mult_acc_5: 0.5281  Mult_acc_7: 0.5126  MAE: 0.5536  Corr: 0.7226  Loss: 0.5346 
2021-02-05 03:07:11:INFO:TRAIN-(misa) (7/8/4)>> loss: 0.0944  Has0_acc_2: 0.8885  Has0_F1_score: 0.8847  Non0_acc_2: 0.9789  Non0_F1_score: 0.9789  Mult_acc_5: 0.8438  Mult_acc_7: 0.8200  MAE: 0.1995  Corr: 0.9728 
2021-02-05 03:07:16:INFO:VAL-(misa) >>  Has0_acc_2: 0.7504  Has0_F1_score: 0.7371  Non0_acc_2: 0.8115  Non0_F1_score: 0.8082  Mult_acc_5: 0.5404  Mult_acc_7: 0.5243  MAE: 0.5666  Corr: 0.7207  Loss: 0.5540 
2021-02-05 03:09:28:INFO:TRAIN-(misa) (8/9/4)>> loss: 0.0886  Has0_acc_2: 0.8914  Has0_F1_score: 0.8877  Non0_acc_2: 0.9808  Non0_F1_score: 0.9808  Mult_acc_5: 0.8565  Mult_acc_7: 0.8349  MAE: 0.1946  Corr: 0.9742 
2021-02-05 03:09:33:INFO:VAL-(misa) >>  Has0_acc_2: 0.8220  Has0_F1_score: 0.8188  Non0_acc_2: 0.8387  Non0_F1_score: 0.8401  Mult_acc_5: 0.5088  Mult_acc_7: 0.4939  MAE: 0.5722  Corr: 0.7233  Loss: 0.5469 
2021-02-05 03:09:45:INFO:TEST-(misa) >>  Has0_acc_2: 0.8075  Has0_F1_score: 0.8012  Non0_acc_2: 0.8525  Non0_F1_score: 0.8521  Mult_acc_5: 0.5321  Mult_acc_7: 0.5184  MAE: 0.5584  Corr: 0.7484  Loss: 0.5565 
2021-02-05 03:09:51:INFO:Start running misa...
2021-02-05 03:09:51:INFO:<Storage{'is_tune': False, 'train_mode': 'regression', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1115}>
2021-02-05 03:09:51:INFO:Let's use 1 GPUs!
2021-02-05 03:10:02:INFO:train samples: (16326,)
2021-02-05 03:10:13:INFO:valid samples: (1871,)
2021-02-05 03:10:24:INFO:test samples: (4659,)
2021-02-05 03:10:24:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 03:10:24:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 03:10:24:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 03:10:24:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 03:10:24:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 03:10:24:INFO:loading file None
2021-02-05 03:10:24:INFO:loading file None
2021-02-05 03:10:24:INFO:loading file None
2021-02-05 03:10:24:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 03:10:24:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 03:10:24:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 03:10:26:INFO:The model has 110917345 trainable parameters
2021-02-05 03:12:37:INFO:TRAIN-(misa) (1/1/5)>> loss: 1.2937  Has0_acc_2: 0.7777  Has0_F1_score: 0.7718  Non0_acc_2: 0.8082  Non0_F1_score: 0.8088  Mult_acc_5: 0.4850  Mult_acc_7: 0.4738  MAE: 0.6301  Corr: 0.6744 
2021-02-05 03:12:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.8343  Has0_F1_score: 0.8342  Non0_acc_2: 0.8331  Non0_F1_score: 0.8362  Mult_acc_5: 0.4564  Mult_acc_7: 0.4420  MAE: 0.6177  Corr: 0.6978  Loss: 0.6211 
2021-02-05 03:14:57:INFO:TRAIN-(misa) (1/2/5)>> loss: 0.5351  Has0_acc_2: 0.8340  Has0_F1_score: 0.8297  Non0_acc_2: 0.8789  Non0_F1_score: 0.8792  Mult_acc_5: 0.5790  Mult_acc_7: 0.5554  MAE: 0.4940  Corr: 0.8218 
2021-02-05 03:15:02:INFO:VAL-(misa) >>  Has0_acc_2: 0.8215  Has0_F1_score: 0.8188  Non0_acc_2: 0.8387  Non0_F1_score: 0.8407  Mult_acc_5: 0.5334  Mult_acc_7: 0.5163  MAE: 0.5562  Corr: 0.7306  Loss: 0.5328 
2021-02-05 03:17:16:INFO:TRAIN-(misa) (1/3/5)>> loss: 0.3396  Has0_acc_2: 0.8578  Has0_F1_score: 0.8538  Non0_acc_2: 0.9164  Non0_F1_score: 0.9166  Mult_acc_5: 0.6637  Mult_acc_7: 0.6351  MAE: 0.3972  Corr: 0.8883 
2021-02-05 03:17:20:INFO:VAL-(misa) >>  Has0_acc_2: 0.8049  Has0_F1_score: 0.7995  Non0_acc_2: 0.8296  Non0_F1_score: 0.8301  Mult_acc_5: 0.5222  Mult_acc_7: 0.5083  MAE: 0.5561  Corr: 0.7290  Loss: 0.5298 
2021-02-05 03:19:33:INFO:TRAIN-(misa) (1/4/5)>> loss: 0.2483  Has0_acc_2: 0.8651  Has0_F1_score: 0.8609  Non0_acc_2: 0.9333  Non0_F1_score: 0.9333  Mult_acc_5: 0.7176  Mult_acc_7: 0.6886  MAE: 0.3381  Corr: 0.9198 
2021-02-05 03:19:38:INFO:VAL-(misa) >>  Has0_acc_2: 0.7707  Has0_F1_score: 0.7602  Non0_acc_2: 0.8261  Non0_F1_score: 0.8246  Mult_acc_5: 0.5216  Mult_acc_7: 0.5094  MAE: 0.5621  Corr: 0.6907  Loss: 0.5633 
2021-02-05 03:21:50:INFO:TRAIN-(misa) (2/5/5)>> loss: 0.1821  Has0_acc_2: 0.8794  Has0_F1_score: 0.8756  Non0_acc_2: 0.9557  Non0_F1_score: 0.9558  Mult_acc_5: 0.7647  Mult_acc_7: 0.7367  MAE: 0.2853  Corr: 0.9436 
2021-02-05 03:21:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.8087  Has0_F1_score: 0.8031  Non0_acc_2: 0.8380  Non0_F1_score: 0.8384  Mult_acc_5: 0.5366  Mult_acc_7: 0.5211  MAE: 0.5531  Corr: 0.7221  Loss: 0.5368 
2021-02-05 03:24:07:INFO:TRAIN-(misa) (3/6/5)>> loss: 0.1303  Has0_acc_2: 0.8864  Has0_F1_score: 0.8827  Non0_acc_2: 0.9670  Non0_F1_score: 0.9670  Mult_acc_5: 0.8146  Mult_acc_7: 0.7885  MAE: 0.2375  Corr: 0.9613 
2021-02-05 03:24:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.8060  Has0_F1_score: 0.8004  Non0_acc_2: 0.8366  Non0_F1_score: 0.8372  Mult_acc_5: 0.5216  Mult_acc_7: 0.5061  MAE: 0.5593  Corr: 0.7310  Loss: 0.5358 
2021-02-05 03:26:24:INFO:TRAIN-(misa) (4/7/5)>> loss: 0.1030  Has0_acc_2: 0.8959  Has0_F1_score: 0.8925  Non0_acc_2: 0.9806  Non0_F1_score: 0.9806  Mult_acc_5: 0.8448  Mult_acc_7: 0.8216  MAE: 0.2064  Corr: 0.9704 
2021-02-05 03:26:28:INFO:VAL-(misa) >>  Has0_acc_2: 0.7739  Has0_F1_score: 0.7630  Non0_acc_2: 0.8289  Non0_F1_score: 0.8268  Mult_acc_5: 0.5222  Mult_acc_7: 0.5077  MAE: 0.5580  Corr: 0.7211  Loss: 0.5425 
2021-02-05 03:28:40:INFO:TRAIN-(misa) (5/8/5)>> loss: 0.0871  Has0_acc_2: 0.8930  Has0_F1_score: 0.8893  Non0_acc_2: 0.9847  Non0_F1_score: 0.9847  Mult_acc_5: 0.8608  Mult_acc_7: 0.8401  MAE: 0.1881  Corr: 0.9754 
2021-02-05 03:28:45:INFO:VAL-(misa) >>  Has0_acc_2: 0.7739  Has0_F1_score: 0.7629  Non0_acc_2: 0.8282  Non0_F1_score: 0.8259  Mult_acc_5: 0.5259  Mult_acc_7: 0.5131  MAE: 0.5631  Corr: 0.7258  Loss: 0.5506 
2021-02-05 03:30:57:INFO:TRAIN-(misa) (6/9/5)>> loss: 0.0834  Has0_acc_2: 0.8936  Has0_F1_score: 0.8899  Non0_acc_2: 0.9869  Non0_F1_score: 0.9869  Mult_acc_5: 0.8616  Mult_acc_7: 0.8402  MAE: 0.1864  Corr: 0.9760 
2021-02-05 03:31:02:INFO:VAL-(misa) >>  Has0_acc_2: 0.7894  Has0_F1_score: 0.7803  Non0_acc_2: 0.8366  Non0_F1_score: 0.8351  Mult_acc_5: 0.5334  Mult_acc_7: 0.5200  MAE: 0.5501  Corr: 0.7282  Loss: 0.5319 
2021-02-05 03:33:14:INFO:TRAIN-(misa) (7/10/5)>> loss: 0.0753  Has0_acc_2: 0.8960  Has0_F1_score: 0.8924  Non0_acc_2: 0.9884  Non0_F1_score: 0.9884  Mult_acc_5: 0.8662  Mult_acc_7: 0.8463  MAE: 0.1764  Corr: 0.9785 
2021-02-05 03:33:19:INFO:VAL-(misa) >>  Has0_acc_2: 0.8290  Has0_F1_score: 0.8270  Non0_acc_2: 0.8470  Non0_F1_score: 0.8494  Mult_acc_5: 0.5179  Mult_acc_7: 0.5067  MAE: 0.5582  Corr: 0.7230  Loss: 0.5300 
2021-02-05 03:35:31:INFO:TRAIN-(misa) (8/11/5)>> loss: 0.0768  Has0_acc_2: 0.8950  Has0_F1_score: 0.8914  Non0_acc_2: 0.9878  Non0_F1_score: 0.9878  Mult_acc_5: 0.8681  Mult_acc_7: 0.8479  MAE: 0.1797  Corr: 0.9774 
2021-02-05 03:35:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.8215  Has0_F1_score: 0.8172  Non0_acc_2: 0.8512  Non0_F1_score: 0.8522  Mult_acc_5: 0.5334  Mult_acc_7: 0.5168  MAE: 0.5530  Corr: 0.7263  Loss: 0.5316 
2021-02-05 03:35:48:INFO:TEST-(misa) >>  Has0_acc_2: 0.8120  Has0_F1_score: 0.8072  Non0_acc_2: 0.8489  Non0_F1_score: 0.8494  Mult_acc_5: 0.5355  Mult_acc_7: 0.5173  MAE: 0.5587  Corr: 0.7615  Loss: 0.5390 
2021-02-05 03:35:53:INFO:Results are added to results/results/normals/mosei-regression.csv...
2021-02-05 12:53:44:INFO:Start running misa...
2021-02-05 12:53:44:INFO:<Storage{'is_tune': False, 'train_mode': 'classification', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1111}>
2021-02-05 12:53:44:INFO:Find gpu: 1, with memory: 2996568064 left!
2021-02-05 12:53:44:INFO:Let's use 1 GPUs!
2021-02-05 12:53:58:INFO:train samples: (16326,)
2021-02-05 12:54:13:INFO:valid samples: (1871,)
2021-02-05 12:54:27:INFO:test samples: (4659,)
2021-02-05 12:54:28:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 12:54:28:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 12:54:28:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 12:54:28:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 12:54:28:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 12:54:28:INFO:loading file None
2021-02-05 12:54:28:INFO:loading file None
2021-02-05 12:54:28:INFO:loading file None
2021-02-05 12:54:28:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 12:54:28:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 12:54:28:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 12:54:32:INFO:The model has 110918115 trainable parameters
2021-02-05 12:57:03:INFO:TRAIN-(misa) (1/1/1)>> loss: 1.3708  Has0_acc_2: 0.6859  Has0_F1_score: 0.6985  Non0_acc_2: 0.2486  Non0_F1_score: 0.2300  Acc_3: 0.6450  F1_score_3: 0.6769 
2021-02-05 12:57:09:INFO:VAL-(misa) >>  Has0_acc_2: 0.7066  Has0_F1_score: 0.7095  Non0_acc_2: 0.2962  Non0_F1_score: 0.3229  Acc_3: 0.6521  F1_score_3: 0.6653  Loss: 0.8031 
2021-02-05 12:59:47:INFO:TRAIN-(misa) (1/2/1)>> loss: 0.7393  Has0_acc_2: 0.7581  Has0_F1_score: 0.7651  Non0_acc_2: 0.3106  Non0_F1_score: 0.3031  Acc_3: 0.7430  F1_score_3: 0.7508 
2021-02-05 12:59:52:INFO:VAL-(misa) >>  Has0_acc_2: 0.7055  Has0_F1_score: 0.7161  Non0_acc_2: 0.2643  Non0_F1_score: 0.2570  Acc_3: 0.6649  F1_score_3: 0.7128  Loss: 0.8393 
2021-02-05 13:02:26:INFO:TRAIN-(misa) (2/3/1)>> loss: 0.5204  Has0_acc_2: 0.7977  Has0_F1_score: 0.8027  Non0_acc_2: 0.3403  Non0_F1_score: 0.3379  Acc_3: 0.8313  F1_score_3: 0.8327 
2021-02-05 13:02:32:INFO:VAL-(misa) >>  Has0_acc_2: 0.6702  Has0_F1_score: 0.6926  Non0_acc_2: 0.2218  Non0_F1_score: 0.1931  Acc_3: 0.6205  F1_score_3: 0.6142  Loss: 0.9435 
2021-02-05 13:05:06:INFO:TRAIN-(misa) (3/4/1)>> loss: 0.3508  Has0_acc_2: 0.8207  Has0_F1_score: 0.8248  Non0_acc_2: 0.3533  Non0_F1_score: 0.3514  Acc_3: 0.8970  F1_score_3: 0.8973 
2021-02-05 13:05:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.6772  Has0_F1_score: 0.6967  Non0_acc_2: 0.2323  Non0_F1_score: 0.2082  Acc_3: 0.6499  F1_score_3: 0.6559  Loss: 1.1943 
2021-02-05 13:07:45:INFO:TRAIN-(misa) (4/5/1)>> loss: 0.2547  Has0_acc_2: 0.8505  Has0_F1_score: 0.8530  Non0_acc_2: 0.3587  Non0_F1_score: 0.3572  Acc_3: 0.9327  F1_score_3: 0.9328 
2021-02-05 13:07:51:INFO:VAL-(misa) >>  Has0_acc_2: 0.7189  Has0_F1_score: 0.7235  Non0_acc_2: 0.2754  Non0_F1_score: 0.2838  Acc_3: 0.6376  F1_score_3: 0.6342  Loss: 1.1941 
2021-02-05 13:10:26:INFO:TRAIN-(misa) (5/6/1)>> loss: 0.1906  Has0_acc_2: 0.8626  Has0_F1_score: 0.8645  Non0_acc_2: 0.3625  Non0_F1_score: 0.3624  Acc_3: 0.9506  F1_score_3: 0.9506 
2021-02-05 13:10:31:INFO:VAL-(misa) >>  Has0_acc_2: 0.6777  Has0_F1_score: 0.6895  Non0_acc_2: 0.2510  Non0_F1_score: 0.2457  Acc_3: 0.6317  F1_score_3: 0.6359  Loss: 1.3326 
2021-02-05 13:13:05:INFO:TRAIN-(misa) (6/7/1)>> loss: 0.1547  Has0_acc_2: 0.8644  Has0_F1_score: 0.8664  Non0_acc_2: 0.3645  Non0_F1_score: 0.3644  Acc_3: 0.9625  F1_score_3: 0.9625 
2021-02-05 13:13:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.7039  Has0_F1_score: 0.7099  Non0_acc_2: 0.2629  Non0_F1_score: 0.2667  Acc_3: 0.6350  F1_score_3: 0.6437  Loss: 1.5342 
2021-02-05 13:15:44:INFO:TRAIN-(misa) (7/8/1)>> loss: 0.1380  Has0_acc_2: 0.8742  Has0_F1_score: 0.8758  Non0_acc_2: 0.3653  Non0_F1_score: 0.3650  Acc_3: 0.9674  F1_score_3: 0.9674 
2021-02-05 13:15:49:INFO:VAL-(misa) >>  Has0_acc_2: 0.7050  Has0_F1_score: 0.7106  Non0_acc_2: 0.2636  Non0_F1_score: 0.2684  Acc_3: 0.6398  F1_score_3: 0.6455  Loss: 1.6028 
2021-02-05 13:18:10:INFO:TRAIN-(misa) (8/9/1)>> loss: 0.1262  Has0_acc_2: 0.8624  Has0_F1_score: 0.8645  Non0_acc_2: 0.3652  Non0_F1_score: 0.3650  Acc_3: 0.9683  F1_score_3: 0.9683 
2021-02-05 13:18:16:INFO:VAL-(misa) >>  Has0_acc_2: 0.7018  Has0_F1_score: 0.7063  Non0_acc_2: 0.2705  Non0_F1_score: 0.2823  Acc_3: 0.5901  F1_score_3: 0.5749  Loss: 1.7532 
2021-02-05 13:18:29:INFO:TEST-(misa) >>  Has0_acc_2: 0.7132  Has0_F1_score: 0.7151  Non0_acc_2: 0.3101  Non0_F1_score: 0.3348  Acc_3: 0.6557  F1_score_3: 0.6639  Loss: 0.7897 
2021-02-05 13:18:35:INFO:Start running misa...
2021-02-05 13:18:35:INFO:<Storage{'is_tune': False, 'train_mode': 'classification', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1112}>
2021-02-05 13:18:35:INFO:Let's use 1 GPUs!
2021-02-05 13:18:48:INFO:train samples: (16326,)
2021-02-05 13:19:01:INFO:valid samples: (1871,)
2021-02-05 13:19:14:INFO:test samples: (4659,)
2021-02-05 13:19:15:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 13:19:15:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 13:19:15:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 13:19:15:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 13:19:15:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 13:19:15:INFO:loading file None
2021-02-05 13:19:15:INFO:loading file None
2021-02-05 13:19:15:INFO:loading file None
2021-02-05 13:19:15:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 13:19:15:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 13:19:15:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 13:19:17:INFO:The model has 110918115 trainable parameters
2021-02-05 13:21:37:INFO:TRAIN-(misa) (1/1/2)>> loss: 1.3768  Has0_acc_2: 0.6881  Has0_F1_score: 0.7009  Non0_acc_2: 0.2523  Non0_F1_score: 0.2338  Acc_3: 0.6469  F1_score_3: 0.6845 
2021-02-05 13:21:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.6756  Has0_F1_score: 0.7029  Non0_acc_2: 0.2170  Non0_F1_score: 0.1783  Acc_3: 0.6595  F1_score_3: 0.6983  Loss: 0.8159 
2021-02-05 13:24:06:INFO:TRAIN-(misa) (1/2/2)>> loss: 0.7339  Has0_acc_2: 0.7644  Has0_F1_score: 0.7708  Non0_acc_2: 0.3150  Non0_F1_score: 0.3092  Acc_3: 0.7449  F1_score_3: 0.7539 
2021-02-05 13:24:11:INFO:VAL-(misa) >>  Has0_acc_2: 0.6815  Has0_F1_score: 0.7007  Non0_acc_2: 0.2434  Non0_F1_score: 0.2216  Acc_3: 0.6665  F1_score_3: 0.6751  Loss: 0.7852 
2021-02-05 13:26:34:INFO:TRAIN-(misa) (1/3/2)>> loss: 0.4863  Has0_acc_2: 0.8049  Has0_F1_score: 0.8099  Non0_acc_2: 0.3442  Non0_F1_score: 0.3410  Acc_3: 0.8433  F1_score_3: 0.8445 
2021-02-05 13:26:40:INFO:VAL-(misa) >>  Has0_acc_2: 0.6804  Has0_F1_score: 0.6936  Non0_acc_2: 0.2483  Non0_F1_score: 0.2386  Acc_3: 0.6419  F1_score_3: 0.6478  Loss: 1.0306 
2021-02-05 13:29:01:INFO:TRAIN-(misa) (2/4/2)>> loss: 0.3216  Has0_acc_2: 0.8218  Has0_F1_score: 0.8260  Non0_acc_2: 0.3557  Non0_F1_score: 0.3545  Acc_3: 0.9071  F1_score_3: 0.9073 
2021-02-05 13:29:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.6505  Has0_F1_score: 0.6822  Non0_acc_2: 0.2010  Non0_F1_score: 0.1627  Acc_3: 0.6371  F1_score_3: 0.6629  Loss: 1.2094 
2021-02-05 13:31:26:INFO:TRAIN-(misa) (3/5/2)>> loss: 0.2458  Has0_acc_2: 0.8407  Has0_F1_score: 0.8437  Non0_acc_2: 0.3601  Non0_F1_score: 0.3599  Acc_3: 0.9346  F1_score_3: 0.9346 
2021-02-05 13:31:31:INFO:VAL-(misa) >>  Has0_acc_2: 0.6921  Has0_F1_score: 0.7100  Non0_acc_2: 0.2448  Non0_F1_score: 0.2218  Acc_3: 0.6563  F1_score_3: 0.6721  Loss: 1.3321 
2021-02-05 13:33:52:INFO:TRAIN-(misa) (4/6/2)>> loss: 0.1818  Has0_acc_2: 0.8418  Has0_F1_score: 0.8451  Non0_acc_2: 0.3631  Non0_F1_score: 0.3621  Acc_3: 0.9550  F1_score_3: 0.9550 
2021-02-05 13:33:58:INFO:VAL-(misa) >>  Has0_acc_2: 0.7012  Has0_F1_score: 0.7051  Non0_acc_2: 0.2782  Non0_F1_score: 0.2952  Acc_3: 0.6350  F1_score_3: 0.6444  Loss: 1.4665 
2021-02-05 13:36:18:INFO:TRAIN-(misa) (5/7/2)>> loss: 0.1423  Has0_acc_2: 0.8539  Has0_F1_score: 0.8564  Non0_acc_2: 0.3644  Non0_F1_score: 0.3637  Acc_3: 0.9659  F1_score_3: 0.9659 
2021-02-05 13:36:23:INFO:VAL-(misa) >>  Has0_acc_2: 0.6670  Has0_F1_score: 0.6848  Non0_acc_2: 0.2337  Non0_F1_score: 0.2160  Acc_3: 0.6307  F1_score_3: 0.6424  Loss: 1.7549 
2021-02-05 13:38:42:INFO:TRAIN-(misa) (6/8/2)>> loss: 0.1221  Has0_acc_2: 0.8661  Has0_F1_score: 0.8680  Non0_acc_2: 0.3651  Non0_F1_score: 0.3646  Acc_3: 0.9707  F1_score_3: 0.9707 
2021-02-05 13:38:47:INFO:VAL-(misa) >>  Has0_acc_2: 0.6734  Has0_F1_score: 0.6938  Non0_acc_2: 0.2295  Non0_F1_score: 0.2046  Acc_3: 0.6360  F1_score_3: 0.6585  Loss: 1.7618 
2021-02-05 13:41:07:INFO:TRAIN-(misa) (7/9/2)>> loss: 0.1161  Has0_acc_2: 0.8469  Has0_F1_score: 0.8499  Non0_acc_2: 0.3656  Non0_F1_score: 0.3652  Acc_3: 0.9733  F1_score_3: 0.9733 
2021-02-05 13:41:12:INFO:VAL-(misa) >>  Has0_acc_2: 0.6980  Has0_F1_score: 0.7087  Non0_acc_2: 0.2517  Non0_F1_score: 0.2425  Acc_3: 0.6398  F1_score_3: 0.6485  Loss: 1.7323 
2021-02-05 13:43:31:INFO:TRAIN-(misa) (8/10/2)>> loss: 0.1042  Has0_acc_2: 0.8396  Has0_F1_score: 0.8432  Non0_acc_2: 0.3663  Non0_F1_score: 0.3655  Acc_3: 0.9744  F1_score_3: 0.9744 
2021-02-05 13:43:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.6975  Has0_F1_score: 0.7070  Non0_acc_2: 0.2566  Non0_F1_score: 0.2517  Acc_3: 0.6440  F1_score_3: 0.6599  Loss: 1.7881 
2021-02-05 13:43:49:INFO:TEST-(misa) >>  Has0_acc_2: 0.6969  Has0_F1_score: 0.7122  Non0_acc_2: 0.2614  Non0_F1_score: 0.2372  Acc_3: 0.6750  F1_score_3: 0.6815  Loss: 0.7628 
2021-02-05 13:43:54:INFO:Start running misa...
2021-02-05 13:43:54:INFO:<Storage{'is_tune': False, 'train_mode': 'classification', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1113}>
2021-02-05 13:43:54:INFO:Let's use 1 GPUs!
2021-02-05 13:44:07:INFO:train samples: (16326,)
2021-02-05 13:44:19:INFO:valid samples: (1871,)
2021-02-05 13:44:30:INFO:test samples: (4659,)
2021-02-05 13:44:31:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 13:44:31:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 13:44:31:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 13:44:31:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 13:44:31:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 13:44:31:INFO:loading file None
2021-02-05 13:44:31:INFO:loading file None
2021-02-05 13:44:31:INFO:loading file None
2021-02-05 13:44:31:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 13:44:31:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 13:44:31:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 13:44:32:INFO:The model has 110918115 trainable parameters
2021-02-05 13:46:52:INFO:TRAIN-(misa) (1/1/3)>> loss: 1.3969  Has0_acc_2: 0.6936  Has0_F1_score: 0.7044  Non0_acc_2: 0.2567  Non0_F1_score: 0.2418  Acc_3: 0.6483  F1_score_3: 0.6805 
2021-02-05 13:46:57:INFO:VAL-(misa) >>  Has0_acc_2: 0.7066  Has0_F1_score: 0.7172  Non0_acc_2: 0.2691  Non0_F1_score: 0.2634  Acc_3: 0.6761  F1_score_3: 0.7104  Loss: 0.7638 
2021-02-05 13:49:18:INFO:TRAIN-(misa) (1/2/3)>> loss: 0.7435  Has0_acc_2: 0.7577  Has0_F1_score: 0.7645  Non0_acc_2: 0.3110  Non0_F1_score: 0.3048  Acc_3: 0.7434  F1_score_3: 0.7515 
2021-02-05 13:49:23:INFO:VAL-(misa) >>  Has0_acc_2: 0.7103  Has0_F1_score: 0.7167  Non0_acc_2: 0.2782  Non0_F1_score: 0.2851  Acc_3: 0.6579  F1_score_3: 0.6630  Loss: 0.8100 
2021-02-05 13:51:43:INFO:TRAIN-(misa) (2/3/3)>> loss: 0.5169  Has0_acc_2: 0.7968  Has0_F1_score: 0.8019  Non0_acc_2: 0.3394  Non0_F1_score: 0.3364  Acc_3: 0.8294  F1_score_3: 0.8303 
2021-02-05 13:51:48:INFO:VAL-(misa) >>  Has0_acc_2: 0.6927  Has0_F1_score: 0.7023  Non0_acc_2: 0.2531  Non0_F1_score: 0.2483  Acc_3: 0.6355  F1_score_3: 0.6322  Loss: 0.9488 
2021-02-05 13:54:07:INFO:TRAIN-(misa) (3/4/3)>> loss: 0.3434  Has0_acc_2: 0.8302  Has0_F1_score: 0.8337  Non0_acc_2: 0.3545  Non0_F1_score: 0.3529  Acc_3: 0.9008  F1_score_3: 0.9012 
2021-02-05 13:54:12:INFO:VAL-(misa) >>  Has0_acc_2: 0.6782  Has0_F1_score: 0.6918  Non0_acc_2: 0.2406  Non0_F1_score: 0.2284  Acc_3: 0.6221  F1_score_3: 0.6227  Loss: 1.0926 
2021-02-05 13:56:31:INFO:TRAIN-(misa) (4/5/3)>> loss: 0.2464  Has0_acc_2: 0.8430  Has0_F1_score: 0.8460  Non0_acc_2: 0.3605  Non0_F1_score: 0.3595  Acc_3: 0.9356  F1_score_3: 0.9357 
2021-02-05 13:56:36:INFO:VAL-(misa) >>  Has0_acc_2: 0.7066  Has0_F1_score: 0.7137  Non0_acc_2: 0.2608  Non0_F1_score: 0.2603  Acc_3: 0.6371  F1_score_3: 0.6457  Loss: 1.3600 
2021-02-05 13:58:56:INFO:TRAIN-(misa) (5/6/3)>> loss: 0.1993  Has0_acc_2: 0.8700  Has0_F1_score: 0.8716  Non0_acc_2: 0.3617  Non0_F1_score: 0.3610  Acc_3: 0.9497  F1_score_3: 0.9498 
2021-02-05 13:59:01:INFO:VAL-(misa) >>  Has0_acc_2: 0.7130  Has0_F1_score: 0.7132  Non0_acc_2: 0.3039  Non0_F1_score: 0.3487  Acc_3: 0.6077  F1_score_3: 0.6078  Loss: 1.5243 
2021-02-05 14:01:20:INFO:TRAIN-(misa) (6/7/3)>> loss: 0.1611  Has0_acc_2: 0.8699  Has0_F1_score: 0.8716  Non0_acc_2: 0.3646  Non0_F1_score: 0.3643  Acc_3: 0.9610  F1_score_3: 0.9611 
2021-02-05 14:01:26:INFO:VAL-(misa) >>  Has0_acc_2: 0.6959  Has0_F1_score: 0.7078  Non0_acc_2: 0.2427  Non0_F1_score: 0.2285  Acc_3: 0.6435  F1_score_3: 0.6523  Loss: 1.5075 
2021-02-05 14:03:45:INFO:TRAIN-(misa) (7/8/3)>> loss: 0.1455  Has0_acc_2: 0.8656  Has0_F1_score: 0.8675  Non0_acc_2: 0.3646  Non0_F1_score: 0.3642  Acc_3: 0.9647  F1_score_3: 0.9648 
2021-02-05 14:03:50:INFO:VAL-(misa) >>  Has0_acc_2: 0.6756  Has0_F1_score: 0.6886  Non0_acc_2: 0.2455  Non0_F1_score: 0.2367  Acc_3: 0.6301  F1_score_3: 0.6422  Loss: 1.4121 
2021-02-05 14:06:09:INFO:TRAIN-(misa) (8/9/3)>> loss: 0.1175  Has0_acc_2: 0.8726  Has0_F1_score: 0.8742  Non0_acc_2: 0.3664  Non0_F1_score: 0.3668  Acc_3: 0.9723  F1_score_3: 0.9723 
2021-02-05 14:06:14:INFO:VAL-(misa) >>  Has0_acc_2: 0.6745  Has0_F1_score: 0.6904  Non0_acc_2: 0.2371  Non0_F1_score: 0.2211  Acc_3: 0.6259  F1_score_3: 0.6265  Loss: 1.6537 
2021-02-05 14:06:27:INFO:TEST-(misa) >>  Has0_acc_2: 0.7336  Has0_F1_score: 0.7402  Non0_acc_2: 0.2931  Non0_F1_score: 0.2874  Acc_3: 0.6914  F1_score_3: 0.7216  Loss: 0.7305 
2021-02-05 14:06:33:INFO:Start running misa...
2021-02-05 14:06:33:INFO:<Storage{'is_tune': False, 'train_mode': 'classification', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1114}>
2021-02-05 14:06:33:INFO:Let's use 1 GPUs!
2021-02-05 14:06:46:INFO:train samples: (16326,)
2021-02-05 14:06:59:INFO:valid samples: (1871,)
2021-02-05 14:07:11:INFO:test samples: (4659,)
2021-02-05 14:07:12:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 14:07:12:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 14:07:12:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 14:07:12:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 14:07:12:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 14:07:12:INFO:loading file None
2021-02-05 14:07:12:INFO:loading file None
2021-02-05 14:07:12:INFO:loading file None
2021-02-05 14:07:12:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 14:07:12:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 14:07:12:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 14:07:14:INFO:The model has 110918115 trainable parameters
2021-02-05 14:09:33:INFO:TRAIN-(misa) (1/1/4)>> loss: 1.4249  Has0_acc_2: 0.6902  Has0_F1_score: 0.7013  Non0_acc_2: 0.2543  Non0_F1_score: 0.2391  Acc_3: 0.6441  F1_score_3: 0.6814 
2021-02-05 14:09:38:INFO:VAL-(misa) >>  Has0_acc_2: 0.6948  Has0_F1_score: 0.7091  Non0_acc_2: 0.2517  Non0_F1_score: 0.2364  Acc_3: 0.6627  F1_score_3: 0.7185  Loss: 0.8116 
2021-02-05 14:11:59:INFO:TRAIN-(misa) (1/2/4)>> loss: 0.7427  Has0_acc_2: 0.7589  Has0_F1_score: 0.7655  Non0_acc_2: 0.3124  Non0_F1_score: 0.3066  Acc_3: 0.7430  F1_score_3: 0.7516 
2021-02-05 14:12:04:INFO:VAL-(misa) >>  Has0_acc_2: 0.6761  Has0_F1_score: 0.6949  Non0_acc_2: 0.2316  Non0_F1_score: 0.2087  Acc_3: 0.6467  F1_score_3: 0.6715  Loss: 0.8405 
2021-02-05 14:14:24:INFO:TRAIN-(misa) (2/3/4)>> loss: 0.4973  Has0_acc_2: 0.8058  Has0_F1_score: 0.8105  Non0_acc_2: 0.3415  Non0_F1_score: 0.3376  Acc_3: 0.8379  F1_score_3: 0.8386 
2021-02-05 14:14:29:INFO:VAL-(misa) >>  Has0_acc_2: 0.7135  Has0_F1_score: 0.7149  Non0_acc_2: 0.2976  Non0_F1_score: 0.3299  Acc_3: 0.6328  F1_score_3: 0.6392  Loss: 0.9771 
2021-02-05 14:16:48:INFO:TRAIN-(misa) (3/4/4)>> loss: 0.3533  Has0_acc_2: 0.8205  Has0_F1_score: 0.8249  Non0_acc_2: 0.3553  Non0_F1_score: 0.3536  Acc_3: 0.8975  F1_score_3: 0.8978 
2021-02-05 14:16:53:INFO:VAL-(misa) >>  Has0_acc_2: 0.6905  Has0_F1_score: 0.6968  Non0_acc_2: 0.2580  Non0_F1_score: 0.2632  Acc_3: 0.6237  F1_score_3: 0.6274  Loss: 1.2165 
2021-02-05 14:19:13:INFO:TRAIN-(misa) (4/5/4)>> loss: 0.2404  Has0_acc_2: 0.8497  Has0_F1_score: 0.8523  Non0_acc_2: 0.3611  Non0_F1_score: 0.3601  Acc_3: 0.9363  F1_score_3: 0.9364 
2021-02-05 14:19:18:INFO:VAL-(misa) >>  Has0_acc_2: 0.6991  Has0_F1_score: 0.7090  Non0_acc_2: 0.2524  Non0_F1_score: 0.2447  Acc_3: 0.6227  F1_score_3: 0.6231  Loss: 1.1700 
2021-02-05 14:21:37:INFO:TRAIN-(misa) (5/6/4)>> loss: 0.1752  Has0_acc_2: 0.8588  Has0_F1_score: 0.8611  Non0_acc_2: 0.3636  Non0_F1_score: 0.3628  Acc_3: 0.9571  F1_score_3: 0.9571 
2021-02-05 14:21:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.6820  Has0_F1_score: 0.7028  Non0_acc_2: 0.2302  Non0_F1_score: 0.2018  Acc_3: 0.6312  F1_score_3: 0.6352  Loss: 1.5027 
2021-02-05 14:24:02:INFO:TRAIN-(misa) (6/7/4)>> loss: 0.1531  Has0_acc_2: 0.8733  Has0_F1_score: 0.8750  Non0_acc_2: 0.3648  Non0_F1_score: 0.3640  Acc_3: 0.9642  F1_score_3: 0.9643 
2021-02-05 14:24:07:INFO:VAL-(misa) >>  Has0_acc_2: 0.6927  Has0_F1_score: 0.7034  Non0_acc_2: 0.2566  Non0_F1_score: 0.2506  Acc_3: 0.6344  F1_score_3: 0.6382  Loss: 1.3710 
2021-02-05 14:26:26:INFO:TRAIN-(misa) (7/8/4)>> loss: 0.1408  Has0_acc_2: 0.8659  Has0_F1_score: 0.8678  Non0_acc_2: 0.3645  Non0_F1_score: 0.3642  Acc_3: 0.9664  F1_score_3: 0.9664 
2021-02-05 14:26:31:INFO:VAL-(misa) >>  Has0_acc_2: 0.6686  Has0_F1_score: 0.6928  Non0_acc_2: 0.2218  Non0_F1_score: 0.1911  Acc_3: 0.6366  F1_score_3: 0.6525  Loss: 1.6843 
2021-02-05 14:28:51:INFO:TRAIN-(misa) (8/9/4)>> loss: 0.1212  Has0_acc_2: 0.8857  Has0_F1_score: 0.8869  Non0_acc_2: 0.3657  Non0_F1_score: 0.3655  Acc_3: 0.9713  F1_score_3: 0.9713 
2021-02-05 14:28:56:INFO:VAL-(misa) >>  Has0_acc_2: 0.7060  Has0_F1_score: 0.7069  Non0_acc_2: 0.2872  Non0_F1_score: 0.3197  Acc_3: 0.5997  F1_score_3: 0.5922  Loss: 1.8530 
2021-02-05 14:29:08:INFO:TEST-(misa) >>  Has0_acc_2: 0.7042  Has0_F1_score: 0.7162  Non0_acc_2: 0.2661  Non0_F1_score: 0.2473  Acc_3: 0.6665  F1_score_3: 0.7179  Loss: 0.7913 
2021-02-05 14:29:14:INFO:Start running misa...
2021-02-05 14:29:14:INFO:<Storage{'is_tune': False, 'train_mode': 'classification', 'modelName': 'misa', 'datasetName': 'mosei', 'num_workers': 0, 'model_save_dir': 'results/models', 'res_save_dir': 'results/results/normals', 'gpu_ids': [1], 'seeds': [1111, 1112, 1113, 1114, 1115], 'dataPath': '/home/sharing/disk3/dataset/multimodal-sentiment-dataset/StandardDatasets/MOSEI/Processed/unaligned_50.pkl', 'seq_lens': (50, 500, 375), 'feature_dims': (768, 74, 35), 'train_samples': 16326, 'num_classes': 3, 'language': 'en', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'use_finetune': True, 'use_bert': True, 'early_stop': 8, 'update_epochs': 2, 'rnncell': 'lstm', 'use_cmd_sim': True, 'batch_size': 32, 'learning_rate': 0.0001, 'hidden_size': 128, 'dropout': 0.2, 'reverse_grad_weight': 0.5, 'diff_weight': 0.1, 'sim_weight': 1.0, 'sp_weight': 1.0, 'recon_weight': 0.8, 'grad_clip': 0.8, 'weight_decay': 0.0, 'seed': 1115}>
2021-02-05 14:29:14:INFO:Let's use 1 GPUs!
2021-02-05 14:29:25:INFO:train samples: (16326,)
2021-02-05 14:29:37:INFO:valid samples: (1871,)
2021-02-05 14:29:49:INFO:test samples: (4659,)
2021-02-05 14:29:50:INFO:Model name 'pretrained_model/bert_en' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'pretrained_model/bert_en' is a path, a model identifier, or url to a directory containing tokenizer files.
2021-02-05 14:29:50:INFO:Didn't find file pretrained_model/bert_en/added_tokens.json. We won't load it.
2021-02-05 14:29:50:INFO:Didn't find file pretrained_model/bert_en/special_tokens_map.json. We won't load it.
2021-02-05 14:29:50:INFO:Didn't find file pretrained_model/bert_en/tokenizer_config.json. We won't load it.
2021-02-05 14:29:50:INFO:loading file pretrained_model/bert_en/vocab.txt
2021-02-05 14:29:50:INFO:loading file None
2021-02-05 14:29:50:INFO:loading file None
2021-02-05 14:29:50:INFO:loading file None
2021-02-05 14:29:50:INFO:loading configuration file pretrained_model/bert_en/config.json
2021-02-05 14:29:50:INFO:Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2021-02-05 14:29:50:INFO:loading weights file pretrained_model/bert_en/pytorch_model.bin
2021-02-05 14:29:52:INFO:The model has 110918115 trainable parameters
2021-02-05 14:32:11:INFO:TRAIN-(misa) (1/1/5)>> loss: 1.3621  Has0_acc_2: 0.6843  Has0_F1_score: 0.6964  Non0_acc_2: 0.2500  Non0_F1_score: 0.2333  Acc_3: 0.6412  F1_score_3: 0.6740 
2021-02-05 14:32:16:INFO:VAL-(misa) >>  Has0_acc_2: 0.7012  Has0_F1_score: 0.7133  Non0_acc_2: 0.2629  Non0_F1_score: 0.2534  Acc_3: 0.6750  F1_score_3: 0.6927  Loss: 0.7431 
2021-02-05 14:34:36:INFO:TRAIN-(misa) (1/2/5)>> loss: 0.7203  Has0_acc_2: 0.7581  Has0_F1_score: 0.7649  Non0_acc_2: 0.3119  Non0_F1_score: 0.3057  Acc_3: 0.7474  F1_score_3: 0.7549 
2021-02-05 14:34:41:INFO:VAL-(misa) >>  Has0_acc_2: 0.6948  Has0_F1_score: 0.7071  Non0_acc_2: 0.2622  Non0_F1_score: 0.2544  Acc_3: 0.6606  F1_score_3: 0.6976  Loss: 0.8611 
2021-02-05 14:37:00:INFO:TRAIN-(misa) (2/3/5)>> loss: 0.4852  Has0_acc_2: 0.7963  Has0_F1_score: 0.8018  Non0_acc_2: 0.3417  Non0_F1_score: 0.3384  Acc_3: 0.8450  F1_score_3: 0.8457 
2021-02-05 14:37:05:INFO:VAL-(misa) >>  Has0_acc_2: 0.6713  Has0_F1_score: 0.6897  Non0_acc_2: 0.2316  Non0_F1_score: 0.2109  Acc_3: 0.6339  F1_score_3: 0.6371  Loss: 1.0077 
2021-02-05 14:39:25:INFO:TRAIN-(misa) (3/4/5)>> loss: 0.3221  Has0_acc_2: 0.8217  Has0_F1_score: 0.8259  Non0_acc_2: 0.3549  Non0_F1_score: 0.3530  Acc_3: 0.9116  F1_score_3: 0.9117 
2021-02-05 14:39:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.6729  Has0_F1_score: 0.6858  Non0_acc_2: 0.2469  Non0_F1_score: 0.2396  Acc_3: 0.5911  F1_score_3: 0.5741  Loss: 1.1902 
2021-02-05 14:41:50:INFO:TRAIN-(misa) (4/5/5)>> loss: 0.2297  Has0_acc_2: 0.8267  Has0_F1_score: 0.8308  Non0_acc_2: 0.3598  Non0_F1_score: 0.3588  Acc_3: 0.9416  F1_score_3: 0.9416 
2021-02-05 14:41:55:INFO:VAL-(misa) >>  Has0_acc_2: 0.6825  Has0_F1_score: 0.6913  Non0_acc_2: 0.2601  Non0_F1_score: 0.2626  Acc_3: 0.6285  F1_score_3: 0.6578  Loss: 1.5383 
2021-02-05 14:44:14:INFO:TRAIN-(misa) (5/6/5)>> loss: 0.1803  Has0_acc_2: 0.8259  Has0_F1_score: 0.8304  Non0_acc_2: 0.3634  Non0_F1_score: 0.3626  Acc_3: 0.9572  F1_score_3: 0.9572 
2021-02-05 14:44:19:INFO:VAL-(misa) >>  Has0_acc_2: 0.6799  Has0_F1_score: 0.6964  Non0_acc_2: 0.2385  Non0_F1_score: 0.2200  Acc_3: 0.6237  F1_score_3: 0.6325  Loss: 1.4844 
2021-02-05 14:46:37:INFO:TRAIN-(misa) (6/7/5)>> loss: 0.1580  Has0_acc_2: 0.8387  Has0_F1_score: 0.8421  Non0_acc_2: 0.3618  Non0_F1_score: 0.3608  Acc_3: 0.9610  F1_score_3: 0.9610 
2021-02-05 14:46:42:INFO:VAL-(misa) >>  Has0_acc_2: 0.6815  Has0_F1_score: 0.6955  Non0_acc_2: 0.2476  Non0_F1_score: 0.2358  Acc_3: 0.6430  F1_score_3: 0.6496  Loss: 1.5048 
2021-02-05 14:49:00:INFO:TRAIN-(misa) (7/8/5)>> loss: 0.1257  Has0_acc_2: 0.8627  Has0_F1_score: 0.8648  Non0_acc_2: 0.3643  Non0_F1_score: 0.3635  Acc_3: 0.9705  F1_score_3: 0.9705 
2021-02-05 14:49:06:INFO:VAL-(misa) >>  Has0_acc_2: 0.7274  Has0_F1_score: 0.7275  Non0_acc_2: 0.3039  Non0_F1_score: 0.3467  Acc_3: 0.6312  F1_score_3: 0.6369  Loss: 1.5750 
2021-02-05 14:51:25:INFO:TRAIN-(misa) (8/9/5)>> loss: 0.0942  Has0_acc_2: 0.8670  Has0_F1_score: 0.8689  Non0_acc_2: 0.3660  Non0_F1_score: 0.3654  Acc_3: 0.9793  F1_score_3: 0.9793 
2021-02-05 14:51:30:INFO:VAL-(misa) >>  Has0_acc_2: 0.6900  Has0_F1_score: 0.6982  Non0_acc_2: 0.2643  Non0_F1_score: 0.2674  Acc_3: 0.6253  F1_score_3: 0.6274  Loss: 1.8954 
2021-02-05 14:51:43:INFO:TEST-(misa) >>  Has0_acc_2: 0.7240  Has0_F1_score: 0.7320  Non0_acc_2: 0.2892  Non0_F1_score: 0.2814  Acc_3: 0.6931  F1_score_3: 0.7087  Loss: 0.7148 
2021-02-05 14:51:48:INFO:Results are added to results/results/normals/mosei-classification.csv...
